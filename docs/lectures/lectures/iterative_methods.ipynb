{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ECON622: Computational Economics with Data Science Applications\n",
        "\n",
        "Numerical Linear Algebra with Iterative Methods\n",
        "\n",
        "Jesse Perla (University of British Columbia)\n",
        "\n",
        "# Overview\n",
        "\n",
        "## Motivation\n",
        "\n",
        "-   In preparation for the ML lectures we cover some core numerical\n",
        "    linear algebra concepts\n",
        "-   Many of these are directly useful\n",
        "    -   e.g. solving large LLS and systems of equations, such as you\n",
        "        might find with a large scale two-way fixed effects model\n",
        "    -   Solving systems of equations is useful in itself\n",
        "-   Others will be helpful in setting up understanding for ML\n",
        "    -   Matrix-free and iterative methods\n",
        "    -   What governs complexity and convergence speed\n",
        "    -   Conditioning\n",
        "    -   Regularization\n",
        "\n",
        "## Summary and Material\n",
        "\n",
        "-   See [QuantEcon Krylov Methods and Matrix\n",
        "    Conditioning](https://julia.quantecon.org/tools_and_techniques/iterative_methods_sparsity.html)"
      ],
      "id": "d23650ff-d6ed-488f-8735-f29651ea1077"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "using LinearAlgebra, Statistics, BenchmarkTools, SparseArrays, Random\n",
        "using LaTeXStrings, Plots, IterativeSolvers, Preconditioners, IncompleteLU, LinearMaps\n",
        "using Arpack\n",
        "Random.seed!(42);  # seed random numbers for reproducibility"
      ],
      "id": "e62dcb17"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Conditioning\n",
        "\n",
        "## Direct Methods and Conditioning\n",
        "\n",
        "-   Some algorithms and some matrices are more numerically stable than\n",
        "    others\n",
        "    -   By “numerically stable” we mean sensitive to accumulated\n",
        "        roundoff errors\n",
        "-   A key issue is when matrices are close to singular, or almost have\n",
        "    collinear columns. Many times this can’t be avoided, other times it\n",
        "    can (e.g., choose orthogonal polynomials rather than monomials)\n",
        "-   This will become even more of an issue with iterative methods, but\n",
        "    is also the key to rapid convergence. Hint: $A x = b$ is easy if\n",
        "    $A = I$, even if it is dense.\n",
        "\n",
        "## Condition Numbers of Matrices\n",
        "\n",
        "-   $\\det(A) \\approx 0$ may say it is “almost” singular, but it is not\n",
        "    scale-invariant\n",
        "\n",
        "-   The condition number $\\kappa$, given matrix norm $||\\cdot||$ uses\n",
        "    the matrix norm\n",
        "\n",
        "    $$\n",
        "    \\text{cond}(A) \\equiv \\|A\\| \\|A^{-1}\\|\\geq 1\n",
        "    $$\n",
        "\n",
        "-   Expensive to calculate, can show that given spectrum\n",
        "\n",
        "    $$\n",
        "    \\text{cond}(A) = \\left|\\frac{\\lambda_{max}}{\\lambda_{min}}\\right|\n",
        "    $$\n",
        "\n",
        "-   Intuition: if $\\text{cond}(A) = K$, then $b \\to b + \\nabla b$ change\n",
        "    in $b$ amplifies to a $x \\to x + K \\nabla b$ error when solving\n",
        "    $A x = b$.\n",
        "\n",
        "-   See [Matlab Docs on\n",
        "    inv](https://www.mathworks.com/help/matlab/ref/inv.html#bu6sfy8-1)\n",
        "    for why `inv` is a bad idea when $\\text{cond}(A)$ is huge\n",
        "\n",
        "## Condition Numbers and Matrix Operations\n",
        "\n",
        "-   The identity matrix is as good as it gets\n",
        "-   Otherwise, the issue is when matrices are of fundamentally different\n",
        "    scales"
      ],
      "id": "ff48b174-6756-45fc-ad3e-e71cf6a6bc60"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "output-location": "column"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cond(I(2)) = 1.0\n",
            "cond(A2) = 2.0000000000005004e6\n",
            "cond(A2') = 2.0000000000004997e6\n",
            "cond(inv(A2)) = 2.0000000002323308e6"
          ]
        }
      ],
      "source": [
        "@show cond(I(2))\n",
        "epsilon = 1E-6\n",
        "A2 = [1.0 0.0\n",
        "     1.0 epsilon]\n",
        "@show cond(A2);\n",
        "@show cond(A2');\n",
        "@show cond(inv(A2));"
      ],
      "id": "4994fa34"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conditioning Under Matrix Products\n",
        "\n",
        "-   Matrix operations can often amplify the condition number, or may be\n",
        "    invariant\n",
        "-   Be especially careful with normal equations/etc."
      ],
      "id": "a5028c62-b444-4cd2-87a1-e45130162a21"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "output-location": "column"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cond(L) = 1.732050807568878e8\n",
            "cond(L' * L) = 2.8104131146758097e32"
          ]
        },
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "3×4 Matrix{Float64}:\n",
              " 1.0  1.0e-8  0.0     0.0\n",
              " 1.0  0.0     1.0e-8  0.0\n",
              " 1.0  0.0     0.0     1.0e-8"
            ]
          }
        }
      ],
      "source": [
        "lauchli(N, epsilon) = [ones(N)';\n",
        "                       epsilon * I(N)]'\n",
        "epsilon = 1E-8\n",
        "L = lauchli(3, epsilon) |> Matrix\n",
        "@show cond(L)\n",
        "@show cond(L' * L)\n",
        "L"
      ],
      "id": "8704cfa2"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "See\n",
        "[here](https://julia.quantecon.org/tools_and_techniques/iterative_methods_sparsity.html#why-a-monomial-basis-is-a-bad-idea)\n",
        "for why a monomial basis is a bad idea\n",
        "\n",
        "# Stationary Iterative Methods\n",
        "\n",
        "## Direct Methods\n",
        "\n",
        "-   Direct methods work with a matrix, stored in memory, and typically\n",
        "    involve factorizations\n",
        "    -   Can be dense or sparse\n",
        "    -   They can be fast, and solve problems to machine precision\n",
        "-   Typically are superior until problems get large or have particular\n",
        "    structure\n",
        "-   But always use the right factorizations and matrix structure! (e.g.,\n",
        "    posdef, sparse, etc)\n",
        "-   The key limitations are the sizes of the matrices (or the sparsity)\n",
        "\n",
        "## Iterative Methods\n",
        "\n",
        "-   Iterative methods are in the spirit of gradient descent and\n",
        "    optimization algorithms\n",
        "    -   They take an initial guess and update until convergence\n",
        "    -   They work on matrix-vector and vector-matrix products, and can\n",
        "        be **matrix-free**, which is a huge advantage for huge problems\n",
        "    -   Rather than waiting until completion like direct methods, you\n",
        "        can control stopping\n",
        "-   The key limitations on performance are geometric (e.g.,\n",
        "    conditioning), not dimensionality\n",
        "-   Two rough types: stationary methods and Krylov methods\n",
        "\n",
        "## Example from Previous Lectures\n",
        "\n",
        "-   Variation on CTMC example: $a >0$ gain, $b > 0$ to lose\n",
        "-   Solve the Bellman Equation for a CTMC"
      ],
      "id": "31dc1896-3b98-4ae7-887c-565f1a192348"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "101.96306207828795"
            ]
          }
        }
      ],
      "source": [
        "N = 100\n",
        "a = 0.1\n",
        "b = 0.05\n",
        "rho = 0.05\n",
        "Q = Tridiagonal(fill(b, N-1),\n",
        "                [-a; fill(-(a + b), N-2); -b],\n",
        "                fill(a, N-1))\n",
        "\n",
        "r = range(0.0, 10.0, length = N)\n",
        "A = rho * I - Q\n",
        "v_direct = A \\ r\n",
        "mean(v_direct)"
      ],
      "id": "29bb9e44"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Diagonal Dominance\n",
        "\n",
        "-   Stationary Iterative Methods reorganize the problem so it is a\n",
        "    contraction mapping and then iterate\n",
        "\n",
        "-   For matrices that are [**strictly diagonal\n",
        "    dominant**](https://en.wikipedia.org/wiki/Diagonally_dominant_matrix)\n",
        "    $$\n",
        "    |A_{ii}| \\geq \\sum_{j\\neq i} |A_{ij}| \\quad\\text{for all } i = 1\\ldots N\n",
        "    $$\n",
        "\n",
        "    -   i.e., sum of all off-diagonal elements in a row is less than the\n",
        "        diagonal element in absolute value\n",
        "\n",
        "-   Note for our problem rows sum to 0 so if $\\rho > 0$ then\n",
        "    $\\rho I - Q$ is strictly diagonally dominant\n",
        "\n",
        "## Jacobi Iteration\n",
        "\n",
        "-   To solve a system $A x = b$, split the matrix $A$ into its diagonal\n",
        "    and off-diagonal elements. That is,\n",
        "\n",
        "$$\n",
        "A \\equiv D + R\n",
        "$$\n",
        "\n",
        "$$\n",
        "D \\equiv \\begin{bmatrix} A_{11} & 0 & \\ldots & 0\\\\\n",
        "                    0    & A_{22} & \\ldots & 0\\\\\n",
        "                    \\vdots & \\vdots & \\vdots & \\vdots\\\\\n",
        "                    0 & 0 &  \\ldots & A_{NN}\n",
        "    \\end{bmatrix}\\,\\,\n",
        "R \\equiv \\begin{bmatrix} 0 & A_{12}  & \\ldots & A_{1N} \\\\\n",
        "                    A_{21}    & 0 & \\ldots & A_{2N} \\\\\n",
        "                    \\vdots & \\vdots & \\vdots & \\vdots\\\\\n",
        "                    A_{N1}  & A_{N2}  &  \\ldots & 0\n",
        "    \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "## Jacobi Iteration Algorithm\n",
        "\n",
        "-   Then we can rewrite $(D + R) x = b$ as $$\n",
        "    \\begin{aligned}\n",
        "    D x &= b - R x\\\\\n",
        "    x &= D^{-1} (b - R x)\n",
        "    \\end{aligned}\n",
        "    $$\n",
        "\n",
        "Where $D^{-1}$ is trivial since diagonal. To solve, take an iteration\n",
        "$x^k$, starting from $x^0$,\n",
        "\n",
        "$$\n",
        "x^{k+1} = D^{-1}(b - R x^k)\n",
        "$$\n",
        "\n",
        "## Code for Jacobi Iteration\n",
        "\n",
        "-   Showing Jacobi Iteration and a better method, successive\n",
        "    over-relaxation (SOR). Many better algoriths exist"
      ],
      "id": "4d5325bc-5d50-43bd-bdfe-a48af73e1d0d"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "output-location": "column"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "norm(v - v_direct, Inf) = 0.0017762754968373429\n",
            "norm(v - v_direct, Inf) = 9.052314453583676e-12"
          ]
        }
      ],
      "source": [
        "v = zeros(N)\n",
        "\n",
        "jacobi!(v, A, r, maxiter = 40)\n",
        "@show norm(v - v_direct, Inf)\n",
        "sor!(v, A, r, 1.1, maxiter = 40)\n",
        "@show norm(v - v_direct, Inf);"
      ],
      "id": "e1bef68b"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Krylov Methods\n",
        "\n",
        "## Krylov Subspaces\n",
        "\n",
        "-   Krylov methods are a class of iterative methods that use a sequence\n",
        "    of subspaces\n",
        "-   The subspaces are generated by repeated matrix-vector products\n",
        "    -   i.e., given an $A$ and a initial value $b$ we could generate the\n",
        "        sequence\n",
        "    -   $b, A b, A^2 b, \\ldots, A^k b$ and see\n",
        "-   Note that the only operation we require from our linear operator $A$\n",
        "    is the matrix-vector product. This is a huge advantage for large\n",
        "    problems\n",
        "-   e.g. Krylov method is [Conjugate\n",
        "    Gradient](https://en.wikipedia.org/wiki/Conjugate_gradient_method)\n",
        "    for posdef $A$\n",
        "\n",
        "## Conjugate Gradient\n",
        "\n",
        "-   Solving this system with the conjugate gradient method\n",
        "-   Using matrix, but could just implement $A$ as a function"
      ],
      "id": "381b0207-ea8d-4def-b080-88c16bad64fa"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "output-location": "column"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cond(Matrix(A * A')) = 1.0375698828984257e12"
          ]
        },
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "Converged after 183 iterations."
            ]
          }
        }
      ],
      "source": [
        "N = 100\n",
        "A = sprand(100, 100, 0.1)\n",
        "A = A * A'  # easy posdef\n",
        "b = rand(N)\n",
        "x_direct = A \\ b\n",
        "@show cond(Matrix(A * A'))\n",
        "x = zeros(N)\n",
        "sol = cg!(x, A, b, log = true, maxiter = 1000)\n",
        "sol[end]"
      ],
      "id": "db51baed"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Iterative Methods for LLS\n",
        "\n",
        "-   [LSMR](https://stanford.edu/group/SOL/software/lsmr/LSMR-SISC-2011.pdf)\n",
        "    is one of several Krylov methods for solving LLS\n",
        "\n",
        "$$\n",
        "\\min_{\\beta} \\| X \\beta -y \\|^2 + \\alpha  \\| \\beta\\|^2\n",
        "$$\n",
        "\n",
        "-   Where $\\alpha \\geq 0$. If $\\alpha = 0$ then it is delivers the\n",
        "    ridgeless regression limit, even if underdetermined\n",
        "\n",
        "## LSMR Example"
      ],
      "id": "e5d04f90-71f0-41eb-b91e-ac6fca7d2011"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "norm(beta_direct - beta_lsmr) = 1.0615184241157149e-5\n",
            "Converged after 15 iterations."
          ]
        }
      ],
      "source": [
        "M = 1000\n",
        "N = 10000\n",
        "sigma = 0.1\n",
        "beta = rand(M)\n",
        "# simulate data\n",
        "X = sprand(N, M, 0.1)\n",
        "y = X * beta + sigma * randn(N)\n",
        "beta_direct = X \\ y\n",
        "results = lsmr(X, y, log = true)\n",
        "beta_lsmr = results[1]\n",
        "@show norm(beta_direct - beta_lsmr)\n",
        "println(\"$(results[end])\")"
      ],
      "id": "966a2360"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Matrix-Free LLS\n",
        "\n",
        "-   To solve LLS problems, we need $X u$ and $X^T v$ products\n",
        "-   We can provide those functions directly (cheating here by just using\n",
        "    the matrix itself)"
      ],
      "id": "e67af1df-676d-4392-827d-917c9f259642"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "output-location": "column"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converged after 15 iterations."
          ]
        }
      ],
      "source": [
        "# matrix-vector product\n",
        "X_func(u) = X * u\n",
        "\n",
        "# adjoint-vector product\n",
        "X_T_func(v) = X' * v\n",
        "\n",
        "X_map = LinearMap(X_func, X_T_func, N, M)\n",
        "results = lsmr(X_map, y, log = true)\n",
        "println(\"$(results[end])\")"
      ],
      "id": "4157181d"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Eigenvalue Problems\n",
        "\n",
        "-   Variation on CTMC example: $a >0$ gain, $b > 0$ to lose"
      ],
      "id": "3f7b375e-3cd6-4545-b09e-e8284587c4a7"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lambda = ComplexF64[-9.270990685613062e-18 + 0.0im]\n",
            "mean(phi) = 0.25"
          ]
        },
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "4×4 Tridiagonal{Float64, Vector{Float64}}:\n",
              " -0.1   0.05    ⋅      ⋅ \n",
              "  0.1  -0.15   0.05    ⋅ \n",
              "   ⋅    0.1   -0.15   0.05\n",
              "   ⋅     ⋅     0.1   -0.05"
            ]
          }
        }
      ],
      "source": [
        "N = 4\n",
        "a = 0.1\n",
        "b = 0.05\n",
        "Q = Tridiagonal(fill(b, N-1),\n",
        "                [-a; fill(-(a + b), N-2); -b],\n",
        "                fill(a, N-1))\n",
        "# Find smallest magnitude eigenvalue (i.e. 0)\n",
        "lambda, phi = eigs( Q', nev = 1, which = :SM, maxiter = 1000)\n",
        "phi = real(phi) ./ sum(real(phi))\n",
        "@show lambda\n",
        "@show mean(phi);\n",
        "Q'"
      ],
      "id": "5a4d407c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Implementing Matrix-Free Operator for Adjoint"
      ],
      "id": "d9041dff-fb4e-4f98-ac11-ff3a63ba0179"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "0.0"
            ]
          }
        }
      ],
      "source": [
        "function Q_adj_product(x)\n",
        "    Q_x = zero(x)\n",
        "    Q_x[1] = -a * x[1] + b * x[2]\n",
        "    for i = 2:(N-1)\n",
        "        Q_x[i] = a * x[i-1] - (a + b) * x[i] + b * x[i+1]\n",
        "    end\n",
        "    Q_x[N] = a * x[N-1] - b * x[N]\n",
        "    return Q_x\n",
        "end\n",
        "x_check = rand(N)\n",
        "norm(Q_adj_product(x_check) - Q' * x_check)"
      ],
      "id": "eb401968"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Solving with a Wrapper for the Matrix-Free Operator\n",
        "\n",
        "-   The `LinearMap` wrapper adds features required for algorithms\n",
        "    (e.g. `size(Q_adj_map` and `Q_adj_map * v` overloads)"
      ],
      "id": "05db1bba-83c0-43e2-8bb9-a51c2f875ad4"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lambda = ComplexF64[1.6525994004046e-17 + 0.0im]\n",
            "mean(phi) = 0.25"
          ]
        }
      ],
      "source": [
        "Q_adj_map = LinearMap(Q_adj_product, N)\n",
        "# Get smallest magnitude only using the Q'(x) map\n",
        "lambda, phi = eigs(Q_adj_map, nev = 1, which = :SM, maxiter = 1000)\n",
        "phi = real(phi) ./ sum(real(phi))\n",
        "@show lambda\n",
        "@show mean(phi);"
      ],
      "id": "24676973"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Preconditioning\n",
        "\n",
        "## Changing the Geometry\n",
        "\n",
        "-   In practice, most Krylov methods are preconditioned in practice or\n",
        "    else direct methods usually dominate. Same with large nonlinear\n",
        "    systems\n",
        "-   As discussed, the key issue for the convergence speed of iterative\n",
        "    methods is the geometry (e.g. condition number of hessian, etc)\n",
        "-   Preconditioning changes the geometry. e.g. more like circles or with\n",
        "    eigenvalue problems spread out the eigenvalues of interest\n",
        "-   Preconditioners for a matrix $A$ requires art and tradeoffs\n",
        "    -   Want be relatively cheap to calculate, and must be invertible\n",
        "    -   Want to have $\\text{cond}(P A) \\ll \\text{cond}(A)$\n",
        "-   Ideal preconditioner for $A x = b$ is $P=A^{-1}$ since\n",
        "    $A^{-1} A x = x = A^{-1} b$\n",
        "    -   $\\text{cond}(A^{-1}A)=1$! But that is equivalent to solving\n",
        "        problem\n",
        "\n",
        "## Right-Preconditioning a Linear System\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "A x &= b\\\\\n",
        "A P^{-1} P x &= b\\\\\n",
        "A P^{-1} y &= b\\\\\n",
        "P x &= y\n",
        "\\end{aligned}\n",
        "$$ That is, solve $(A P^{-1})y = b$ for $y$, and then solve $P x = y$\n",
        "for $x$.\n",
        "\n",
        "## Raw Conjugate Gradient"
      ],
      "id": "1b934d50-60c8-497e-803a-5996886feec1"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cond(Matrix(A)) = 972303.1119375983"
          ]
        },
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "Converged after 389 iterations."
            ]
          }
        }
      ],
      "source": [
        "N = 200\n",
        "A = sprand(N, N, 0.1)   # 10 percent non-zeros\n",
        "A = A * A'\n",
        "b = rand(N)\n",
        "@show cond(Matrix(A))\n",
        "sol = cg(A, b, log = true, maxiter = 1000)\n",
        "sol[end]"
      ],
      "id": "1bb0846d"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Diagonal Preconditioner\n",
        "\n",
        "-   A simple preconditioner is the diagonal of $A$\n",
        "-   This is cheap to calculate, and is invertible if the diagonal has no\n",
        "    zeros\n",
        "-   For some problems this has a huge impact on convergence/condition,\n",
        "    for others it does nothing"
      ],
      "id": "729d9e53-bdd2-4b55-8ecd-02c6b6c68558"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "Converged after 367 iterations."
            ]
          }
        }
      ],
      "source": [
        "P = DiagonalPreconditioner(A)\n",
        "sol = cg(A, b; Pl = P, log = true, maxiter = 1000)\n",
        "sol[end]"
      ],
      "id": "d1b6705e"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Incomplete LU or Cholesky\n",
        "\n",
        "-   Iterate part of the way on an LU or Cholesky factorization\n",
        "-   Not the total inverse, but can make a big difference"
      ],
      "id": "3e305ba7-6b9a-41f9-a3ca-d601606dd766"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "Converged after 151 iterations."
            ]
          }
        }
      ],
      "source": [
        "P = ilu(A, τ = 0.1)\n",
        "sol = cg(A, b; Pl = P, log = true, maxiter = 1000)\n",
        "sol[end]"
      ],
      "id": "becb05c0"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Others\n",
        "\n",
        "-   In the above we aren’t getting huge gains, but it is also lacking\n",
        "    structure\n",
        "-   If you have problems with multiple scales, as might come out of\n",
        "    discretizing multiple dimensions in a statepsace, see\n",
        "    [multigrid](https://en.wikipedia.org/wiki/Multigrid_method) methods\n",
        "    -   Algebraic Multigrid preconditioner is often useful even outside\n",
        "        of having different scales\n",
        "-   Other preconditioners include ones intended for [Graph\n",
        "    Laplacians](https://github.com/danspielman/Laplacians.jl) such as\n",
        "    approximate cholesky decompositions and combinatorial multigrid\n",
        "    preconditioners.\n",
        "    -   See [paper](https://arxiv.org/abs/2303.00709) for more"
      ],
      "id": "5b633fc9-fb6a-424e-aec2-c7154d0449d0"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "julia-1.10",
      "display_name": "Julia 1.10.4",
      "language": "julia"
    },
    "language_info": {
      "name": "julia",
      "file_extension": ".jl",
      "mimetype": "application/julia",
      "version": "1.10.4"
    }
  }
}