<!DOCTYPE html>
<html lang="en"><head>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-html/tabby.min.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/light-border.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-html.min.css" rel="stylesheet" data-mode="light">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.3.450">

  <meta name="author" content="Jesse Perla">
  <title>ECON 622 Lectures - ECON622: Computational Economics with Data Science Applications</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="../site_libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="../site_libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
    /* CSS for syntax highlighting */
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      { color: #003b4f; background-color: #f1f3f5; }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span { color: #003b4f; } /* Normal */
    code span.al { color: #ad0000; } /* Alert */
    code span.an { color: #5e5e5e; } /* Annotation */
    code span.at { color: #657422; } /* Attribute */
    code span.bn { color: #ad0000; } /* BaseN */
    code span.bu { } /* BuiltIn */
    code span.cf { color: #003b4f; } /* ControlFlow */
    code span.ch { color: #20794d; } /* Char */
    code span.cn { color: #8f5902; } /* Constant */
    code span.co { color: #5e5e5e; } /* Comment */
    code span.cv { color: #5e5e5e; font-style: italic; } /* CommentVar */
    code span.do { color: #5e5e5e; font-style: italic; } /* Documentation */
    code span.dt { color: #ad0000; } /* DataType */
    code span.dv { color: #ad0000; } /* DecVal */
    code span.er { color: #ad0000; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #ad0000; } /* Float */
    code span.fu { color: #4758ab; } /* Function */
    code span.im { color: #00769e; } /* Import */
    code span.in { color: #5e5e5e; } /* Information */
    code span.kw { color: #003b4f; } /* Keyword */
    code span.op { color: #5e5e5e; } /* Operator */
    code span.ot { color: #003b4f; } /* Other */
    code span.pp { color: #ad0000; } /* Preprocessor */
    code span.sc { color: #5e5e5e; } /* SpecialChar */
    code span.ss { color: #20794d; } /* SpecialString */
    code span.st { color: #20794d; } /* String */
    code span.va { color: #111111; } /* Variable */
    code span.vs { color: #20794d; } /* VerbatimString */
    code span.wa { color: #5e5e5e; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="../site_libs/revealjs/dist/theme/quarto.css">
  <link href="../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-chalkboard/font-awesome/css/all.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-chalkboard/style.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">

  .callout {
    margin-top: 1em;
    margin-bottom: 1em;  
    border-radius: .25rem;
  }

  .callout.callout-style-simple { 
    padding: 0em 0.5em;
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
    display: flex;
  }

  .callout.callout-style-default {
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
  }

  .callout .callout-body-container {
    flex-grow: 1;
  }

  .callout.callout-style-simple .callout-body {
    font-size: 1rem;
    font-weight: 400;
  }

  .callout.callout-style-default .callout-body {
    font-size: 0.9rem;
    font-weight: 400;
  }

  .callout.callout-titled.callout-style-simple .callout-body {
    margin-top: 0.2em;
  }

  .callout:not(.callout-titled) .callout-body {
      display: flex;
  }

  .callout:not(.no-icon).callout-titled.callout-style-simple .callout-content {
    padding-left: 1.6em;
  }

  .callout.callout-titled .callout-header {
    padding-top: 0.2em;
    margin-bottom: -0.2em;
  }

  .callout.callout-titled .callout-title  p {
    margin-top: 0.5em;
    margin-bottom: 0.5em;
  }
    
  .callout.callout-titled.callout-style-simple .callout-content  p {
    margin-top: 0;
  }

  .callout.callout-titled.callout-style-default .callout-content  p {
    margin-top: 0.7em;
  }

  .callout.callout-style-simple div.callout-title {
    border-bottom: none;
    font-size: .9rem;
    font-weight: 600;
    opacity: 75%;
  }

  .callout.callout-style-default  div.callout-title {
    border-bottom: none;
    font-weight: 600;
    opacity: 85%;
    font-size: 0.9rem;
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-default div.callout-content {
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-simple .callout-icon::before {
    height: 1rem;
    width: 1rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 1rem 1rem;
  }

  .callout.callout-style-default .callout-icon::before {
    height: 0.9rem;
    width: 0.9rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 0.9rem 0.9rem;
  }

  .callout-title {
    display: flex
  }
    
  .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  .callout.no-icon::before {
    display: none !important;
  }

  .callout.callout-titled .callout-body > .callout-content > :last-child {
    margin-bottom: 0.5rem;
  }

  .callout.callout-titled .callout-icon::before {
    margin-top: .5rem;
    padding-right: .5rem;
  }

  .callout:not(.callout-titled) .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  /* Callout Types */

  div.callout-note {
    border-left-color: #4582ec !important;
  }

  div.callout-note .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEU0lEQVRYCcVXTWhcVRQ+586kSUMMxkyaElstCto2SIhitS5Ek8xUKV2poatCcVHtUlFQk8mbaaziwpWgglJwVaquitBOfhQXFlqlzSJpFSpIYyXNjBNiTCck7x2/8/LeNDOZxDuEkgOXe++553zfefee+/OYLOXFk3+1LLrRdiO81yNqZ6K9cG0P3MeFaMIQjXssE8Z1JzLO9ls20MBZX7oG8w9GxB0goaPrW5aNMp1yOZIa7Wv6o2ykpLtmAPs/vrG14Z+6d4jpbSKuhdcSyq9wGMPXjonwmESXrriLzFGOdDBLB8Y6MNYBu0dRokSygMA/mrun8MGFN3behm6VVAwg4WR3i6FvYK1T7MHo9BK7ydH+1uurECoouk5MPRyVSBrBHMYwVobG2aOXM07sWrn5qgB60rc6mcwIDJtQrnrEr44kmy+UO9r0u9O5/YbkS9juQckLed3DyW2XV/qWBBB3ptvI8EUY3I9p/67OW+g967TNr3Sotn3IuVlfMLVnsBwH4fsnebJvyGm5GeIUA3jljERmrv49SizPYuq+z7c2H/jlGC+Ghhupn/hcapqmcudB9jwJ/3jvnvu6vu5lVzF1fXyZuZZ7U8nRmVzytvT+H3kilYvH09mLWrQdwFSsFEsxFVs5fK7A0g8gMZjbif4ACpKbjv7gNGaD8bUrlk8x+KRflttr22JEMRUbTUwwDQScyzPgedQHZT0xnx7ujw2jfVfExwYHwOsDTjLdJ2ebmeQIlJ7neo41s/DrsL3kl+W2lWvAga0tR3zueGr6GL78M3ifH0rGXrBC2aAR8uYcIA5gwV8zIE8onoh8u0Fca/ciF7j1uOzEnqcIm59sEXoGc0+z6+H45V1CvAvHcD7THztu669cnp+L0okAeIc6zjbM/24LgGM1gZk7jnRu1aQWoU9sfUOuhrmtaPIO3YY1KLLWZaEO5TKUbMY5zx8W9UJ6elpLwKXbsaZ4EFl7B4bMtDv0iRipKoDQT2sNQI9b1utXFdYisi+wzZ/ri/1m7QfDgEuvgUUEIJPq3DhX/5DWNqIXDOweC2wvIR90Oq3lDpdMIgD2r0dXvGdsEW5H6x6HLRJYU7C69VefO1x8Gde1ZFSJLfWS1jbCnhtOPxmpfv2LXOA2Xk2tvnwKKPFuZ/oRmwBwqRQDcKNeVQkYcOjtWVBuM/JuYw5b6isojIkYxyYAFn5K7ZBF10fea52y8QltAg6jnMqNHFBmGkQ1j+U43HMi2xMar1Nv0zGsf1s8nUsmUtPOOrbFIR8bHFDMB5zL13Gmr/kGlCkUzedTzzmzsaJXhYawnA3UmARpiYj5ooJZiUoxFRtK3X6pgNPv+IZVPcnwbOl6f+aBaO1CNvPW9n9LmCp01nuSaTRF2YxHqZ8DYQT6WsXT+RD6eUztwYLZ8rM+rcPxamv1VQzFUkzFXvkiVrySGQgJNvXHJAxiU3/NwiC03rSf05VBaPtu/Z7/B8Yn/w7eguloAAAAAElFTkSuQmCC');
  }

  div.callout-note.callout-style-default .callout-title {
    background-color: #dae6fb
  }

  div.callout-important {
    border-left-color: #d9534f !important;
  }

  div.callout-important .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEKklEQVRYCcVXTWhcVRS+575MJym48A+hSRFr00ySRQhURRfd2HYjk2SSTokuBCkU2o0LoSKKraKIBTcuFCoidGFD08nkBzdREbpQ1EDNIv8qSGMFUboImMSZd4/f9zJv8ibJMC8xJQfO3HPPPef7zrvvvnvviIkpC9nsw0UttFunbUhpFzFtarSd6WJkStVMw5xyVqYTvkwfzuf/5FgtkVoB0729j1rjXwThS7Vio+Mo6DNnvLfahoZ+i/o32lULuJ3NNiz7q6+pyAUkJaFF6JwaM2lUJlV0MlnQn5aTRbEu0SEqHUa0A4AdiGuB1kFXRfVyg5d87+Dg4DL6m2TLAub60ilj7A1Ec4odSAc8X95sHh7+ZRPCFo6Fnp7HfU/fBng/hi10CjCnWnJjsxvDNxWw0NfV6Rv5GgP3I3jGWXumdTD/3cbEOP2ZbOZp69yniG3FQ9z1jD7bnBu9Fc2tKGC2q+uAJOQHBDRiZX1x36o7fWBs7J9ownbtO+n0/qWkvW7UPIfc37WgT6ZGR++EOJyeQDSb9UB+DZ1G6DdLDzyS+b/kBCYGsYgJbSQHuThGKRcw5xdeQf8YdNHsc6ePXrlSYMBuSIAFTGAtQo+VuALo4BX83N190NWZWbynBjhOHsmNfFWLeL6v+ynsA58zDvvAC8j5PkbOcXCMg2PZFk3q8MjI7WAG/Dp9AwP7jdGBOOQkAvlFUB+irtm16I1Zw9YBcpGTGXYmk3kQIC/Cds55l+iMI3jqhjAuaoe+am2Jw5GT3Nbz3CkE12NavmzN5+erJW7046n/CH1RO/RVa8lBLozXk9uqykkGAyRXLWlLv5jyp4RFsG5vGVzpDLnIjTWgnRy2Rr+tDKvRc7Y8AyZq10jj8DqXdnIRNtFZb+t/ZRtXcDiVnzpqx8mPcDWxgARUqx0W1QB9MeUZiNrV4qP+Ehc+BpNgATsTX8ozYKL2NtFYAHc84fG7ndxUPr+AR/iQSns7uSUufAymwDOb2+NjK27lEFocm/EE2WpyIy/Hi66MWuMKJn8RvxIcj87IM5Vh9663ziW36kR0HNenXuxmfaD8JC7tfKbrhFr7LiZCrMjrzTeGx+PmkosrkNzW94ObzwocJ7A1HokLolY+AvkTiD/q1H0cN48c5EL8Crkttsa/AXQVDmutfyku0E7jShx49XqV3MFK8IryDhYVbj7Sj2P2eBxwcXoe8T8idsKKPRcnZw1b+slFTubwUwhktrfnAt7J++jwQtLZcm3sr9LQrjRzz6cfMv9aLvgmnAGvpoaGLxM4mAEaLV7iAzQ3oU0IvD5x9ix3yF2RAAuYAOO2f7PEFWCXZ4C9Pb2UsgDeVnFSpbFK7/IWu7TPTvBqzbGdCHOJQSxiEjt6IyZmxQyEJHv6xyQsYk//moVFsN2zP6fRImjfq7/n/wFDguUQFNEwugAAAABJRU5ErkJggg==');
  }

  div.callout-important.callout-style-default .callout-title {
    background-color: #f7dddc
  }

  div.callout-warning {
    border-left-color: #f0ad4e !important;
  }

  div.callout-warning .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAETklEQVRYCeVWW2gcVRg+58yaTUnizqbipZeX4uWhBEniBaoUX1Ioze52t7sRq6APio9V9MEaoWlVsFasRq0gltaAPuxms8lu0gcviE/FFOstVbSIxgcv6SU7EZqmdc7v9+9mJtNks51NTUH84ed889/PP+cmxP+d5FIbMJmNbpREu4WUkiTtCicKny0l1pIKmBzovF2S+hIJHX8iEu3hZJ5lNZGqyRrGSIQpq15AzF28jgpeY6yk6GVdrfFqdrD6Iw+QlB8g0YS2g7dyQmXM/IDhBhT0UCiRf59lfqmmDvzRt6kByV/m4JjtzuaujMUM2c5Z2d6JdKrRb3K2q6mA+oYVz8JnDdKPmmNthzkAk/lN63sYPgevrguc72aZX/L9C6x09GYyxBgCX4NlvyGUHOKELlm5rXeR1kchuChJt4SSwyddZRXgvwMGvYo4QSlk3/zkHD8UHxwVJA6zjZZqP8v8kK8OWLnIZtLyCAJagYC4rTGW/9Pqj92N/c+LUaAj27movwbi19tk/whRCIE7Q9vyI6yvRpftAKVTdUjOW40X3h5OXsKCdmFcx0xlLJoSuQngnrJe7Kcjm4OMq9FlC7CMmScQANuNvjfP3PjGXDBaUQmbp296S5L4DrpbrHN1T87ZVEZVCzg1FF0Ft+dKrlLukI+/c9ENo+TvlTDbYFvuKPtQ9+l052rXrgKoWkDAFnvh0wTOmYn8R5f4k/jN/fZiCM1tQx9jQQ4ANhqG4hiL0qIFTGViG9DKB7GYzgubnpofgYRwO+DFjh0Zin2m4b/97EDkXkc+f6xYAPX0KK2I/7fUQuwzuwo/L3AkcjugPNixC8cHf0FyPjWlItmLxWw4Ou9YsQCr5fijMGoD/zpdRy95HRysyXA74MWOnscpO4j2y3HAVisw85hX5+AFBRSHt4ShfLFkIMXTqyKFc46xdzQM6XbAi702a7sy04J0+feReMFKp5q9esYLCqAZYw/k14E/xcLLsFElaornTuJB0svMuJINy8xkIYuL+xPAlWRceH6+HX7THJ0djLUom46zREu7tTkxwmf/FdOZ/sh6Q8qvEAiHpm4PJ4a/doJe0gH1t+aHRgCzOvBvJedEK5OFE5jpm4AGP2a8Dxe3gGJ/pAutug9Gp6he92CsSsWBaEcxGx0FHytmIpuqGkOpldqNYQK8cSoXvd+xLxXADw0kf6UkJNFtdo5MOgaLjiQOQHcn+A6h5NuL2s0qsC2LOM75PcF3yr5STuBSAcGG+meA14K/CI21HcS4LBT6tv0QAh8Dr5l93AhZzG5ZJ4VxAqdZUEl9z7WJ4aN+svMvwHHL21UKTd1mqvChH7/Za5xzXBBKrUcB0TQ+Ulgkfbi/H/YT5EptrGzsEK7tR1B7ln9BBwckYfMiuSqklSznIuoIIOM42MQO+QnduCoFCI0bpkzjCjddHPN/F+2Yu+sd9bKNpVwHhbS3LluK/0zgfwD0xYI5dXuzlQAAAABJRU5ErkJggg==');
  }

  div.callout-warning.callout-style-default .callout-title {
    background-color: #fcefdc
  }

  div.callout-tip {
    border-left-color: #02b875 !important;
  }

  div.callout-tip .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAADr0lEQVRYCe1XTWgTQRj9ZjZV8a9SPIkKgj8I1bMHsUWrqYLVg4Ue6v9BwZOxSYsIerFao7UiUryIqJcqgtpimhbBXoSCVxUFe9CTiogUrUp2Pt+3aUI2u5vdNh4dmMzOzHvvezuz8xNFM0mjnbXaNu1MvFWRXkXEyE6aYOYJpdW4IXuA4r0fo8qqSMDBU0v1HJUgVieAXxzCsdE/YJTdFcVIZQNMyhruOMJKXYFoLfIfIvVIMWdsrd+Rpd86ZmyzzjJmLStqRn0v8lzkb4rVIXvnpScOJuAn2ACC65FkPzEdEy4TPWRLJ2h7z4cArXzzaOdKlbOvKKX25Wl00jSnrwVxAg3o4dRxhO13RBSdNvH0xSARv3adTXbBdTf64IWO2vH0LT+cv4GR1DJt+DUItaQogeBX/chhbTBxEiZ6gftlDNXTrvT7co4ub5A6gp9HIcHvzTa46OS5fBeP87Qm0fQkr4FsYgVQ7Qg+ZayaDg9jhg1GkWj8RG6lkeSacrrHgDaxdoBiZPg+NXV/KifMuB6//JmYH4CntVEHy/keA6x4h4CU5oFy8GzrBS18cLJMXcljAKB6INjWsRcuZBWVaS3GDrqB7rdapVIeA+isQ57Eev9eCqzqOa81CY05VLd6SamW2wA2H3SiTbnbSxmzfp7WtKZkqy4mdyAlGx7ennghYf8voqp9cLSgKdqNfa6RdRsAAkPwRuJZNbpByn+RrJi1RXTwdi8RQF6ymDwGMAtZ6TVE+4uoKh+MYkcLsT0Hk8eAienbiGdjJHZTpmNjlbFJNKDVAp2fJlYju6IreQxQ08UJDNYdoLSl6AadO+fFuCQqVMB1NJwPm69T04Wv5WhfcWyfXQB+wXRs1pt+nCknRa0LVzSA/2B+a9+zQJadb7IyyV24YAxKp2Jqs3emZTuNnKxsah+uabKbMk7CbTgJx/zIgQYErIeTKRQ9yD9wxVof5YolPHqaWo7TD6tJlh7jQnK5z2n3+fGdggIOx2kaa2YI9QWarc5Ce1ipNWMKeSG4DysFF52KBmTNMmn5HqCFkwy34rDg05gDwgH3bBi+sgFhN/e8QvRn8kbamCOhgrZ9GJhFDgfcMHzFb6BAtjKpFhzTjwv1KCVuxHvCbsSiEz4CANnj84cwHdFXAbAOJ4LTSAawGWFn5tDhLMYz6nWeU2wJfIhmIJBefcd/A5FWQWGgrWzyORZ3Q6HuV+Jf0Bj+BTX69fm1zWgK7By1YTXchFDORywnfQ7GpzOo6S+qECrsx2ifVQAAAABJRU5ErkJggg==');
  }

  div.callout-tip.callout-style-default .callout-title {
    background-color: #ccf1e3
  }

  div.callout-caution {
    border-left-color: #fd7e14 !important;
  }

  div.callout-caution .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAACV0lEQVRYCdVWzWoUQRCuqp2ICBLJXgITZL1EfQDBW/bkzUMUD7klD+ATSHBEfAIfQO+iXsWDxJsHL96EHAwhgzlkg8nBg25XWb0zIb0zs9muYYWkoKeru+vn664fBqElyZNuyh167NXJ8Ut8McjbmEraKHkd7uAnAFku+VWdb3reSmRV8PKSLfZ0Gjn3a6Xlcq9YGb6tADjn+lUfTXtVmaZ1KwBIvFI11rRXlWlatwIAAv2asaa9mlB9wwygiDX26qaw1yYPzFXg2N1GgG0FMF8Oj+VIx7E/03lHx8UhvYyNZLN7BwSPgekXXLribw7w5/c8EF+DBK5idvDVYtEEwMeYefjjLAdEyQ3M9nfOkgnPTEkYU+sxMq0BxNR6jExrAI31H1rzvLEfRIdgcv1XEdj6QTQAS2wtstEALLG1yEZ3QhH6oDX7ExBSFEkFINXH98NTrme5IOaaA7kIfiu2L8A3qhH9zRbukdCqdsA98TdElyeMe5BI8Rs2xHRIsoTSSVFfCFCWGPn9XHb4cdobRIWABNf0add9jakDjQJpJ1bTXOJXnnRXHRf+dNL1ZV1MBRCXhMbaHqGI1JkKIL7+i8uffuP6wVQAzO7+qVEbF6NbS0LJureYcWXUUhH66nLR5rYmva+2tjRFtojkM2aD76HEGAD3tPtKM309FJg5j/K682ywcWJ3PASCcycH/22u+Bh7Aa0ehM2Fu4z0SAE81HF9RkB21c5bEn4Dzw+/qNOyXr3DCTQDMBOdhi4nAgiFDGCinIa2owCEChUwD8qzd03PG+qdW/4fDzjUMcE1ZpIAAAAASUVORK5CYII=');
  }

  div.callout-caution.callout-style-default .callout-title {
    background-color: #ffe5d0
  }

  </style>
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
  <style>
  .reveal .custom3070 > div.column:first-child {
    width: 30%;
  }
  .reveal .custom3070 div.column:not(:first-child) {
    width: 70%;
  }
  .reveal .custom4060 > div.column:first-child {
    width: 40%;
  }
  .reveal .custom4060 div.column:not(:first-child) {
    width: 60%;
  }
  </style>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">ECON622: Computational Economics with Data Science Applications</h1>
  <p class="subtitle">Introduction to Kernel Methods and Gaussian Processes</p>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Jesse Perla 
</div>
<div class="quarto-title-author-email">
<a href="mailto:jesse.perla@ubc.ca">jesse.perla@ubc.ca</a>
</div>
        <p class="quarto-title-affiliation">
            University of British Columbia
          </p>
    </div>
</div>

</section><section id="TOC">
<nav role="doc-toc"> 
<h2 id="toc-title">Table of contents</h2>
<ul>
<li><a href="#/overview" id="/toc-overview">Overview</a></li>
<li><a href="#/normal-equations" id="/toc-normal-equations">Normal Equations</a></li>
<li><a href="#/features-and-the-kernel-trick" id="/toc-features-and-the-kernel-trick">Features and The Kernel Trick</a></li>
<li><a href="#/kernels" id="/toc-kernels">Kernels</a></li>
<li><a href="#/gaussian-processes" id="/toc-gaussian-processes">Gaussian Processes</a></li>
<li><a href="#/gpytorch" id="/toc-gpytorch">GPytorch</a></li>
</ul>
</nav>
</section>
<section>
<section id="overview" class="title-slide slide level1 center">
<h1>Overview</h1>

</section>
<section id="motivation" class="slide level2">
<h2>Motivation</h2>
<ul>
<li>We have discussed various forms of representation learning and function approximation including neural networks, LLS, etc.</li>
<li>Another type of approach is called “Instance (or Exemplar) Based Learning” of which Kernel Methods and Gaussian Processes are a special case
<ul>
<li>The idea there is that we find approximations from the data by keeping track of all observations and use them for inference</li>
<li>These methods are non-parametric in the sense that the number of parameters grows with the number of data points</li>
<li>Often Bayesian in practice, which provides regularization and inference</li>
</ul></li>
</ul>
</section>
<section id="limitations-and-advantages" class="slide level2">
<h2>Limitations and Advantages</h2>
<ul>
<li>While it seems like this is limiting to cases with small amounts of data
<ul>
<li>We have already explored high-dimensional linear algebra methods (e.g., matrix-free, iterative methods, preconditioning)</li>
<li>Many problems in economics do not have an enormous number of observations, or we have choice on where the observations occur</li>
</ul></li>
<li>Typically these methods are used without any “representation learning”, but at the end we will give some pointers to how these methods generalize to allow representation learning</li>
<li>One major benefit of these methods to economists is that they will provide <strong>posteriors in a function space</strong>, which we can use for inference</li>
</ul>
</section>
<section id="references" class="slide level2">
<h2>References</h2>
<ul>
<li>These notes are a bare-bones introduction. Look to other references</li>
<li><a href="https://probml.github.io/pml-book/book1.html">ProbML Book 1</a> Section 17 and <a href="https://probml.github.io/pml-book/book2.html">ProbML Book 2</a> Section 18</li>
<li><a href="http://gaussianprocess.org/gpml/chapters/">Gaussian Processes for Machine Learning</a> a beloved textbook available online</li>
<li><a href="https://www.cs.ubc.ca/~schmidtm/Courses/340-F22/L22.pdf">UBC CPSC 340</a> and <a href="https://www.cs.ubc.ca/~schmidtm/Courses/5XX-S22/S8.5.pdf">Mark Schmidt’s Topics Notes</a></li>
<li><a href="https://members.cbio.mines-paristech.fr/~jvert/svn/kernelcourse/slides/master2017/master2017.pdf">Machine Learning with Kernel Methods</a> course with <a href="https://www.youtube.com/channel/UCotztBOmGVl9pPGIN4YqcRw/videos">youtube lectures</a></li>
</ul>
</section></section>
<section>
<section id="normal-equations" class="title-slide slide level1 center">
<h1>Normal Equations</h1>

</section>
<section id="reminder-normal-equations-for-ridge-regression" class="slide level2">
<h2>Reminder: Normal Equations for Ridge Regression</h2>
<ul>
<li><p>Let <span class="math inline">\(x \in \mathbb{R}^M\)</span> be observations and <span class="math inline">\(X \in \mathbb{R}^{N \times M}\)</span> be the “design” matrix</p></li>
<li><p>To solve <span class="math inline">\(\min_{\theta}\left\{||X \theta - y||^2 + \frac{\lambda}{2}||\theta||^2\right\}\)</span> for <span class="math inline">\(\lambda \geq 0\)</span>, form normal eqs.</p>
<p><span class="math display">\[
  \theta = (X^{\top}X + \lambda I)^{-1} X^{\top} y
  \]</span></p>
<ul>
<li>The <span class="math inline">\(X^{\top}X \in \mathbb{R}^{M\times M}\)</span> is in the “feature” space and of size <span class="math inline">\(M\)</span></li>
</ul></li>
</ul>
</section>
<section id="the-other-normal-equations" class="slide level2">
<h2>The “Other” Normal Equations</h2>
<ul>
<li><p>Without proof, terms out you can rearrange this expression to get the “other” normal equations</p>
<p><span class="math display">\[
  \theta = X^{\top} (X X^{\top} + \lambda I)^{-1} y
  \]</span></p>
<ul>
<li><span class="math inline">\(X X^{\top} \in \mathbb{R}^{N\times N}\)</span> is called the Gram matrix</li>
<li><span class="math inline">\([X X^{\top}]_{ij} = x_i \cdot x_j\)</span>. i.e., pairwise comparisons of all data</li>
</ul></li>
<li><p>Since <span class="math inline">\(M \ll N\)</span> typically, often a bigger system to solve</p>
<ul>
<li>But note that we we have rewritten as pairwise comparisons</li>
</ul></li>
</ul>
</section>
<section id="prediction-with-kernels" class="slide level2">
<h2>Prediction with Kernels</h2>
<ul>
<li>Define <span class="math inline">\(\mathcal{K}(x,x') \equiv x \cdot x'\)</span>, then</li>
</ul>
<p><span class="math display">\[
K_{ij} \equiv [X X^{\top}]_{ij} = \mathcal{K}(x_i, x_j)\,\text{ for i = 1, ..., N and j = 1, ..., N}
\]</span></p>
<ul>
<li>Now, assume we received <span class="math inline">\(P\)</span> new points of data <span class="math inline">\(\tilde{X}\)</span> and want to predict <span class="math inline">\(\tilde{y}\)</span></li>
</ul>
<p><span class="math display">\[
\tilde{K}_{ij} \equiv \mathcal{K}(\tilde{x}_i, x_j),\,\text{ for i = 1, ..., P and j = 1, ..., N}
\]</span></p>
<ul>
<li>Finally (see linked derivations), can predict with</li>
</ul>
<p><span class="math display">\[
\tilde{y} = \tilde{K} (K + \lambda I)^{-1} y
\]</span></p>
</section>
<section id="are-working-with-pairwise-comparisons-better" class="slide level2">
<h2>Are Working with Pairwise Comparisons Better?</h2>
<ul>
<li>We have rewritten the normal equations in terms of pairwise comparisons (the Gram matrix) using some function <span class="math inline">\(\mathcal{K}(x,x')\equiv x \cdot x'\)</span></li>
<li>Is this computationally preferable for standard applications of OLS?
<ul>
<li>Probably not unless <span class="math inline">\(M\)</span> is very large</li>
</ul></li>
<li>The real advantage is that by rewriting the problem in terms of pairwise comparisons we can generalize to other functions <span class="math inline">\(\mathcal{K}(x,x')\)</span>
<ul>
<li>This is the basis of kernel methods and Gaussian processes</li>
</ul></li>
</ul>
</section></section>
<section>
<section id="features-and-the-kernel-trick" class="title-slide slide level1 center">
<h1>Features and The Kernel Trick</h1>

</section>
<section id="motivation-from-classification" class="slide level2">
<h2>Motivation from Classification</h2>
<ul>
<li>One starting point here is to consider simple problems of separating hyperplanes and binary classification</li>
<li>If we transform our data, sometimes into higher dimensions, then we can find separating hyperplanes
<ul>
<li>i.e., with the right “features”, many problems become linear</li>
</ul></li>
<li>See <a href="https://www.cs.ubc.ca/~schmidtm/Courses/340-F22/L22.pdf">CPSC 340</a></li>
</ul>
</section>
<section id="transforming-with-polynomials" class="slide level2">
<h2>Transforming with Polynomials</h2>

<img data-src="figures/polynomial_classification.png" class="r-stretch quarto-figure-center"><p class="caption">https://members.cbio.mines-paristech.fr/~jvert/svn/kernelcourse/slides/master2017/master2017.pdf</p></section>
<section id="do-we-need-to-form-the-features" class="slide level2">
<h2>Do we Need to Form the Features?</h2>
<ul>
<li>In the previous with <span class="math inline">\(x \in \mathcal{D}\)</span>, we could have:
<ul>
<li>For all <span class="math inline">\(x\equiv \begin{bmatrix}x_1 &amp; x_2\end{bmatrix}^{\top}\)</span></li>
<li>Calculate <span class="math inline">\(x \mapsto \phi(x) \equiv \begin{bmatrix}x_1^2 &amp; \sqrt{2} x_1 x_2 &amp; x_2^2 &amp;\end{bmatrix}^{\top}\)</span></li>
<li>This is creating our new “features”</li>
<li>Then run standard classification algorithms on the new features</li>
</ul></li>
<li>Define <span class="math inline">\(\mathcal{K}(x,x') \equiv \phi(x) \cdot \phi(x')\)</span>
<ul>
<li>Then use <span class="math inline">\(\mathcal{K}(x,x')\)</span> rather than explicitly designing the <span class="math inline">\(\phi(\cdot)\)</span> or calculating <span class="math inline">\(\phi(x)\)</span> for <span class="math inline">\(x\in \mathcal{D}\)</span></li>
</ul></li>
</ul>
</section>
<section id="example-with-polynomial-kernels" class="slide level2">
<h2>Example with Polynomial Kernels</h2>
<ul>
<li><p>Generalizing our previous case, <span class="math inline">\(\mathcal{K}(x,x') \equiv (x \cdot x')^2\)</span>, to polynomial order <span class="math inline">\(p\)</span> with cross-products <span class="math display">\[
\mathcal{K}(x,x') \equiv (x \cdot x' + c)^p
\]</span></p></li>
<li><p>The dimensionality of the underlying <span class="math inline">\(\phi(x)\)</span> is combinatorial in <span class="math inline">\(p\)</span></p></li>
<li><p>But now we can just evaluate pairwise <span class="math inline">\(\mathcal{K}(x,x')\)</span>, just an inner product in <span class="math inline">\(M\)</span> independent of <span class="math inline">\(p\)</span>!</p></li>
<li><p>The cost will be that we need to store the <span class="math inline">\(N \times N\)</span> matrix of pairwise comparisons</p></li>
</ul>
</section>
<section id="the-kernel-trick" class="slide level2">
<h2>The Kernel Trick</h2>
<ul>
<li>Note in the previous examples that we never formed the <span class="math inline">\([\phi(x_i)]_{i=1}^N\)</span></li>
<li>Even further, we didn’t need to even write down the <span class="math inline">\(\phi(x)\)</span> function explicitly</li>
<li>This is called the “Kernel Trick” and is the basis of Kernel Methods
<ul>
<li>By choosing a <span class="math inline">\(\mathcal{K}(x,x')\)</span> and using “the other normal equations” or similar formulations in other algorithms, you implicitly define features <span class="math inline">\(\phi(x)\)</span></li>
<li>In fact, the <span class="math inline">\(\phi(x)\)</span> may be infinite dimensional!</li>
</ul></li>
<li>The downside is that this will require keeping track of all previous observations (as we did with the normal equations forming <span class="math inline">\(\tilde{K}\)</span>)</li>
</ul>
</section>
<section id="kernelized-methods" class="slide level2">
<h2>Kernelized Methods</h2>
<ul>
<li>Many ML methods are “kernelized”, which means that they are written in terms of pairwise comparisons for a given kernel
<ul>
<li>A kernelized regression just takes the standard <span class="math inline">\(\mathcal{K}(x,x') = x \cdot x'\)</span> and lets us swap out with other <span class="math inline">\(\mathcal{K}(x,x')\)</span></li>
</ul></li>
<li>For example, the <a href="https://en.wikipedia.org/wiki/Support_vector_machine">Support Vector Machines (SVM)</a> are just kernelized linear classifiers. There kernelized versions of PCA, etc.</li>
<li>These are powerful because they enable us to work with (often linear) methods in a richer feature space
<ul>
<li>But we have to choose the right sorts of kernels to deliver those features</li>
</ul></li>
<li>Next we build a little intuition on kernels before formalizing GPs</li>
</ul>
</section></section>
<section>
<section id="kernels" class="title-slide slide level1 center">
<h1>Kernels</h1>

</section>
<section id="kernels-as-similarity-measures" class="slide level2">
<h2>Kernels as Similarity Measures</h2>
<ul>
<li><p>Our canonical example is <span class="math inline">\(\mathcal{K}(x,x') = x \cdot x'\)</span>, which is a measure of similarity between <span class="math inline">\(x\)</span> and <span class="math inline">\(x'\)</span>. Note that <span class="math display">\[
  ||x - x'||_2 = \sqrt{\mathcal{K}(x, x) - 2 \mathcal{K}(x,x') + \mathcal{K}(x', x')}
  \]</span></p></li>
<li><p>Fix the lengths <span class="math inline">\(||x||_2 = ||x'||_2 = 1\)</span> then notice that <span class="math inline">\(||x - x'||_2 = \sqrt{2 (1 - \mathcal{K}(x,x'))}\)</span></p></li>
<li><p>As <span class="math inline">\(\mathcal{K}(x,x')\)</span> increases, the distance between <span class="math inline">\(x\)</span> and <span class="math inline">\(x'\)</span> decreases</p></li>
<li><p>Also notice that when <span class="math inline">\(\mathcal{K}(x,x') = 0\)</span> (i.e.&nbsp;orthogonal since <span class="math inline">\(x \cdot x' = 0\)</span>) we maximize the distance between <span class="math inline">\(x\)</span> and <span class="math inline">\(x'\)</span></p></li>
</ul>
</section>
<section id="gaussian-kernel" class="slide level2">
<h2>Gaussian Kernel</h2>
<ul>
<li>One of the most commonly used kernels is called the <strong>radial basis function(RBF)</strong> kernel, also known as the Gaussian kernel</li>
</ul>
<p><span class="math display">\[
\mathcal{K}(x, x';\ell) \equiv \exp(-\frac{||x - x'||_2^2}{2 \ell^2})
\]</span></p>
<ul>
<li>Note that this uses the Euclidean norm, but we could use other norms</li>
<li>The <span class="math inline">\(\ell\)</span> is a scale which determines the distances over which we expect differences to matter</li>
<li>Kernels which are only a function of some distance between <span class="math inline">\(x\)</span> and <span class="math inline">\(x'\)</span> (where the location is irrelevant) are called <strong>stationary kernels</strong></li>
</ul>
</section>
<section id="example-kernels" class="slide level2">
<h2>Example Kernels</h2>
<ul>
<li>See <a href="https://probml.github.io/pml-book/book2.html">ProbML Book 2</a> Section 18.2 for more and <a href="https://en.wikipedia.org/wiki/Mercer%27s_theorem">Mercer’s Theorem</a> - which shows all kernels are associated with an inner product in some orthonormal feature space</li>
<li>Polynomial kernel <span class="math inline">\(\mathcal{K}(x,x') = (x \cdot x' + c)^p\)</span> for <span class="math inline">\(p \in \mathbb{N}\)</span> the polynomial order, including <span class="math inline">\(p=1\)</span> which is just the linear kernel</li>
<li>Other kernels can be richer and depend on the data (e.g., for network, text, image data). Pick your <span class="math inline">\(\mathcal{K}(x,x')\)</span> to implement similarity measures</li>
<li>Can construct kernels from other kernels (e.g.&nbsp;<span class="math inline">\(\exp(\mathcal{K}(x,x'))\)</span> is a valid kernel)</li>
<li>Kernel functions could even be neural networks (Deep Kernels)</li>
<li>Kernels typically have parameters (e.g., <span class="math inline">\(\ell\)</span> in the Gaussian kernel) which must be fit - usually by maximizing the marginal likelihood</li>
</ul>
<!--
In future iterations, add stuff on the mercer kernel, positive definite kernel, etc.
-->
</section></section>
<section>
<section id="gaussian-processes" class="title-slide slide level1 center">
<h1>Gaussian Processes</h1>

</section>
<section id="parameter-space-vs.-function-space" class="slide level2">
<h2>Parameter Space vs.&nbsp;Function Space</h2>
<ul>
<li>There are two basic approaches to understanding Gaussian Processes (GPs)
<ol type="1">
<li>The parameter (or weight) space</li>
<li>The function space</li>
</ol></li>
<li>See the <a href="https://gaussianprocess.org/gpml/chapters/RW2.pdf">Gaussian Processes for Machine Learning</a> chapter 2 for more details</li>
<li>Given what you have learned about the kernel trick, you can probably understand a lot of the “parameter space” approach yourself.</li>
<li>Read both, but we will focus on the function space - which is its main advantage</li>
</ul>
</section>
<section id="gaussian-processes-1" class="slide level2">
<h2>Gaussian Processes</h2>
<ul>
<li>A Gaussian processes are <strong>random functions</strong> where for any finite set of points <span class="math inline">\(x_1, ..., x_N \in \mathcal{D}\)</span>, the marginal distribution <span class="math inline">\(f(x_1), ..., f(x_N)\)</span> is a multivariate Gaussian distribution</li>
<li>Another way to think about a GP is that it is a <a href="https://gaussianprocess.org/gpml/chapters/RW2.pdf">collection of random variables, any finite number of which have a joint Gaussian</a></li>
</ul>
</section>
<section id="why-random-functions" class="slide level2">
<h2>Why Random Functions?</h2>
<ul>
<li>The random functions with GPs enable us to have
<ul>
<li>distributions over functions</li>
<li>priors and posteriors in function spaces</li>
<li>meaningful notions of norms to compare functions and regularize them</li>
</ul></li>
<li><strong>Uncertainty Quantification</strong>: An issue with ERM-methods (including deep learning) is that they find a single function (think MLE or MAP solutions)
<ul>
<li>Alternatives like sample-splitting, cross-validation are limited in their ability to quantify uncertainty</li>
</ul></li>
<li><strong>Surrogates:</strong> GPs can be used to form probabilistic surrogate functions given data. Then we can use those surrogates for optimization, uncertainty quantification, etc.</li>
</ul>
</section>
<section id="bayesian-optimization-and-its-generalizations" class="slide level2">
<h2>Bayesian Optimization and Its Generalizations</h2>
<ul>
<li>One particularly useful application is Bayesian optimization</li>
<li>The idea, used in <a href="https://en.wikipedia.org/wiki/Kriging">Kriging</a> and many <a href="https://docs.wandb.ai/tutorials/sweeps">HPO methods</a>
<ul>
<li>For <span class="math inline">\(y = f(x)\)</span> function, which is expensive and blackbox</li>
<li>Take observables <span class="math inline">\((y_i, x_i)\)</span> and form a GP surrogate</li>
<li>Optimize that surrogate instead of the original function to get a new <span class="math inline">\(\tilde{x}\)</span></li>
<li>Evaluate the true function <span class="math inline">\(f(\tilde{x})\)</span> to get <span class="math inline">\(\tilde{y}\)</span></li>
<li>Update the surrogate with the new data and repeat</li>
</ul></li>
<li>The idea of bayesian optimization and information aquisition (i.e.&nbsp;the next point to evaluate) is very general and can be applied <a href="https://willieneis.github.io/bax-website/">to many problems</a></li>
</ul>
</section>
<section id="mean-and-covariance-functions" class="slide level2">
<h2>Mean and Covariance Functions</h2>
<ul>
<li><p>Unsurprisingly, since Gaussian random variables can be characterized by the first two moments, so can GPs</p></li>
<li><p>Let the mean function be</p>
<p><span class="math display">\[
m(x) = \mathbb{E}[f(x)]
\]</span></p></li>
<li><p>And the covariance function (kernel) be</p>
<p><span class="math display">\[
\mathcal{K}(x,x') = \mathbb{E}[(f(x) - m(x))(f(x') - m(x'))]
\]</span></p></li>
<li><p>Then we can denote a GP as</p>
<p><span class="math display">\[
f(x) \sim GP(m(x), \mathcal{K}(x,x'))
\]</span></p></li>
</ul>
</section>
<section id="probability-of-observables" class="slide level2">
<h2>Probability of Observables</h2>
<ul>
<li>Let <span class="math inline">\(f_X \equiv [f(x_i)]_{i=1}^N\)</span>, then because it is jointly Gaussian, we can write the probability of the observables as</li>
</ul>
<p><span class="math display">\[
\begin{aligned}
f_X\,|\,X &amp;\sim \mathcal{N}(\mu_X, K_{XX})\\
\mu_X &amp;\equiv [m(x_i)]_{i=1}^N\\
K_{XX} &amp;\equiv [\mathcal{K}(x_i, x_j)]_{i,j=1}^N
\end{aligned}
\]</span></p>
</section>
<section id="conditional-forecasts" class="slide level2">
<h2>Conditional Forecasts</h2>
<ul>
<li><p>Let <span class="math inline">\(X\)</span> and <span class="math inline">\(f_X\)</span> be observable (without noise in simplest case)</p></li>
<li><p>Let <span class="math inline">\(\tilde{X}\)</span> be new data where we want to forecast the distribution of <span class="math inline">\(f_{\tilde{X}}\)</span></p></li>
<li><p>See <a href="https://probml.github.io/pml-book/book2.html">ProbML Book 2</a> Section 18.3 for more details</p>
<p><span class="math display">\[
\begin{aligned}
f_{\tilde{X}}\,|\,X, f_X, \tilde{X} &amp;\sim \mathcal{N}(\mu_{\tilde{X}}, \Sigma_{\tilde{X}})\\
\mu_{\tilde{X}} &amp;\equiv \mu_{\tilde{X}} + K_{X\tilde{X}}^{\top} K_{XX}^{-1} (f_X - \mu_X)\\
\Sigma_{\tilde{X}} &amp;\equiv K_{\tilde{X}\tilde{X}} - K_{X\tilde{X}}^{\top} K_{XX}^{-1} K_{X\tilde{X}}
\end{aligned}
\]</span></p></li>
<li><p>Can adjust for noisy observation <span class="math inline">\(y = f(x) + \sigma^2 \epsilon\)</span> for <span class="math inline">\(\epsilon \sim \mathcal{N}(0,I)\)</span> easily</p></li>
</ul>
</section>
<section id="example-posteriors-probml-book-2-section-18.3.2" class="slide level2">
<h2>Example Posteriors (ProbML Book 2 Section 18.3.2)</h2>

<img data-src="figures/GP_posteriors.png" class="r-stretch"></section>
<section id="reproducing-kernel-hilbert-spaces-rkhs" class="slide level2">
<h2>Reproducing Kernel Hilbert Spaces (RKHS)</h2>
<ul>
<li>See <a href="https://probml.github.io/pml-book/book2.html">ProbML Book 2</a> Section 18.3.7 for more details</li>
<li>Since we are working with function spaces, and there is an implicit inner product <span class="math inline">\(\phi(x)\)</span> associated with the kernel <span class="math inline">\(\mathcal{K}(x,x')\)</span></li>
<li>We can construct a <a href="https://en.wikipedia.org/wiki/Hilbert_space">Hilbert Space</a> using those inner products
<ul>
<li>Denote the space <span class="math inline">\(\mathcal{H}_{\mathcal{K}}\)</span> for kernel <span class="math inline">\(\mathcal{K}\)</span></li>
<li>All inner product spaces induce a norm, so we can write <span class="math inline">\(||f||_{\mathcal{H}_{\mathcal{K}}}\)</span></li>
</ul></li>
</ul>
</section>
<section id="erm-in-a-function-space" class="slide level2">
<h2>ERM in a Function Space</h2>
<ul>
<li>ERM in a function space with LLS and a norm <span class="math inline">\(||f||\)</span></li>
</ul>
<p><span class="math display">\[
\min_{f \in \mathcal{F}} \left\{\sum_{n=1}^N (y_n - f(x_n))^2 + \frac{\lambda}{2}||f||\right\}
\]</span></p>
<ul>
<li>If we use the RKHS function space associated with our kernel, we have the <strong>kernel ridge regression</strong></li>
</ul>
<p><span class="math display">\[
\min_{f \in \mathcal{H}_{\mathcal{K}}} \left\{\sum_{n=1}^N(y_n - f(x_n))^2 + \frac{\lambda}{2}||f||_{\mathcal{H}_{\mathcal{K}}}\right\}
\]</span></p>
</section>
<section id="representer-theorem" class="slide level2">
<h2>Representer Theorem</h2>
<ul>
<li><p>See <a href="https://probml.github.io/pml-book/book2.html">ProbML Book 2</a> Section 18.3.7.3 for more details</p></li>
<li><p>The <strong>representer theorem</strong> says that a solution to ERM in a function space</p>
<p><span class="math display">\[
f^* = \arg\min_{f \in \mathcal{H}_{\mathcal{K}}} \left\{\sum_{n=1}^N\ell(f, x_n, y_n) + \frac{\lambda}{2}||f||_{\mathcal{H}_{\mathcal{K}}}\right\}
\]</span></p>
<ul>
<li>Has a solution, for some coefficients <span class="math inline">\(\alpha_n\)</span> a function of the data</li>
</ul>
<p><span class="math display">\[
f^*(x) = \sum_{n=1}^N \alpha_n \mathcal{K}(x, x_n)
\]</span></p>
<ul>
<li>See <a href="https://probml.github.io/pml-book/book2.html">ProbML Book 2</a> for solutions for kernel ridge regression</li>
</ul></li>
</ul>
</section>
<section id="fitting-gps" class="slide level2">
<h2>Fitting GPs</h2>
<ul>
<li>Kernels themselves typically have parameters which must be fit/learned/etc. from the data
<ul>
<li>e.g.&nbsp;the <span class="math inline">\(\ell\)</span> bandwidth in the Gaussian kernel</li>
</ul></li>
<li>This is typically done with maximum marginal likelihood or MAP</li>
<li>See <a href="https://probml.github.io/pml-book/book2.html">ProbML Book 2</a> Section 18.3.5 and 18.6 for more details</li>
<li>Crucially, we can “learn the kernel” to mix <strong>representation learning</strong> with kernel methods/GPs</li>
</ul>
</section>
<section id="manually-fitting-a-gp" class="slide level2">
<h2>Manually Fitting a GP</h2>
<ul>
<li>Assume <span class="math inline">\(m(x) = 0\)</span> for simplicity</li>
<li>Remember <span class="math inline">\(K_{XX} \equiv [K_{ij}]_{i,j=1}^N\)</span> and <span class="math inline">\(\mathbf{\alpha}\equiv [\alpha_i]_{i=1}^N\)</span> then, <span class="math display">\[
||f^*||_{\mathcal{H}_{\mathcal{K}}} = \sqrt{f^*\cdot f^*} = \sqrt{\mathbf{\alpha}^{\top} K_{XX} \mathbf{\alpha}}
\]</span></li>
<li>Substitute into the Representer Theorem <span class="math display">\[
\begin{aligned}
&amp;\min_{f \in \mathcal{H}_{\mathcal{K}}} \left\{\sum_{n=1}^N\ell(f, x_n, y_n) + \frac{\lambda}{2}||f||_{\mathcal{H}_{\mathcal{K}}}\right\}\\
&amp;\min_{\mathbf{\alpha}} \left\{\sum_{n=1}^N\ell\left(\sum_{i=1}^N \alpha_n \mathcal{K}(x_n, x_i), x_n, y_n\right) + \sqrt{\frac{\lambda}{2}\mathbf{\alpha}^{\top} K_{XX}\mathbf{\alpha}}\right\}
\end{aligned}
\]</span></li>
</ul>
</section>
<section id="mean-functions-and-hyperparameters" class="slide level2">
<h2>Mean Functions and Hyperparameters</h2>
<ul>
<li>Kernels often have hyperparameters <span class="math inline">\(\theta\)</span> (e.g., bandwidth) and parametric <span class="math inline">\(m(x;\eta, \theta)\)</span></li>
<li>Let <span class="math inline">\(K_{XX}(\theta) \equiv [K(x_i, x_j;\theta)]_{i,j=1}^N, \mathbf{x} \equiv [x_n]_{n=1}^N, \mathbf{y} \equiv [y_n]_{n=1}^N,\)</span> and <span class="math inline">\(\mathbf{m}(\eta;\theta)\equiv [m(x;\theta, \eta)]_{n=1}^N\)</span></li>
<li>Often loss is written in terms of residuals, <span class="math inline">\(\mathbf{r}(f, \mathbf{x}, \mathbf{y})\)</span> where <span class="math inline">\(\sum_{n=1}^N \ell(f, x_n, y_n) = ||\mathbf{r}(f, \mathbf{x}, \mathbf{y})||_2^2\)</span>
<ul>
<li>Then we can usually write the optimization problem as <span class="math display">\[
\begin{aligned}
\min_{\mathbf{\alpha}, \eta} &amp;\left\{ ||\mathbf{r}(K_{XX}(\theta)\cdot \mathbf{\alpha}+ \mathbf{m}(\eta, \theta);\theta, \eta)||_2^2 + \sqrt{\frac{\lambda}{2}\mathbf{\alpha}^{\top}\cdot K_{XX}(\theta)\cdot\mathbf{\alpha}}\right\}\\
\end{aligned}
\]</span></li>
<li>Then an outer optimization process for <span class="math inline">\(\theta\)</span>!</li>
</ul></li>
</ul>
</section>
<section id="ridgeless-kernel-solution" class="slide level2">
<h2>Ridgeless Kernel Solution</h2>
<ul>
<li><p>Recall the min-norm, ridgeless OLS: <span class="math display">\[
\lim_{\lambda \to 0} \arg\max_{\alpha} ||\alpha \cdot X - y||^2_2 + \lambda ||\alpha||^2_2 = \arg\max_{\alpha} ||\alpha||_2^2 \text{ s t. } \alpha \cdot X = y
\]</span></p></li>
<li><p>With interpolating problem (i.e., <span class="math inline">\(\mathbf{r}(\cdot) = 0\)</span> possible) take <span class="math inline">\(\lambda \to 0\)</span> in Representer</p>
<p><span class="math display">\[
  \begin{aligned}
  \min_{\mathbf{\alpha}, \eta} &amp;\left\{\mathbf{\alpha}^{\top}K_{XX}(\theta)\mathbf{\alpha}\right\}\\
  \text{s.t. }\, &amp; \mathbf{r}(K_{XX}(\theta)\cdot \mathbf{\alpha}+ \mathbf{m}(\eta, \theta);\theta, \eta) = 0
  \end{aligned}
\]</span></p>
<ul>
<li>i.e.&nbsp;the min-norm (according to <span class="math inline">\(|f||_{\mathcal{H}_{\mathcal{K}}}\)</span>) interpolating solution</li>
<li>Can stack inequalities, additional constraints etc.</li>
<li>With linear (in <span class="math inline">\(\mathcal{H}_{\mathcal{K}}\)</span>) residuals/constraints can use very fast LQ solvers</li>
<li>More explicit and reliable than the inductive bias in neural networks</li>
</ul></li>
</ul>
</section></section>
<section>
<section id="gpytorch" class="title-slide slide level1 center">
<h1>GPytorch</h1>

</section>
<section id="gpytorch-1" class="slide level2">
<h2>GPytorch</h2>
<ul>
<li>One can manually create kernels and run the optimizer, or use a package such as <a href="https://docs.gpytorch.ai/en/stable/examples/01_Exact_GPs/Simple_GP_Regression.html">GPyTorch</a></li>
<li>See <a href="examples/gpytorch_regression.py">examples/gpytorch_regression.py</a></li>
</ul>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a><span class="im">import</span> math</span>
<span id="cb1-2"><a href="#cb1-2"></a><span class="im">import</span> torch</span>
<span id="cb1-3"><a href="#cb1-3"></a><span class="im">import</span> gpytorch</span>
<span id="cb1-4"><a href="#cb1-4"></a><span class="im">from</span> gpytorch.kernels <span class="im">import</span> ScaleKernel, RBFKernel, LinearKernel</span>
<span id="cb1-5"><a href="#cb1-5"></a><span class="im">from</span> matplotlib <span class="im">import</span> pyplot <span class="im">as</span> plt</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="example-training-data" class="slide level2">
<h2>Example Training Data</h2>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1"></a><span class="co"># Training data is 100 points in [0,1] inclusive regularly spaced</span></span>
<span id="cb2-2"><a href="#cb2-2"></a>train_x <span class="op">=</span> torch.linspace(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">100</span>)</span>
<span id="cb2-3"><a href="#cb2-3"></a><span class="co"># True function is sin(2*pi*x) with Gaussian noise</span></span>
<span id="cb2-4"><a href="#cb2-4"></a>train_y <span class="op">=</span> torch.sin(train_x <span class="op">*</span> (<span class="dv">2</span> <span class="op">*</span> math.pi)) <span class="op">+</span> torch.randn(train_x.size()) <span class="op">*</span> math.sqrt(<span class="fl">0.04</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="choosing-kernels" class="slide level2">
<h2>Choosing Kernels</h2>
<ul>
<li>Kernels can be created from other kernels. e.g., given <span class="math inline">\(\mathcal{K}(x,x')\)</span>,
<ul>
<li><span class="math inline">\(\exp(\mathcal{K}(x,x'))\)</span> is a valid kernel</li>
<li><span class="math inline">\(C \mathcal{K}(x,x')\)</span> for a fixed scaling <span class="math inline">\(C &gt; 0\)</span></li>
<li>See <a href="https://docs.gpytorch.ai/en/stable/kernels.html">the docs</a> for more</li>
</ul></li>
<li>Also, we can provide a “learnable” mean, <span class="math inline">\(m(x)\)</span> for the process
<ul>
<li>e.g.&nbsp;<code>gpytorch.means.ConstantMean()</code>, but could be function of <span class="math inline">\(x\)</span></li>
</ul></li>
</ul>
</section>
<section id="simple-gp-model-with-exact-inference" class="slide level2">
<h2>Simple GP Model with exact inference</h2>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1"></a><span class="kw">class</span> ExactGPModel(gpytorch.models.ExactGP):</span>
<span id="cb3-2"><a href="#cb3-2"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, train_x, train_y, likelihood):</span>
<span id="cb3-3"><a href="#cb3-3"></a>        <span class="bu">super</span>(ExactGPModel, <span class="va">self</span>).<span class="fu">__init__</span>(train_x, train_y, likelihood)</span>
<span id="cb3-4"><a href="#cb3-4"></a>        <span class="va">self</span>.mean_module <span class="op">=</span> gpytorch.means.ConstantMean()</span>
<span id="cb3-5"><a href="#cb3-5"></a>        <span class="va">self</span>.covar_module <span class="op">=</span> gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())</span>
<span id="cb3-6"><a href="#cb3-6"></a></span>
<span id="cb3-7"><a href="#cb3-7"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb3-8"><a href="#cb3-8"></a>        mean_x <span class="op">=</span> <span class="va">self</span>.mean_module(x)</span>
<span id="cb3-9"><a href="#cb3-9"></a>        covar_x <span class="op">=</span> <span class="va">self</span>.covar_module(x)</span>
<span id="cb3-10"><a href="#cb3-10"></a>        <span class="cf">return</span> gpytorch.distributions.MultivariateNormal(mean_x, covar_x)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="setup-likelihood-and-loss" class="slide level2">
<h2>Setup likelihood and Loss</h2>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1"></a>likelihood <span class="op">=</span> gpytorch.likelihoods.GaussianLikelihood()</span>
<span id="cb4-2"><a href="#cb4-2"></a>model <span class="op">=</span> ExactGPModel(train_x, train_y, likelihood)</span>
<span id="cb4-3"><a href="#cb4-3"></a>model.train()</span>
<span id="cb4-4"><a href="#cb4-4"></a>likelihood.train()</span>
<span id="cb4-5"><a href="#cb4-5"></a>optimizer <span class="op">=</span> torch.optim.Adam(model.parameters(), lr<span class="op">=</span><span class="fl">0.1</span>)  <span class="co"># Includes GaussianLikelihood parameters</span></span>
<span id="cb4-6"><a href="#cb4-6"></a><span class="co"># "Loss" for GPs - the marginal log likelihood</span></span>
<span id="cb4-7"><a href="#cb4-7"></a>mll <span class="op">=</span> gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="run-the-optimizer" class="slide level2">
<h2>Run the Optimizer</h2>
<div class="cell columns column-output-location" data-execution_count="5">
<div class="column">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">100</span>):</span>
<span id="cb5-2"><a href="#cb5-2"></a>    optimizer.zero_grad()</span>
<span id="cb5-3"><a href="#cb5-3"></a>    <span class="co"># Output from model</span></span>
<span id="cb5-4"><a href="#cb5-4"></a>    output <span class="op">=</span> model(train_x)</span>
<span id="cb5-5"><a href="#cb5-5"></a>    <span class="co"># Calc loss and backprop gradients</span></span>
<span id="cb5-6"><a href="#cb5-6"></a>    loss <span class="op">=</span> <span class="op">-</span>mll(output, train_y)</span>
<span id="cb5-7"><a href="#cb5-7"></a>    loss.backward()</span>
<span id="cb5-8"><a href="#cb5-8"></a>    <span class="cf">if</span> i <span class="op">%</span> <span class="dv">10</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb5-9"><a href="#cb5-9"></a>      <span class="bu">print</span>(<span class="ss">f"Iter=</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">, Loss=</span><span class="sc">{</span>loss<span class="sc">.</span>item()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb5-10"><a href="#cb5-10"></a>    optimizer.step()</span>
<span id="cb5-11"><a href="#cb5-11"></a>model.<span class="bu">eval</span>() <span class="co"># evaluation mode</span></span>
<span id="cb5-12"><a href="#cb5-12"></a>likelihood.<span class="bu">eval</span>()    </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div><div class="column">
<div class="cell-output cell-output-stdout">
<pre><code>Iter=0, Loss=0.9302332997322083
Iter=10, Loss=0.5079350471496582
Iter=20, Loss=0.17511649429798126
Iter=30, Loss=-0.02991340681910515
Iter=40, Loss=-0.020419616252183914
Iter=50, Loss=-0.034119490534067154
Iter=60, Loss=-0.03867233172059059
Iter=70, Loss=-0.0389210507273674
Iter=80, Loss=-0.04004982113838196
Iter=90, Loss=-0.039925917983055115</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="5">
<pre><code>GaussianLikelihood(
  (noise_covar): HomoskedasticNoise(
    (raw_noise_constraint): GreaterThan(1.000E-04)
  )
)</code></pre>
</div>
</div>
</div>
</section>
<section id="visualize-results" class="slide level2">
<h2>Visualize Results</h2>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1"></a><span class="cf">with</span> torch.no_grad(), gpytorch.settings.fast_pred_var():</span>
<span id="cb8-2"><a href="#cb8-2"></a>    test_x <span class="op">=</span> torch.linspace(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">51</span>)</span>
<span id="cb8-3"><a href="#cb8-3"></a>    observed_pred <span class="op">=</span> likelihood(model(test_x))</span>
<span id="cb8-4"><a href="#cb8-4"></a></span>
<span id="cb8-5"><a href="#cb8-5"></a>    fig, ax <span class="op">=</span> plt.subplots()</span>
<span id="cb8-6"><a href="#cb8-6"></a>    lower, upper <span class="op">=</span> observed_pred.confidence_region()</span>
<span id="cb8-7"><a href="#cb8-7"></a>    ax.plot(train_x.numpy(), train_y.numpy(), <span class="st">'k*'</span>)</span>
<span id="cb8-8"><a href="#cb8-8"></a>    ax.plot(test_x.numpy(), observed_pred.mean.numpy(), <span class="st">'b'</span>)</span>
<span id="cb8-9"><a href="#cb8-9"></a>    <span class="co"># Shade between the lower and upper confidence bounds</span></span>
<span id="cb8-10"><a href="#cb8-10"></a>    ax.fill_between(test_x.numpy(), lower.numpy(), upper.numpy(), alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb8-11"><a href="#cb8-11"></a>    ax.set_ylim([<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>])</span>
<span id="cb8-12"><a href="#cb8-12"></a>    ax.legend([<span class="st">'Observed Data'</span>, <span class="st">'Mean'</span>, <span class="st">'Confidence'</span>])    </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>


<div class="footer footer-default">

</div>
</section><section id="visualize-results-output" class="slide level2 output-location-slide"><h2>Visualize Results</h2><div class="cell output-location-slide" data-execution_count="6">
<div class="cell-output cell-output-display">
<p><img data-src="kernel_methods_files/figure-revealjs/cell-7-output-1.png" width="792" height="416"></p>
</div>
</div></section></section>
    </div>
  </div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="../site_libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="../site_libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-chalkboard/plugin.js"></script>
  <script src="../site_libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="../site_libs/revealjs/plugin/notes/notes.js"></script>
  <script src="../site_libs/revealjs/plugin/search/search.js"></script>
  <script src="../site_libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="../site_libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': false,
'smaller': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleChalkboard(event)\"><kbd>b</kbd> Toggle Chalkboard</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleNotesCanvas(event)\"><kbd>c</kbd> Toggle Notes Canvas</a></li>\n<li class=\"slide-tool-item\" data-item=\"6\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.downloadDrawings(event)\"><kbd>d</kbd> Download Drawings</a></li>\n<li class=\"slide-tool-item\" data-item=\"7\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'chalkboard': {"buttons":true},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: false,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1280,

        height: 720,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, RevealChalkboard, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
    window.document.addEventListener("DOMContentLoaded", function (event) {
      const toggleBodyColorMode = (bsSheetEl) => {
        const mode = bsSheetEl.getAttribute("data-mode");
        const bodyEl = window.document.querySelector("body");
        if (mode === "dark") {
          bodyEl.classList.add("quarto-dark");
          bodyEl.classList.remove("quarto-light");
        } else {
          bodyEl.classList.add("quarto-light");
          bodyEl.classList.remove("quarto-dark");
        }
      }
      const toggleBodyColorPrimary = () => {
        const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
        if (bsSheetEl) {
          toggleBodyColorMode(bsSheetEl);
        }
      }
      toggleBodyColorPrimary();  
      const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
      tabsets.forEach(function(tabset) {
        const tabby = new Tabby('#' + tabset.id);
      });
      const isCodeAnnotation = (el) => {
        for (const clz of el.classList) {
          if (clz.startsWith('code-annotation-')) {                     
            return true;
          }
        }
        return false;
      }
      const clipboard = new window.ClipboardJS('.code-copy-button', {
        text: function(trigger) {
          const codeEl = trigger.previousElementSibling.cloneNode(true);
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
        }
      });
      clipboard.on('success', function(e) {
        // button target
        const button = e.trigger;
        // don't keep focus
        button.blur();
        // flash "checked"
        button.classList.add('code-copy-button-checked');
        var currentTitle = button.getAttribute("title");
        button.setAttribute("title", "Copied!");
        let tooltip;
        if (window.bootstrap) {
          button.setAttribute("data-bs-toggle", "tooltip");
          button.setAttribute("data-bs-placement", "left");
          button.setAttribute("data-bs-title", "Copied!");
          tooltip = new bootstrap.Tooltip(button, 
            { trigger: "manual", 
              customClass: "code-copy-button-tooltip",
              offset: [0, -8]});
          tooltip.show();    
        }
        setTimeout(function() {
          if (tooltip) {
            tooltip.hide();
            button.removeAttribute("data-bs-title");
            button.removeAttribute("data-bs-toggle");
            button.removeAttribute("data-bs-placement");
          }
          button.setAttribute("title", currentTitle);
          button.classList.remove('code-copy-button-checked');
        }, 1000);
        // clear code selection
        e.clearSelection();
      });
      function tippyHover(el, contentFn) {
        const config = {
          allowHTML: true,
          content: contentFn,
          maxWidth: 500,
          delay: 100,
          arrow: false,
          appendTo: function(el) {
              return el.closest('section.slide') || el.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'light-border',
          placement: 'bottom-start'
        };
          config['offset'] = [0,0];
          config['maxWidth'] = 700;
        window.tippy(el, config); 
      }
      const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
      for (var i=0; i<noterefs.length; i++) {
        const ref = noterefs[i];
        tippyHover(ref, function() {
          // use id or data attribute instead here
          let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
          try { href = new URL(href).hash; } catch {}
          const id = href.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          return note.innerHTML;
        });
      }
      const findCites = (el) => {
        const parentEl = el.parentElement;
        if (parentEl) {
          const cites = parentEl.dataset.cites;
          if (cites) {
            return {
              el,
              cites: cites.split(' ')
            };
          } else {
            return findCites(el.parentElement)
          }
        } else {
          return undefined;
        }
      };
      var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
      for (var i=0; i<bibliorefs.length; i++) {
        const ref = bibliorefs[i];
        const citeInfo = findCites(ref);
        if (citeInfo) {
          tippyHover(citeInfo.el, function() {
            var popup = window.document.createElement('div');
            citeInfo.cites.forEach(function(cite) {
              var citeDiv = window.document.createElement('div');
              citeDiv.classList.add('hanging-indent');
              citeDiv.classList.add('csl-entry');
              var biblioDiv = window.document.getElementById('ref-' + cite);
              if (biblioDiv) {
                citeDiv.innerHTML = biblioDiv.innerHTML;
              }
              popup.appendChild(citeDiv);
            });
            return popup.innerHTML;
          });
        }
      }
    });
    </script>
    

</body></html>