{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ECON622: Computational Economics with Data Science Applications\n",
        "\n",
        "Generalization, Deep Learning, and Representations\n",
        "\n",
        "Jesse Perla (University of British Columbia)\n",
        "\n",
        "# Overview\n",
        "\n",
        "## Summary\n",
        "\n",
        "-   Step back from optimization and fitting process to discuss the\n",
        "    broader issues of statistics and function approximation\n",
        "-   Key topics:\n",
        "    -   ERM, interpolation, and generalization error\n",
        "    -   Features and Representations\n",
        "    -   Neural Networks and broader hypothesis classes\n",
        "-   In the subsequent lecture we can briefly return to the optimization\n",
        "    process itself to discuss\n",
        "    -   Global vs. Local solutions\n",
        "    -   Inductive bias and regularization\n",
        "\n",
        "# Neural Networks, Part I\n",
        "\n",
        "## Neural Networks Include Almost Everything\n",
        "\n",
        "-   **Neural Networks** as just an especially flexible functional form\n",
        "    -   Linear, polynomials, and all of our standard approximation fit\n",
        "        in there\n",
        "-   However, when we say “Neural Network” you should have in mind\n",
        "    approximations that are typically\n",
        "    -   parameterized by some complicated:\n",
        "        $\\theta \\equiv \\{\\theta_1, \\theta_2, \\cdots, \\theta_L\\}$\n",
        "    -   nested with **layers**:\n",
        "        $y = f_1(f_2(\\ldots f_L(x;\\theta_L);\\theta_2);\\theta_1) \\equiv f(x;\\theta)$\n",
        "    -   **highly overparameterized**, such that $\\theta$ is often\n",
        "        massive, often far larger than the amount of data\n",
        "-   Terminology: **depth** is number of layers $L$, **width** is the\n",
        "    size of $\\theta_{\\ell}$\n",
        "\n",
        "## (Asymptotic) Universal Approximation Theorems\n",
        "\n",
        "-   At this point you may expect a theorem that says that neural\n",
        "    networks are [**universal\n",
        "    approximators**](https://en.wikipedia.org/wiki/Universal_approximation_theorem#Arbitrary-width_case).\n",
        "    e.g., Cybenko, Hornik, and others\n",
        "    -   i.e., for any function $f^*(x)$, there exists a neural network\n",
        "        $f(x;\\theta)$ that can approximate it arbitrarily well\n",
        "    -   Takes limits of the number of parameters at each layer\n",
        "        (e.g. $\\theta_{\\ell}\\nearrow$), or sometimes the number of\n",
        "        layers (e.g. $L\\nearrow$)\n",
        "-   A low bar to pass that rarely gives useful guidance or bounds\n",
        "    -   We do not use enormously “wide” approximations\n",
        "    -   The theorems are too pessimistic, as NNs do much better in\n",
        "        practice\n",
        "    -   Important when doing core functional analysis and asymptotics\n",
        "\n",
        "## How Can we Fit Huge Approximations?\n",
        "\n",
        "-   At this point you should not be scared off by a big $\\theta$\n",
        "-   The ML and deep-learning revolution is built on ideas you have\n",
        "    covered\n",
        "    -   With a scalar loss $L(\\theta)$, you can use VJPs (reverse-mode\n",
        "        AD) to get gradients $\\nabla_{\\theta} L(\\theta)$\n",
        "    -   AD software like PyTorch or JAX makes it easy to collect the\n",
        "        $\\theta$ and run the AD on complicated $L(\\theta)$\n",
        "    -   Hardware (e.g. GPUs) can make common operations like VJPs fast\n",
        "    -   Optimization methods like SGD work great, using gradient\n",
        "        estimates when memory is an issue for the full\n",
        "        $\\nabla_{\\theta} L(\\theta)$\n",
        "    -   Regularization, in all its forms, helps when used appropriately\n",
        "-   But that doesn’t explain why this can help generalization\n",
        "    performance?\n",
        "\n",
        "## Puzzling Empirical Success, Ahead of Theory\n",
        "\n",
        "-   Deep learning and AD are old, but only recently have the software\n",
        "    and hardware been good enough to scale\n",
        "-   Bias-variance tradeoff: adding parameters can make things worse\n",
        "    -   But ML practitioners often find the opposite **empirically**\n",
        "    -   Frequent success with lots of layers and massive\n",
        "        over-parameterization\n",
        "    -   This seems counter to all sorts of basic statistics intuition\n",
        "-   ML theory is still catching up to the empirical success\n",
        "    -   We will try to give some perspectives on **when/why deep\n",
        "        learning works**\n",
        "    -   First, we will have to be precise on what we mean by “works”\n",
        "\n",
        "# ERM and Interpolation\n",
        "\n",
        "## Estimation and Interpolation Setup\n",
        "\n",
        "-   See [ProbML Book 1](https://probml.github.io/pml-book/book1.html)\n",
        "    Section 5.4 on ERM\n",
        "-   Generalizing that notation somewhat to better nest functional\n",
        "    equations\n",
        "    -   Let $p^*(x,y)$ be the **true** distribution on inputs\n",
        "        $x \\in \\mathcal{X}$ and outputs $y \\in \\mathcal{Y}$\n",
        "    -   Let $f : \\mathcal{X} \\to \\mathcal{Y}$ be a function mapping\n",
        "        inputs to outputs. For example, maybe $\\hat{y} = f(x)$ is an\n",
        "        estimator for the relationship between $x$ and $y$ in $p^*(x,y)$\n",
        "    -   For now, assume that $f \\in \\mathcal{F}$ for some very large set\n",
        "        of functions appropriate to the problem (e.g., Sobolev space,\n",
        "        etc.)\n",
        "    -   Define the loss as an operator $(f, x, y) \\to \\ell(f, x, y)$\n",
        "        -   For example, $\\ell(f, x, y) = (y - f(x))^2$ for LLS\n",
        "\n",
        "## Population Risk\n",
        "\n",
        "-   The **population risk** is then defined as\n",
        "\n",
        "$$\n",
        "R(f, p^*) \\equiv \\mathbb{E}_{p^*(x,y)}\\left[\\ell(f, x, y)\\right]\n",
        "$$\n",
        "\n",
        "-   This lets us define the ideal minimizer of the population risk as\n",
        "\n",
        "$$\n",
        "f^{**} = \\arg\\min_{f \\in \\mathcal{F}} R(f, p^*)\n",
        "$$\n",
        "\n",
        "-   If $\\min_{f \\in \\mathcal{F}} R(f, p^*) = 0$, there is an\n",
        "    interpolating solution across the whole of $p^*$. Common with\n",
        "    functional equations, rare in statistics applications\n",
        "\n",
        "## Functional Equations\n",
        "\n",
        "-   The deviations from the ProbML book are to ensure we can nest\n",
        "    solving functional equations. In that case, we can drop the $y$.\n",
        "-   We can loosely think of $f$ as solving the functional equation as\n",
        "    long as\n",
        "\n",
        "$$\n",
        "\\min_{f \\in \\mathcal{F}} R(f, p^*) \\equiv \\min_{f \\in \\mathcal{F}} \\mathbb{E}_{p^*(x)}\\left[\\ell(f, x)\\right] = 0\n",
        "$$\n",
        "\n",
        "-   Set the function class $\\mathcal{F}$ to be consistent with the\n",
        "    $\\ell$\n",
        "    -   e.g., for $\\ell(f,x) = (\\partial_x f(x) - a f(x))^2$ choose a\n",
        "        Sobolev $\\mathcal{F}$\n",
        "    -   We are leaving it out here, but you might add on other terms\n",
        "        like boundary conditions to the $R(f,p^*)$ loss\n",
        "    -   Note the key difference relative to standard methods is the\n",
        "        $p^*(x)$!\n",
        "\n",
        "## Empirical Risk\n",
        "\n",
        "-   Denote $N$ samples from $p^*(x,y)$ as\n",
        "    $\\mathcal{D} \\equiv \\{x_n, y_n\\}_{n=1}^N$\n",
        "-   We can think of this as empirical distribution\n",
        "    $p_{\\mathcal{D}}(x,y)$\n",
        "-   Which lets us define the **empirical risk**\n",
        "\n",
        "$$\n",
        "R(f, \\mathcal{D}) \\equiv \\frac{1}{N}\\sum_{n=1}^N \\ell(f, x_n, y_n) = R(f, p_{\\mathcal{D}}) \n",
        "$$\n",
        "\n",
        "-   Note that the empirical risk is decoupled from the function space of\n",
        "    $f$ and simply captures the role of finite-samples from $p^*(x,y)$\n",
        "\n",
        "## Hypothesis Class\n",
        "\n",
        "-   If $\\mathcal{F}$ is too large then it may be hard to solve, or we\n",
        "    may introduce errors due to overfitting\n",
        "-   Instead, consider a **hypothesis class**\n",
        "    $\\mathcal{H}\\subseteq \\mathcal{F}$, which is parameterized in\n",
        "    practice by $\\theta \\in \\Theta$\n",
        "-   For example, we might choose an $\\mathcal{H}$ that is a\n",
        "    -   Linear function\n",
        "    -   Orthogonal polynomial\n",
        "    -   Neural network with large number of parameters and\n",
        "        nonlinearities\n",
        "\n",
        "## Empirical Risk Minimization\n",
        "\n",
        "-   Finally, we can now look at the **empirical risk minimization**\n",
        "    problem, which is the core of most statistical estimation problems\n",
        "    like regressions\n",
        "\n",
        "$$\n",
        "f^*_{\\mathcal{D}} = \\arg\\min_{f \\in \\mathcal{H}} R(f, \\mathcal{D}) = \\arg\\min_{f \\in \\mathcal{H}}\\frac{1}{N}\\sum_{n=1}^N \\ell(f, x_n, y_n)\n",
        "$$\n",
        "\n",
        "-   If the $\\mathcal{H}(\\Theta)$, then we could implement this as\n",
        "\n",
        "$$\n",
        "\\arg\\min_{\\theta \\in \\Theta} \\frac{1}{N}\\sum_{n=1}^N \\ell(f_{\\theta}, x_n, y_n)\n",
        "$$\n",
        "\n",
        "## Linear Least Squares\n",
        "\n",
        "-   For example, if\n",
        "    -   $\\mathcal{H}(\\Theta)$ only includes linear functions\n",
        "        -   i.e. $f_{\\theta}(x) = \\theta \\cdot x$ for some\n",
        "            $\\theta \\in \\Theta$\n",
        "    -   $\\ell(f, x, y) = (y - f(x))^2$\n",
        "-   Then this problem is just OLS\n",
        "    -   $\\arg\\min_{\\theta \\in \\Theta} \\frac{1}{N}\\sum_{n=1}^N (y_n - \\theta \\cdot x_n)^2$\n",
        "-   Economists are very good at analyzing the properties of the\n",
        "    $\\mathcal{D}$ in $p^*(x,y)$ but often ignore the differences between\n",
        "    $\\mathcal{H}$ vs. $\\mathcal{F}$?\n",
        "-   In contrast: for solving functional equations, we are better at\n",
        "    analyzing $\\mathcal{F}$ vs. $\\mathcal{H}$, but typically implicitly\n",
        "    assume a uniform $p^*(x)$\n",
        "\n",
        "## Approximation Error\n",
        "\n",
        "-   To evaluate what we lose by using $\\mathcal{H}$ instead of\n",
        "    $\\mathcal{F}$, we can define\n",
        "\n",
        "$$\n",
        "f^* \\equiv \\arg\\min_{f \\in \\mathcal{H}} R(f, p^*)\n",
        "$$\n",
        "\n",
        "-   Then the **approximation error** is defined as\n",
        "\n",
        "$$\n",
        "\\varepsilon_{app}(\\mathcal{H}) \\equiv R(f^*, p^*) - R(f^{**}, p^*)\n",
        "$$\n",
        "\n",
        "-   This says, taking the population distribution as given, how close we\n",
        "    can get a $f^*$ to the ideal solution $f^{**}$ using $\\mathcal{H}$\n",
        "    instead of $\\mathcal{F}$\n",
        "    -   The weighting by $p^*(x)$ is crucial to gauge “success”\n",
        "\n",
        "## Generalization Error\n",
        "\n",
        "-   Alternatively, we can fix the hypothesis class and ask how much\n",
        "    error we are introducing by the use of finite data $\\mathcal{D}$\n",
        "    instead of the population distribution $p^*(x,y)$\n",
        "-   The **generalization error** (or estimation error) is\n",
        "\n",
        "$$\n",
        "\\varepsilon_{est}(\\mathcal{H}) \\equiv \\mathbb{E}_{\\mathcal{D} \\sim p^*}\\left[R(f^*_{\\mathcal{D}}, \\mathcal{D}) - R(f^*, p^*)\\right]\n",
        "$$\n",
        "\n",
        "-   By that notation we are showing that this is taking the expectation\n",
        "    over samples $\\mathcal{D}$ from the true $p^*(x,y)$\n",
        "\n",
        "## Calculating the Generalization Error\n",
        "\n",
        "-   Since we typically do not have the true $p^*$, or can at most sample\n",
        "    from it, we need to find ways to approximate the $\\varepsilon_{est}$\n",
        "    for a given problem.\n",
        "-   A typical approach is the data splitting we discussed in the\n",
        "    previous section.\n",
        "    -   Partition $\\mathcal{D}$ into $\\mathcal{D}_{train}$ and\n",
        "        $\\mathcal{D}_{test}$, then solve ERM to find $$\n",
        "        f^*_{\\mathcal{D}_{train}} = \\arg\\min_{f \\in \\mathcal{H}} R(f, \\mathcal{D}_{train}) = \\arg\\min_{f \\in \\mathcal{H}}\\frac{1}{N_{train}}\\sum_{n=1}^{N_{train}} \\ell(f, x_n, y_n)\n",
        "        $$\n",
        "    -   Then, we can approximate with what is sometimes called the\n",
        "        **generalization gap** $$\n",
        "        \\varepsilon_{est}(\\mathcal{H}) \\approx R(f^*_{\\mathcal{D}_{train}} , \\mathcal{D}_{train}) - R(f^*_{\\mathcal{D}_{train}} , \\mathcal{D}_{test})\n",
        "        $$\n",
        "\n",
        "## Decomposing the Error\n",
        "\n",
        "-   Armed with these definitions, we can now decompose the error of\n",
        "    using a particular hypothesis class $\\mathcal{H}$ and finite set of\n",
        "    samples $\\mathcal{D}$ from $p^*$ as\n",
        "\n",
        "$$\n",
        "\\mathbb{E}_{\\mathcal{D} \\sim p^*}\\left[\\min_{f \\in \\mathcal{H}} R(f, \\mathcal{D}) - \\min_{f \\in \\mathcal{F}} R(f, p^*)\\right] = \\varepsilon_{app}(\\mathcal{H}) + \\varepsilon_{est}(\\mathcal{H})\n",
        "$$\n",
        "\n",
        "-   Note that this is a property of the approximation class\n",
        "    $\\mathcal{H}$ and the selection process for the $\\mathcal{D}$, not a\n",
        "    particular $\\mathcal{D}$\n",
        "\n",
        "## Tradeoffs\n",
        "\n",
        "-   This is at the core of the bias-variance tradeoff\n",
        "    -   $\\varepsilon_{app}(\\mathcal{H})$ is the bias, i.e. approximation\n",
        "        error\n",
        "    -   $\\varepsilon_{est}(\\mathcal{H})$ is the variance,\n",
        "        i.e. estimation error\n",
        "-   The classic tradeoff here is that if you make $\\mathcal{H}$ too\n",
        "    rich, you may decrease the $\\varepsilon_{app}(\\mathcal{H})$ but the\n",
        "    extra flexibility may lead to a much higher\n",
        "    $\\varepsilon_{est}(\\mathcal{H})$\n",
        "    -   i.e., flexibility leads to overfitting\n",
        "-   In the next lectures we will investigate this classic intuition in\n",
        "    more detail and discuss when it falls apart.\n",
        "    -   Hint: consider the role of regularization in all its forms\n",
        "-   The bigger challenge is that as data and economic mechanisms become\n",
        "    richer, you may not be able to choose the appropriate $\\mathcal{H}$\n",
        "    manually\n",
        "\n",
        "# Features and Representations\n",
        "\n",
        "## Features\n",
        "\n",
        "-   The **features** of a problem start with the inputs that we use\n",
        "    within the hypothesis class $\\mathcal{H}(\\Theta)$\n",
        "-   Economists are used to **shallow** approximations, e.g.,\n",
        "    -   Linear functions, $f_{\\theta}(x) = \\theta \\cdot x$\n",
        "    -   Orthogonal polynomials,\n",
        "        $f_{\\theta}(x) = \\sum_{m=1}^M \\theta_m \\phi_m(x)$ for basis\n",
        "        $\\phi_m(x)$\n",
        "-   Economists **feature engineer** to choose the appropriate\n",
        "    $x\\in\\mathcal{X}$ form raw data, typically then used with shallow\n",
        "    approximations\n",
        "    -   e.g. log, dummies, polynomials, first-differences, means, etc.\n",
        "    -   Embeds all sorts of priors, wisdom, and economic intuition\n",
        "    -   Priors are inescapable - and a good thing as long as you are\n",
        "        aware when you use them and how they affect inference\n",
        "\n",
        "## Simple Representation in ERM\n",
        "\n",
        "-   To abstract from this manual process, we can think of instead taking\n",
        "    the raw data $x$ and transforming it into a **representation**\n",
        "    $z\\in \\mathcal{Z}$ with $g : \\mathcal{X} \\to \\mathcal{Z}$\n",
        "\n",
        "-   Then, instead of finding a $f : \\mathcal{X} \\to \\mathcal{Y}$, we can\n",
        "    find a $\\tilde{f} : \\mathcal{Z} \\to \\mathcal{Y}$ and in our loss use\n",
        "    $\\ell(\\tilde{f}\\circ g, x, y)$\n",
        "\n",
        "    $$\n",
        "    \\tilde{f}^*_{\\mathcal{D}} = \\arg\\min_{\\tilde{f} \\in \\mathcal{H}}\\frac{1}{N}\\sum_{n=1}^N \\ell(\\tilde{f} \\circ g, x_n, y_n)\n",
        "    $$\n",
        "\n",
        "    -   And then define\n",
        "        $f^*_{\\mathcal{D}} \\equiv \\tilde{f}^*_{\\mathcal{D}} \\circ g$\n",
        "\n",
        "-   Our approximation class $\\mathcal{H}$ then changes - for better or\n",
        "    worse.\n",
        "\n",
        "## Finding Representations is an Art\n",
        "\n",
        "-   This is the process of finding **latent variables** and inverse\n",
        "    mapping from observables to them\n",
        "-   What is our goal when choosing $g$?\n",
        "    -   Drop irrelevant information, which is the simplest feature\n",
        "        engineering\n",
        "    -   Find $z$ that captures the relevant information in $x$ for the\n",
        "        problem at hand (i.e., the particular $\\ell, y, \\mathcal{D}$)\n",
        "    -   See [ProbML Book\n",
        "        2](https://probml.github.io/pml-book/book1.html) Section 5.6 for\n",
        "        more on information theory\n",
        "    -   Disentangled (i.e., the factors of variation are separated out\n",
        "        in $z$)\n",
        "\n",
        "## Problem Specific or Reusable?\n",
        "\n",
        "-   Are representations reusable between tasks?\n",
        "    -   e.g., our wisdom of when to take logs or first-differences is\n",
        "        often reusable\n",
        "-   Remember: representations are on $\\mathcal{X}$, not $\\mathcal{Y}$\n",
        "    -   Encodes age old wisdom from working with the datasources\n",
        "    -   Though wisdom probably included seeing $\\mathcal{Y}$ from\n",
        "        previous tasks\n",
        "-   What if $\\mathcal{X}$ is complicated, or we are worried we may have\n",
        "    chosen the wrong $z = g(x)$?\n",
        "    -   Can we learn this automatically?\n",
        "\n",
        "## Can Representations Be Learned?\n",
        "\n",
        "-   The short answer is: yes, but it is still an art (with finite data)\n",
        "-   **Representation learning** finds representations\n",
        "    $g : \\mathcal{X} \\to \\mathcal{Z}$ using $\\mathcal{D}$\n",
        "    -   Hopefully: works well for $x \\sim p^*(x,y)$, and for many\n",
        "        $\\ell(\\tilde{f} \\circ g, x, y)$\n",
        "-   This happens in subtle ways in many different methods. e.g. \n",
        "    -   If we run “unsupervised” clustering or embeddings on our data to\n",
        "        embed it into a lower dimensional space then run a regression\n",
        "    -   “Learning the kernel”. See [ProbML Book\n",
        "        2](https://probml.github.io/pml-book/book2.html) Section 18.6\n",
        "    -   Autoencoders and variational autoencoders\n",
        "    -   Deep learning approximations, which we will discuss in detail\n",
        "\n",
        "## Benefits of Having Learned Representations\n",
        "\n",
        "-   If you know the correct features, there is no benefit besides maybe\n",
        "    dimension reduction. But are you so sure they are correct?\n",
        "-   Can handle complicated data (e.g. text, networks, high-dimensions)\n",
        "-   Maybe the representations are reusable across problems, just like\n",
        "    they were for our manual feature engineering\n",
        "    -   This is part of the process of **transfer learning** and\n",
        "        **fine-tuning**\n",
        "-   Using a good representation is more sample efficient because data\n",
        "    ends up used in fitting $\\tilde{f}$ instead of jointly finding\n",
        "    $f = \\tilde{f} \\circ g$\n",
        "-   Maybe problems which are complicated and nonlinear in $\\mathcal{X}$\n",
        "    are simpler in $\\mathcal{Z}$ (i.e., linear regression in\n",
        "    $\\mathcal{Z}$)\n",
        "-   Above all: good representations overfit less and **generalize\n",
        "    better**\n",
        "\n",
        "## Jointly Learning Representations and Functions?\n",
        "\n",
        "-   Because $f \\equiv \\tilde{f} \\circ g$, the simplest approach is just\n",
        "    to jointly learn both\n",
        "-   Come up with some hypothesis class $\\mathcal{H}$ that flexible\n",
        "    enough to have both the representation and function of interest\n",
        "-   Extra flexibility could overfit (i.e.,\n",
        "    $\\varepsilon_{est}(\\mathcal{H})$ could increase even if\n",
        "    $\\varepsilon_{app}(\\mathcal{H}) \\searrow 0$)\n",
        "    -   Hence the crucial need for regularization in various forms\n",
        "-   Notice the nested structure here of $\\tilde{f} \\circ g$\n",
        "    -   Hints at why Neural Networks might work so well\n",
        "\n",
        "## Is this a Mixture of Supervised and Unsupervised?\n",
        "\n",
        "-   Remember that we talked about finding representations as intrinsic\n",
        "    to the data itself, and hence “unsupervised”\n",
        "    -   Fitting $\\tilde{f} \\circ g$ jointly combines supervised and\n",
        "        unsupervised\n",
        "-   Nested structure means we may be able to use this for new problems\n",
        "    -   Isolate the $g$ parameters and structure from the $\\tilde{f}$\n",
        "    -   Train $\\tilde{f} \\circ g$ for a new $\\tilde{f}$ and fit jointly\n",
        "        again or **freeze** the $g$\n",
        "    -   Can work shockingly well in practice!\n",
        "    -   [What do Neural Networks Learn When Trained on Random\n",
        "        Labels?](https://arxiv.org/pdf/2006.10455.pdf)\n",
        "-   Best to start with existing representations and **fine-tune**\n",
        "    (essential in LLMs)\n",
        "\n",
        "# Neural Networks, Part II\n",
        "\n",
        "## Rough Intuition on The Success of Deep Learning\n",
        "\n",
        "-   The broadest intuition on why deep learning with flexible,\n",
        "    overparameterized neural networks often works well is a combination\n",
        "    of:\n",
        "    1.  The massive number of parameters makes the optimization process\n",
        "        find more generalizable solutions (sometimes through\n",
        "        regularization)\n",
        "    2.  The depth of approximations allowing for better representations\n",
        "    3.  The optimization process seems to be able to learn those\n",
        "        representations\n",
        "    4.  The representations are reusable across problems\n",
        "    5.  Regularization (implicit and explicit) helps us avoid\n",
        "        overfitting\n",
        "-   Art, not magic\n",
        "    -   Need to design architectures ($\\mathcal{H}$), optimization, and\n",
        "        regularization\n",
        "\n",
        "## Neural Networks and Representations\n",
        "\n",
        "-   Neural networks are typically “deep”:\n",
        "    $f_1(f_2(\\ldots f_L(x;\\theta_L);\\theta_2);\\theta_1) \\equiv f(x;\\theta)$\n",
        "-   If we fit $f(x) \\equiv \\tilde{f}(g(x;\\theta_1);\\theta_2)$ then there\n",
        "    is a chance that we could fit both a good representation and a\n",
        "    generalizable function\n",
        "-   So it seems that having two “layers” helps. What is less clear is\n",
        "    that\n",
        "    1.  Having further nesting of representations helps\n",
        "    2.  That the representations themselves can be learned in this way\n",
        "-   For examples on why multiple layers help see:[Mark Schmidt’s CPSC\n",
        "    440](https://www.cs.ubc.ca/~schmidtm/Courses/440-W22/L6.pdf),\n",
        "    [CPSC340](https://www.cs.ubc.ca/~schmidtm/Courses/340-F22/L32.pdf),\n",
        "    [ProbML Book 1](https://probml.github.io/pml-book/book1.html)\n",
        "    Section 13.2.1 on the XOR Problem and 13.2.5-13.2.7 for more\n",
        "\n",
        "## Common Neural Network Designs\n",
        "\n",
        "-   A common pattern for $\\mathcal{H}$ is called the Multi Layer\n",
        "    Perception (MLP)\n",
        "    -   See [ProbML Book\n",
        "        1](https://probml.github.io/pml-book/book1.html) Section 13.2\n",
        "    -   Alternate linear/nonlinear then end with linear (or match\n",
        "        domain)\n",
        "    -   One **hidden** layer $f(x) = W_2 \\sigma(W_1 x + b_1) + b_2$\n",
        "    -   Another layer:\n",
        "        $f(x) = W_3 \\sigma(W_2 \\sigma(W_1 x + b_1) + b_2) + b_3$\n",
        "    -   Where $\\sigma(\\cdot)$ is a nonlinear **activation function** in\n",
        "        the jargon. e.g. $\\tanh(\\cdot)$ or $\\max(0, \\cdot)$ (called\n",
        "        ReLU)\n",
        "    -   See [ProbML Book\n",
        "        1](https://probml.github.io/pml-book/book1.html) Section 13.2.4\n",
        "        numerical properties of gradients\n",
        "-   If $f : \\mathbb{R}^N \\to \\mathbb{R}$ with 2 hidden layers and\n",
        "    **width** of $M$:\n",
        "    -   $W_1 \\in \\mathbb{R}^{M \\times N}, b_1 \\in \\mathbb{R}^M, W_2 \\in \\mathbb{R}^{M \\times M}, b_2 \\in \\mathbb{R}^M, W_3 \\in \\mathbb{R}^{1 \\times M}, b_3 \\in \\mathbb{R}$\n",
        "\n",
        "## Many Problem Specific Variations\n",
        "\n",
        "-   Use economic intuition and problem specific knowledge to design\n",
        "    $\\mathcal{H}$\n",
        "\n",
        "    -   Encode knowledge of good representations, easier learning\n",
        "\n",
        "-   For example, you can approximation function\n",
        "    $f : \\mathbb{R}^N \\to \\mathbb{R}$ which are symmetric in arguments\n",
        "    (i.e. permutation invariance) with\n",
        "\n",
        "    $$\n",
        "    f(X) = \\rho\\left(\\frac{1}{N}\\sum_{x\\in X} \\phi(x)\\right)\n",
        "    $$\n",
        "\n",
        "    -   $\\rho : \\mathbb{R}^M \\to \\mathbb{R}$,\n",
        "        $\\phi : \\mathbb{R} \\to \\mathbb{R}^M$ both neural networks\n",
        "\n",
        "-   See [Probabilistic Symmetries and Invariant Neural\n",
        "    Networks](https://www.jmlr.org/papers/volume21/19-322/19-322.pdf) or\n",
        "    [Exploiting Symmetry in High Dimensional Dynamic\n",
        "    Programming](https://www.jesseperla.com/publication/symmetry-dynamic-programming/symmetry-dynamic-programming.pdf)\n",
        "\n",
        "## Transfer Learning and Few-Shot Learning\n",
        "\n",
        "-   If you have a good representation, you can often use it for new\n",
        "    problems\n",
        "-   Take the $\\theta$ as an initial condition for the optimizer and\n",
        "    **fine-tune** on the new problem\n",
        "-   e.g. take $g \\equiv f_1 \\circ f_2 \\circ \\ldots f_{L-1}$ (e.g. the\n",
        "    all but the last layer) and **freeze them** only changing the last\n",
        "    $f_L$ layer with training\n",
        "    -   Or only freeze some of them\n",
        "    -   May work well even if the task was completely different. Many\n",
        "        $y$ \\$are simply linear combinations of the disentangled\n",
        "        representations - part of why kernel methods (which we will\n",
        "        discuss in a future lecture) work well\n",
        "-   Can find sometimes “one-shot”, “few-shot”, or “zero-shot” learning\n",
        "    -   e.g. [Zero-Shot Learning](https://arxiv.org/pdf/1706.03466.pdf)\n",
        "        for image classification\n",
        "\n",
        "# More Perspectives on Representations\n",
        "\n",
        "## The Tip of the Iceberg\n",
        "\n",
        "-   The ideas sketched out previously are just the beginning\n",
        "-   This section points out a few important ideas and directions for you\n",
        "    to explore on your own\n",
        "\n",
        "## Autoencoders and Representations\n",
        "\n",
        "-   One unsupervised approach is to consider **autoencoders**. See\n",
        "    [ProbML Book 1](https://probml.github.io/pml-book/book1.html)\n",
        "    Section 20.3, [ProbML Book\n",
        "    2](https://probml.github.io/pml-book/book2.html) Section 21, and\n",
        "    [CPSC 440\n",
        "    Notes](https://www.cs.ubc.ca/~schmidtm/Courses/440-W22/L10.pdf)\n",
        "-   Consider connection between representations and compression\n",
        "-   Let $g : \\mathcal{X} \\to \\mathcal{Z}$ be a **encoder** and\n",
        "    $h : \\mathcal{Z} \\to \\mathcal{X}$ be a **decoder**, parameterized by\n",
        "    some $\\theta_d$ and $\\theta_e$ then we want to find the empirical\n",
        "    equivalent to\n",
        "\n",
        "$$\n",
        "\\min_{\\theta_e, \\theta_d} \\mathbb{E}_{p^*(x)} (h(g(x;\\theta_e);\\theta_d) - x)^2 + \\text{regularizer}\n",
        "$$\n",
        "\n",
        "-   Whether the $g(x;\\theta_e)$ is a good representation depends on\n",
        "    whether “compression” of the information of $x$ is useful for\n",
        "    downstream tasks\n",
        "\n",
        "## Manifold Hypothesis\n",
        "\n",
        "-   Are there always simple, reusable representations?\n",
        "-   On perspective is called the [Manifold\n",
        "    Hypothesis](https://en.wikipedia.org/wiki/Manifold_hypothesis), see\n",
        "    [ProbML Book 1](https://probml.github.io/pml-book/book1.html)\n",
        "    Section 20.4.2\n",
        "-   The basic idea is that even the most complicated data sources in the\n",
        "    real world ultimately are concentrated on a low-dimensional manifold\n",
        "    -   including images, text, networks, high-dimensional time-series,\n",
        "        etc.\n",
        "    -   i.e., the world is much simpler than it appears if you find the\n",
        "        right transformation\n",
        "-   See [ProbML Book 1](https://probml.github.io/pml-book/book1.html)\n",
        "    Section 20.4.2.4 describes Manifold Learning, which tries to learn\n",
        "    the geometry of these manifolds through variations on nonlinear\n",
        "    dimension reduction\n",
        "\n",
        "## Embeddings\n",
        "\n",
        "-   A related idea is to **embed** data into a different (sometimes\n",
        "    lower-dimensional) space\n",
        "-   For example, text or images or networks, mapped into $\\mathbb{R}^M$\n",
        "-   Whether it is lower or higher dimensional, the key is to preserve\n",
        "    some geometric properties, typically a norm. i.e.,\n",
        "    $||x - y|| \\approx ||g(x) - g(y)||$\n",
        "-   You can think of learned representations within the inside of neural\n",
        "    networks as often doing embedding in some form, especially if you\n",
        "    use an [information\n",
        "    bottleneck](https://arxiv.org/pdf/1503.02406.pdf)\n",
        "-   See [ProbML Book 1](https://probml.github.io/pml-book/book1.html)\n",
        "    Section 20.5 and 23 for examples with text and networks\n",
        "\n",
        "## Lottery Tickets and Representations\n",
        "\n",
        "-   [The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural\n",
        "    Networks](https://arxiv.org/pdf/1803.03635.pdf)\n",
        "-   The idea is that inside of every huge, overparameterized\n",
        "    approximation with random initialization and layers is a small\n",
        "    sparse approximation\n",
        "    -   You can show this by pruning most of the parameters and training\n",
        "        it\n",
        "-   Perhaps huge, overparameterized functions with many layers are more\n",
        "    likely to contain the lottery ticket\n",
        "    -   Then the optimization methods are just especially good at\n",
        "        finding them\n",
        "    -   Ex-post you could prune, but ex-ante you don’t know the sparsity\n",
        "        structure\n",
        "\n",
        "## Distentangling Representations\n",
        "\n",
        "-   Representations that separate out different factors of variation\n",
        "    into separate variables are often easier to use for downstream tasks\n",
        "    and are more transferrable\n",
        "-   The keyword in a literature review is to look for is [disentangled\n",
        "    representations](https://arxiv.org/pdf/1812.02230.pdf)\n",
        "-   It seems like this is hard to do [without\n",
        "    supervision](https://proceedings.mlr.press/v97/locatello19a/locatello19a.pdf)\n",
        "-   But\n",
        "    [semi-supervised](https://www.cs.toronto.edu/~bonner/courses/2022s/csc2547/papers/generative/disentangled-representations/semi-supervised-disentangling,-siddharth,-nips2017.pdf)\n",
        "    approaches seem to be a good approach\n",
        "\n",
        "## Out of Distribution Learning\n",
        "\n",
        "-   We have been discussing a $\\mathcal{D}$ from some idealized,\n",
        "    constant $p^*(x,y)$\n",
        "-   What if the distribution changes, or our samples are not IID (e.g.,\n",
        "    in control and reinforcement learning applications where it is\n",
        "    $p^*(x,y;f)$)?\n",
        "-   See [ProbML Book 2](https://probml.github.io/pml-book/book1.html)\n",
        "    Section 19.1 for more\n",
        "-   This is called “robustness to distribution shift”, covariate shift,\n",
        "    etc. in different settings\n",
        "-   There are many methods, but one common element is:\n",
        "    -   Models with better “generalization” tends to perform much better\n",
        "        under distribution shift\n",
        "    -   Good representations are much more robust\n",
        "    -   With transfer learning you may be able to adapt very easily"
      ],
      "id": "7cd249fa-049d-40b3-b144-041bc5b140a8"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  }
}