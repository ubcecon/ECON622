{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ECON622: Computational Economics with Data Science Applications\n",
        "\n",
        "Optimization for Machine Learning\n",
        "\n",
        "Jesse Perla (University of British Columbia)\n",
        "\n",
        "# Overview\n",
        "\n",
        "## Summary\n",
        "\n",
        "-   This lecture continues from the previous lecture on gradients to\n",
        "    further explore optimization methods in machine learning, and\n",
        "    discusses training pipelines and tooling\n",
        "-   Primary reference materials are:\n",
        "    -   [ProbML Book 1:\n",
        "        Introduction](https://probml.github.io/pml-book/book1.html)\n",
        "    -   [ProbML Book 2: Advanced\n",
        "        Topics](https://probml.github.io/pml-book/book2.html) including\n",
        "        Section 6.3\n",
        "    -   [Mark Schmidt’s ML Lecture\n",
        "        Notes](https://www.cs.ubc.ca/~schmidtm/Courses/LecturesOnML/)\n",
        "-   We will also give a sense of a standard machine learning pipeline of\n",
        "    training, validation, and test data and discuss generalization,\n",
        "    logging, etc.\n",
        "\n",
        "## Why the Emphasis on Optimization and Gradients?\n",
        "\n",
        "-   A huge number of algorithms for economists can be written as\n",
        "    optimization problems (e.g., MLE, interpolation) or as something\n",
        "    similar in spirit (e.g. Bayesian Sampling, Reinforcement Learning)\n",
        "-   Previous lectures on AD showed ways to find VJPs for extremely\n",
        "    complicated functions. **Differentiate everything, no excuses!**\n",
        "-   In practice, **all** problems with high-dimensions parameters or\n",
        "    latents require gradients for algorithms to be feasible\n",
        "-   We will soon take a further step: **are (unbiased) estimates of the\n",
        "    gradient good enough for many algorithms?**\n",
        "\n",
        "# Optimization Crash Course\n",
        "\n",
        "## Optimization Methods\n",
        "\n",
        "-   Learning continuous optimization methods is an enormous project\n",
        "-   See referenced materials and lecture notes\n",
        "-   Here we will give an overview of some key concepts\n",
        "-   Be warned! The details matter, so more study is required if you want\n",
        "    to use these methods in practice\n",
        "\n",
        "## Crash Course in Unconstrained Optimization\n",
        "\n",
        "$$\n",
        "\\min_{\\theta} \\mathcal{L}(\\theta)\n",
        "$$\n",
        "\n",
        "Will briefly introduce\n",
        "\n",
        "-   First-order methods\n",
        "-   Second-order methods\n",
        "-   Preconditioning\n",
        "-   Momentum\n",
        "-   Regularization\n",
        "\n",
        "## First-Order Methods\n",
        "\n",
        "-   See [ProbML Book 1](https://probml.github.io/pml-book/book1.html)\n",
        "    Section 8.2\n",
        "-   Armed with reverse-mode AD for\n",
        "    $\\mathcal{L} : \\mathbb{R}^N \\to \\mathbb{R}$ we can calculate\n",
        "    $\\nabla \\mathcal{L}(\\theta)$ with the same computational order as\n",
        "    $\\mathcal{L}(\\theta)$\n",
        "-   Furthermore, given JVPs we know we can calculate these objective\n",
        "    functions for extremely complicated functions (e.g., nested fixed\n",
        "    points, and implicit functions)\n",
        "-   Iterative: take $\\theta_0$ and provide $\\theta_t \\to \\theta_{t+1}$\n",
        "    -   May converge to a stationary point (hopefully close to a global\n",
        "        argmin)\n",
        "    -   If it doesn’t converge, the solution may still be an argmin\n",
        "    -   See references for details on convergence for convex and\n",
        "        non-convex problems\n",
        "\n",
        "## Gradient Descent\n",
        "\n",
        "-   See [Mark Schmidt’s\n",
        "    Notes](https://www.cs.ubc.ca/~schmidtm/Courses/340-F22/L13.pdf)\n",
        "-   Gradient descent takes $\\theta_0$, and stepsize $\\eta_t$ and\n",
        "    iterates until $\\nabla \\mathcal{L}(\\theta_t)$ is small, or\n",
        "    $\\theta_t$ stationary\n",
        "\n",
        "$$\n",
        "\\theta_{t+1} = \\theta_t - \\eta_t \\nabla \\mathcal{L}(\\theta_t)\n",
        "$$\n",
        "\n",
        "-   It is the simplest “first-order” method (i.e., using just the\n",
        "    gradient of $\\mathcal{L}$)\n",
        "-   Will call $\\eta_t$ a “learning rate schedule”\n",
        "-   Think of line-search methods as choosing the stepsize $\\eta_t$\n",
        "    optimally. Can help with many of our problems\n",
        "\n",
        "## When and Where Does This Converge?\n",
        "\n",
        "-   Skipping a million details, see [ProbML Book\n",
        "    1](https://probml.github.io/pml-book/book1.html) Section 8.2.2 and\n",
        "    [Mark Schmidt’s\n",
        "    basic](https://www.cs.ubc.ca/~schmidtm/Courses/340-F22/L14.pdf) and\n",
        "    [more advanced\n",
        "    notes](https://www.cs.ubc.ca/~schmidtm/Courses/5XX-S22/S1.pdf)\n",
        "\n",
        "-   For strictly convex problems this converges to the global minima,\n",
        "    though sufficient conditions include Robbins-Monro\n",
        "    $\\lim_{T\\to \\infty} \\eta_T = 0$ and\n",
        "\n",
        "    $$\n",
        "    \\lim_{T\\to\\infty}\\frac{\\sum_{t=1}^T \\eta_t}{\\sum_{t=1}^T \\eta_t^2} = 0\n",
        "    $$\n",
        "\n",
        "-   For problems that not globally convex this may go to local optima,\n",
        "    but if the function is locally strictly convex then it will converge\n",
        "    to a local optima\n",
        "\n",
        "-   For other types of functions (e.g.,\n",
        "    [invex](https://en.wikipedia.org/wiki/Invex_function)) it may still\n",
        "    converge to the “right” solution in some important sense\n",
        "\n",
        "## Preconditioned Gradient Descent\n",
        "\n",
        "-   As we saw analyzing LLS, badly conditioned problems converge slowly\n",
        "    with iterative methods\n",
        "\n",
        "-   We can precondition a problem as we did with linear systems, and it\n",
        "    has the same stationary point\n",
        "\n",
        "-   Choose some $C_t$ for preconditioned gradient descent $$\n",
        "    \\theta_{t+1} = \\theta_t - \\eta_t C_t \\nabla \\mathcal{L}(\\theta_t)\n",
        "    $$\n",
        "\n",
        "-   We saw before that the Hessian tells us the geometry, so the optimal\n",
        "    preconditioner must be related to $\\nabla^2 \\mathcal{L}(\\theta_t)$\n",
        "\n",
        "## Second-Order Methods\n",
        "\n",
        "-   See [ProbML Book 1](https://probml.github.io/pml-book/book1.html)\n",
        "    Section 8.3 and [Mark Schmidt’s\n",
        "    Notes](https://www.cs.ubc.ca/~schmidtm/Courses/5XX-S22/S2.pdf)\n",
        "-   Adapt $\\eta_t C_t$ to use the Hessian (e.g., Newton’s Method)\n",
        "\n",
        "$$\n",
        "\\theta_{t+1} = \\theta_t - \\eta_t \\left[\\nabla^2 \\mathcal{L}(\\theta_t) \\right]^{-1}\\nabla \\mathcal{L}(\\theta_t)\n",
        "$$\n",
        "\n",
        "-   Second order methods are rarer because the calculating the Hessian\n",
        "    is no longer the same computational order as $\\mathcal{L}(\\theta)$\n",
        "-   See [ProbML Book 1](https://probml.github.io/pml-book/book1.html)\n",
        "    Section 8.3.2 for info on Quasi-Newtonian methods which\n",
        "    approximation Hessian using gradients like BFGS\n",
        "\n",
        "## Momentum\n",
        "\n",
        "-   See [ProbML Book 1](https://probml.github.io/pml-book/book1.html)\n",
        "    Section 8.2.4 and [Mark Schmidt’s\n",
        "    Notes](https://www.cs.ubc.ca/~schmidtm/Courses/5XX-S22/S2.pdf)\n",
        "-   Can use “momentum”, which speeds up convergence, helps avoid local\n",
        "    optima, and moves fast in flat regions\n",
        "-   Momentum will be a common feature of many ML optimizers (e.g. Adam,\n",
        "    RMSProp, etc.) as it helps with heavily non-convex problems\n",
        "-   A classic method is called Nesterov Accelerated Gradient (NAG),\n",
        "    which is a modification of gradient descent for some\n",
        "    $\\beta_t\\in (0,1)$ (e.g., $0.9$)\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\hat{\\theta}_{t+1} &= \\theta_t + \\beta_t(\\theta_t - \\theta_{t-1})\\\\\n",
        "\\theta_{t+1} &= \\hat{\\theta}_{t+1} - \\eta_t \\nabla \\mathcal{L}(\\hat{\\theta}_{t+1})\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "## Does Uniqueness Matter?\n",
        "\n",
        "-   Remember from our previous lecture on Sobolev norms and\n",
        "    regularization that we care about functions, not parameters.\n",
        "-   Consider when $\\theta$ is used as parameters for a function\n",
        "    (e.g. $\\hat{f}_{\\theta}$)\n",
        "    -   Then what does a lack of convergence of the $\\theta_t$ or\n",
        "        multiplicity with multiple $\\theta$ solutions mean?\n",
        "    -   Maybe nothing! If\n",
        "        $||\\hat{f}_{\\theta_0} - \\hat{f}_{\\theta_1}||_S$ is small, then\n",
        "        the functions themselves may be in the same equivalence class.\n",
        "        Depends on the norm, of course.\n",
        "-   This topic will be discussed when we consider double-descent curves,\n",
        "    but the punchline for now is that the training/optimization is a\n",
        "    means to an end (i.e., generalization) and not an end in itself.\n",
        "\n",
        "## Regularization\n",
        "\n",
        "-   See [Mark Schmidt’s\n",
        "    Notes](https://www.cs.ubc.ca/~schmidtm/Courses/340-F22/L16.pdf). For\n",
        "    LLS this is the ridge regression\n",
        "-   We discussed regularization as a way to deal with multiplicity\n",
        "\n",
        "$$\n",
        "\\min_{\\theta}\\left[\\mathcal{L}(\\theta) + \\frac{\\alpha}{2} ||\\theta||^2\\right]\n",
        "$$\n",
        "\n",
        "-   Gradient descent becomes (called “weight decay” in ML, and “ridge\n",
        "    regression” if objective is LLS)\n",
        "\n",
        "$$\n",
        "\\theta_{t+1} = \\theta_t - \\eta_t \\left[\\nabla \\mathcal{L}(\\theta_t) + \\alpha \\theta_t\\right]\n",
        "$$\n",
        "\n",
        "-   Mapping of regularized $\\theta_t$ to a $f_{\\theta_t}$ is subtle if\n",
        "    nonlinear\n",
        "\n",
        "# Stochastic Optimization\n",
        "\n",
        "## Are Gradients Really that Cheap to Calculate?\n",
        "\n",
        "-   Consider that the objective often involves data (or grid points for\n",
        "    interpolation)\n",
        "    -   Denote $x_n$, and observables $y_n$ for $n=1, \\ldots N$\n",
        "-   With VJPs, the computational order of\n",
        "    $\\nabla_{\\theta} \\mathcal{L}(\\theta;\\{x_n, y_n\\}_{n=1}^N)$ may be\n",
        "    the same as that of $\\mathcal{L}$ itself\n",
        "-   However, keep in mind that reverse-mode requires storing the\n",
        "    intermediate values in the “primal” calculation (i.e.,\n",
        "    $\\mathcal{L}(\\theta;\\{x_n, y_n\\}_{n=1}^N)$)\n",
        "    -   Hence, the memory requirements grow with $N$\n",
        "    -   This may be a big problem for large datasets or complicated\n",
        "        calculations, especially with GPUs which have more limited\n",
        "        memory\n",
        "\n",
        "## Do We Need the Full Gradient?\n",
        "\n",
        "-   In practice, it is impossible to calculate the full gradient for\n",
        "    large datasets\n",
        "\n",
        "-   In GD, the gradient provided the direction of steepest descent\n",
        "\n",
        "-   Consider an algorithm with a $g_t$ as an unbiased estimate of the\n",
        "    gradient\n",
        "\n",
        "    $$\n",
        "    \\begin{aligned}\n",
        "    \\theta_{t+1} = \\theta_t - \\eta_t g_t\\\\\n",
        "    \\mathbb{E}[g_t] = \\nabla \\mathcal{L}(\\theta_t)\n",
        "    \\end{aligned}\n",
        "    $$\n",
        "\n",
        "    -   Make the $\\eta_t$ smaller to deal with noise if this is\n",
        "        high-variance\n",
        "    -   Choose $g_t$ to be far cheaper to calculate than\n",
        "        $\\nabla \\mathcal{L}(\\theta_t)$\n",
        "\n",
        "-   Remember: we don’t need the actual value of the objective function\n",
        "    to optimize it!\n",
        "\n",
        "-   Will also add additional regularization, which can help with\n",
        "    generalization\n",
        "\n",
        "## Stochastic Optimization\n",
        "\n",
        "-   To formalize: Up until now our optimizers have been “deterministic”\n",
        "-   Now we introduce a source of randomness $z \\sim q_{\\theta}(z)$,\n",
        "    i.e. it might depend on the estimated parameters $\\theta$ later with\n",
        "    RL/etc.\n",
        "    -   $z$ could be a source of uncertainty in the environment\n",
        "    -   $z$ could involve latent variables\n",
        "    -   $z$ could come from randomness in the optimization process\n",
        "        (e.g., using subsets of data to form $g_t$)\n",
        "-   Denote expectations using this distribution as\n",
        "    $\\mathbb{E}_{q_{\\theta}(z)}$\n",
        "-   For now, drop the dependence on $\\theta$ for simplicity, though it\n",
        "    becomes crucial for understanding reinforcement learning/etc.\n",
        "\n",
        "## Stochastic Objective\n",
        "\n",
        "-   The full optimization problem is then to minimize this stochastic\n",
        "    objective\n",
        "\n",
        "$$\n",
        "\\min_{\\theta}\\overbrace{\\mathbb{E}_{q(z)} \\tilde{\\mathcal{L}}(\\theta, z)}^{\\equiv \\mathcal{L}(\\theta)}\n",
        "$$\n",
        "\n",
        "-   Under appropriate regularity conditions, could use GD on this\n",
        "    objective\n",
        "\n",
        "$$\n",
        "\\nabla \\mathcal{L}(\\theta) = \\mathbb{E}_{q(z)}\\left[\\nabla \\tilde{\\mathcal{L}}(\\theta, z)\\right]\n",
        "$$\n",
        "\n",
        "-   But in practice, it is rare that we can marginalize out the $z$\n",
        "\n",
        "## Unbiased Draws from the Gradient\n",
        "\n",
        "-   Assume we can sample $z_t \\sim q(z)$ IID\n",
        "-   Then with enough regularity the gradient using just $z_t$ is\n",
        "    unbiased\n",
        "\n",
        "$$\n",
        "\\mathbb{E}_{q(z)}\\left[  \\nabla \\tilde{\\mathcal{L}}(\\theta_t, z_t) \\right] = \\nabla \\mathcal{L}(\\theta_t)\n",
        "$$\n",
        "\n",
        "-   That is, on average $\\nabla \\tilde{\\mathcal{L}}(\\theta_t, z_t)$ is\n",
        "    in the right direction for minimizing $\\mathcal{L}(\\theta_t)$\n",
        "-   This basic approach of finding unbiased estimators of the gradient\n",
        "    (and finding ways to lower the variance) is at the heart of most ML\n",
        "    optimization algorithms\n",
        "\n",
        "## Stochastic Gradient Descent\n",
        "\n",
        "-   See [ProbML Book 1](https://probml.github.io/pml-book/book1.html)\n",
        "    Section 8.4, [ProbML Book\n",
        "    2](https://probml.github.io/pml-book/book2.html) Section 6.3, and\n",
        "    [Mark Schmidt’s\n",
        "    Notes](https://www.cs.ubc.ca/~schmidtm/Courses/340-F22/L23.pdf)\n",
        "-   Given the previous slide, given IID samples $z_t \\sim q$, the\n",
        "    gradient is unbiased and we have the simplest version of stochastic\n",
        "    gradient descent (SGD)\n",
        "\n",
        "$$\n",
        "\\theta_{t+1} = \\theta_t - \\eta_t \\nabla \\tilde{\\mathcal{L}}(\\theta_t, z_t)\n",
        "$$\n",
        "\n",
        "-   Which converges to the minima of $\\min_{\\theta} \\mathcal{L}(\\theta)$\n",
        "    under appropriate conditions\n",
        "-   We can layer on all of the other features we discussed (e.g.,\n",
        "    momentum, preconditioning, etc) with SGD, but some become especially\n",
        "    important (e.g. the $\\eta_t$ schedule)\n",
        "\n",
        "## Finite-Sum Objectives\n",
        "\n",
        "-   Consider a special case of the loss function which is the sum of $N$\n",
        "    terms. For example with empirical risk minimization used in LLS/etc.\n",
        "\n",
        "    -   $z_n \\equiv (x_n, y_n)$ are typically data, observables, or grid\n",
        "        points\n",
        "    -   $\\ell(\\theta, x_n, y_n)$ is a loss function for a single data\n",
        "        point (e.g., forecasting using some $f_{\\theta}$)\n",
        "\n",
        "    $$\n",
        "    \\mathcal{L}(\\theta) = \\frac{1}{N}\\sum_{n=1}^N \\tilde{\\mathcal{L}}(\\theta, z_n) \\equiv \\frac{1}{N}\\sum_{n=1}^N \\ell(\\theta, x_n, y_n)\n",
        "    $$\n",
        "\n",
        "    -   For example, LLS is\n",
        "        $\\ell(\\theta, x_n, y_n) = ||y_n - \\theta \\cdot x_n||^2_2$\n",
        "\n",
        "-   In this case, the randomness of $z_t$ is which data point is chosen\n",
        "\n",
        "## SGD for Finite-Sum Objectives\n",
        "\n",
        "-   Hence consider sampling $z_t \\equiv (x_t, y_t)$ from our data.\n",
        "    -   In principle, IID with replacement\n",
        "-   Then run SGD on one data point at a time\n",
        "\n",
        "$$\n",
        "\\theta_{t+1} = \\theta_t - \\eta_t \\nabla_{\\theta} \\ell(\\theta_t, x_t, y_t)\n",
        "$$\n",
        "\n",
        "-   This may converges to the minima of $\\mathcal{L}(\\theta)$, and\n",
        "    potentially the storage requirements for calculations the gradient\n",
        "    are radically reduced\n",
        "-   You can guess that the $\\eta_t$ parameter is especially sensitive to\n",
        "    the variance of the gradient estimate\n",
        "\n",
        "## Decrease Variance with Multiple Draws\n",
        "\n",
        "-   With a single draw, the variance of the gradient estimate may be\n",
        "    high\n",
        "\n",
        "$$\n",
        "\\mathbb{E}\\left[\\nabla_{\\theta} \\ell(\\theta_t, x_t, y_t)- \\nabla \\mathcal{L}(\\theta_t)\\right]^2\n",
        "$$\n",
        "\n",
        "-   One tool to decrease the variance is just more monte-carlo draws.\n",
        "    With finite-sum objectives draw $B \\subseteq \\{1,\\ldots N\\}$ indices\n",
        "\n",
        "$$\n",
        "\\frac{1}{|B|}\\sum_{n \\in B} \\nabla_{\\theta} \\ell(\\theta_t, x_n, y_n)\n",
        "$$\n",
        "\n",
        "-   Classic SGD: $|B|=1$; GD: $B = \\{1, \\ldots N\\}$ and in between is\n",
        "    called “minibatch SGD”. Usually minibatch is implied with “SGD”\n",
        "\n",
        "## Minibatch SGD\n",
        "\n",
        "-   Algorithm is to draw $B_t$ indices at each step and execute SGD $$\n",
        "    \\begin{aligned}\n",
        "    g_t &\\equiv \\frac{1}{|B_t|}\\sum_{n \\in B_t} \\nabla_{\\theta} \\ell(\\theta_t, x_n, y_n)\\\\\n",
        "    \\theta_{t+1} &= \\theta_t - \\eta_t g_t\n",
        "    \\end{aligned}\n",
        "    $$\n",
        "\n",
        "-   Note that we never need to calculate $\\mathcal{L}(\\theta_t)$\n",
        "    directly, so can write our code to all operate on batches $B_t$\n",
        "\n",
        "-   Then layer other tricks on top (e.g., momentum, preconditioning,\n",
        "    etc.)\n",
        "\n",
        "    -   In principle you could also use minibatch with second-order or\n",
        "        quasi-newtonian methods but much rarer\n",
        "\n",
        "## Choosing Batches\n",
        "\n",
        "-   Choosing the $B_t$ process may be tricky. You could sample from\n",
        "    $\\{1,\\ldots N\\}$\n",
        "    -   with replacement\n",
        "    -   without replacement\n",
        "    -   without replacement after shuffling the data, and then ensure\n",
        "        you have gone through all of the data before repeating\n",
        "    -   etc.\n",
        "-   Just remember the goal: variance reduction on gradient estimates\n",
        "-   You want it to be unbiased in principle (consider partitioning the\n",
        "    data into batches and operating sequentially?)\n",
        "-   More art than science in many cases, because it requires many priors\n",
        "\n",
        "## Stochastic Optimization with $q_{\\theta}(z)$\n",
        "\n",
        "-   Previously, $q(z)$ was independent of $\\theta$ (i.e.,\n",
        "    $\\min_{\\theta}\\mathbb{E}_{q(z)} \\tilde{\\mathcal{L}}(\\theta, z)$)\n",
        "-   For many problems, the population distribution is dependent on the\n",
        "    parameters $\\theta$ (e.g., reinforcement learning, variational\n",
        "    inference, etc.). $$\n",
        "    \\min_{\\theta}\\overbrace{\\mathbb{E}_{q_{\\theta}(z)} \\tilde{\\mathcal{L}}(\\theta, z)}^{\\equiv \\mathcal{L}(\\theta)}\n",
        "    $$\n",
        "-   if $q_{\\theta}(z)$ we have our previous special case, otherwise $$\n",
        "    \\nabla_{\\theta} \\mathcal{L}(\\theta) = \\underbrace{\\int \\nabla_{\\theta} \\tilde{\\mathcal{L}}(\\theta, z)q_{\n",
        "      \\theta}(z) dz}_{=\\mathbb{E}_{q_{\\theta}(z)}\\left[\\nabla_{\\theta} \\tilde{\\mathcal{L}}(\\theta, z)\\right]} + \\int\\tilde{\\mathcal{L}}(\\theta, z) \\nabla_{\\theta}q_{\\theta}(z) d z\n",
        "    $$\n",
        "\n",
        "## Computational Challenges with Approximation\n",
        "\n",
        "-   Given $\\theta$ we can use $B$ samples from $q_{\\theta}(z)$ we can\n",
        "    still approximate\n",
        "    $\\mathbb{E}_{q(z)}\\left[\\nabla_{\\theta} \\tilde{\\mathcal{L}}(\\theta, z)\\right] \\approx \\frac{1}{|B|}\\sum_{z \\in B} \\nabla_{\\theta} \\tilde{\\mathcal{L}}(\\theta, z)$\n",
        "-   But note that\n",
        "    $\\int\\tilde{\\mathcal{L}}(\\theta, z) \\nabla_{\\theta}q_{\\theta}(z) d z$,\n",
        "    requires $\\nabla_{\\theta}q_{\\theta}(z)$\n",
        "    -   “Vanilla” monte-carlo approximations which can only sample from\n",
        "        $q_{\\theta}(z)$ in order to find gradient estimates of the\n",
        "        objective are no longer possible\n",
        "-   There are several common approaches, often used in reinforcement\n",
        "    learning, variational inference, etc.\n",
        "-   See [ProbML Book 2: Advanced\n",
        "    Topics](https://probml.github.io/pml-book/book2.html) section 6.3.3\n",
        "    for more\n",
        "\n",
        "## Score Function Estimation (REINFORCE)\n",
        "\n",
        "-   Apply the chain rule to $\\log q_{\\theta}(z)$, we find\n",
        "    $\\nabla_{\\theta} q_{\\theta}(z) = q_{\\theta}(z) \\nabla_{\\theta} \\log q_{\\theta}(z)$.\n",
        "    Rewrite\n",
        "    $\\int\\tilde{\\mathcal{L}}(\\theta, z) \\nabla_{\\theta}q_{\\theta}(z) d z$\n",
        "    as $$\n",
        "    \\int\\tilde{\\mathcal{L}}(\\theta, z)(\\nabla_{\\theta} \\log q_{\\theta}(z))  q_{\\theta}(z)dz = \\mathbb{E}_{q(z)}\\left[ \\tilde{\\mathcal{L}}(\\theta, z) \\nabla_{\\theta} \\log q_{\\theta}(z)\\right]\n",
        "    $$\n",
        "-   Then the REINFORCE estimator using $B$ samples from $q_{\\theta}(z)$\n",
        "    is $$\n",
        "    \\mathbb{E}_{q(z)}\\left[ \\tilde{\\mathcal{L}}(\\theta, z) \\nabla_{\\theta} \\log q_{\\theta}(z)\\right]\\approx \\frac{1}{|B|}\\sum_{z\\in B} \\tilde{\\mathcal{L}}(\\theta, z) \\nabla_{\\theta} \\log q_{\\theta}(z)\n",
        "    $$\n",
        "    -   Requires a differentiable $q_{\\theta}(z)$ calculated local to\n",
        "        each $z$\n",
        "    -   This estimator is usually high variance, so there are many\n",
        "        tricks to reduce this. See [ProbML Book\n",
        "        2](https://probml.github.io/pml-book/book2.html) Sections\n",
        "        6.3.4-6.3.7 or RL books for more\n",
        "\n",
        "## The “Reparameterization Trick”\n",
        "\n",
        "-   Alternatively, if you can rewrite the $q_{\\theta}(z)$ in terms of\n",
        "    $\\epsilon \\sim q_{\\epsilon}$\n",
        "    -   e.g., $z = g(\\epsilon;\\theta)$ for some $g(\\cdot;\\theta)$\n",
        "        differentiable wrt $\\theta$\n",
        "    -   Then the optimization objective is $$\n",
        "        \\nabla_{\\theta} \\mathcal{L}(\\theta) = \\mathbb{E}_{q_{\\epsilon}(\\epsilon)}\\left[\\nabla_{\\theta} \\tilde{\\mathcal{L}}(\\theta, g(\\epsilon;\\theta))\\right]\n",
        "        $$\n",
        "-   This lets us use stochastic gradient approximations sampling\n",
        "    $\\epsilon$ batches $$ \n",
        "    \\nabla_{\\theta} \\mathcal{L}(\\theta) = \\approx \\frac{1}{|B|}\\sum_{\\epsilon\\in B}\\nabla_{\\theta} \\tilde{\\mathcal{L}}(\\theta, g(\\epsilon;\\theta))\n",
        "    $$\n",
        "\n",
        "# Training Loops\n",
        "\n",
        "## “Grad Student Descent”\n",
        "\n",
        "-   Those methods for stochastic optimization make deep learning\n",
        "    possible. Just swap SGD with slightly fancier algorithms using\n",
        "    momentum, tinker with parameters, etc.\n",
        "-   In practice, all of these optimizer settings (e.g., how large for\n",
        "    $|B_t|$, $\\eta_t$, convergence criteria, etc.) are fragile and\n",
        "    require a lot of tuning\n",
        "    -   Part of a a process called **hyperparameter optimization (HPO)**\n",
        "        where you try to find the best non-model parameters for your\n",
        "        goals\n",
        "    -   Same issue with all numerical methods in economics\n",
        "        (e.g. convergence criteria of fixed point iteration, initial\n",
        "        conditions)\n",
        "-   The concern is not just that it is time-consuming for researchers\n",
        "    (and ML “Grad Students”), but that it is easy for priors to sneak in\n",
        "    and bias results\n",
        "\n",
        "## What was our Goal?\n",
        "\n",
        "-   We will address this more formally next lecture, but it is worth\n",
        "    stepping back to think about our goals. Loosely:\n",
        "    -   If we are solving an empirical risk minimization problem (like\n",
        "        regressions, etc.) or interpolation, then our goal is to use the\n",
        "        “data” to find a function $\\hat{f}_{\\theta}$ that is close to\n",
        "        the “true” function $f^*$\n",
        "-   Fitting $\\hat{f}_{\\theta}$ is easy, but we want it to **generalize**\n",
        "    within the true distribution\n",
        "    -   But we don’t know that distribution (hence the “empirical”)\n",
        "    -   So a typical approach is to emulate this by splitting the data\n",
        "        we have\n",
        "    -   But HPO is dangerous because if we are not careful we can\n",
        "        “contaminate” our process for finding $\\hat{f}_{\\theta}$ using\n",
        "        some of the data we intend to check it with. Which might lead to\n",
        "        overfitting/etc.\n",
        "\n",
        "## Splitting the Data\n",
        "\n",
        "A standard way to do this for Empirical Risk\n",
        "Minimization/Regressions/etc. is to split it into three parts:\n",
        "\n",
        "1.  **Training** data used in fitting our approximations\n",
        "    -   This is just a means to an end in ML and economics\n",
        "2.  **Validation** data used for HPO and checking convergence criteria\n",
        "    -   Be cautious to avoid using it for training\n",
        "3.  **Test** data used to evaluate the generalization performance\n",
        "    -   Ensure we don’t accidentally use it in training or validation\n",
        "\n",
        "Not all problems will have this structure (in particular, a “validation”\n",
        "set).\n",
        "\n",
        "## Why Separate Validation and Test?\n",
        "\n",
        "-   As we will see in deep learning, with massive over-parameterization\n",
        "    you typically can interpolate all of the training data.\n",
        "    -   Minimizing training loss is a means to an end, which usually\n",
        "        ends at zero\n",
        "-   The validation data might be used to check stopping criteria by\n",
        "    checking how well the approximation generalizes to data outside of\n",
        "    training\n",
        "-   But if we are using it for a stopping criteria or HPO, then is is\n",
        "    **contaminated**!\n",
        "    -   Distorts our picture of generalization if we combine it into\n",
        "        test data\n",
        "\n",
        "## What about Interpolation Problems?\n",
        "\n",
        "-   When simply trying to find interpolating functions which solve\n",
        "    functional equations, the risk of prior contamination is less clear\n",
        "-   However, you may still want to separate out validation and test grid\n",
        "    points because any data you use for HPO or convergence criteria\n",
        "    can’t be used to understand generalization.\n",
        "-   For example consider:\n",
        "    1.  Fit until “training” loss is zero\n",
        "    2.  Keep running stochastic optimizer until “validation” loss is\n",
        "        zero\n",
        "-   In that case, it crudely interpolating the validation data, which\n",
        "    makes it equivalent to training data? Not useful for generalization\n",
        "    -   May find that the model generalized better if you **stopped\n",
        "        earlier**\n",
        "\n",
        "## Level of Abstraction for Optimizers\n",
        "\n",
        "-   While you can setup a standard optimization objective and optimizer,\n",
        "    most ML frameworks work at a lower level\n",
        "-   The key reasons are that:\n",
        "    -   Minibatching (usually just called “batches”) requires more\n",
        "        flexibility in implementation to be efficient\n",
        "    -   Stopping criteria is more complicated with highly\n",
        "        overparameterized models\n",
        "    -   Logging and validation logic requires more flexibility\n",
        "    -   Often you will want to take a snapshot of the current best\n",
        "        solution and continue later for refinement (or to solve in\n",
        "        parallel)\n",
        "\n",
        "## Steps and Epochs\n",
        "\n",
        "-   There is a great deal of flexibility in how you setup the optimizer\n",
        "-   But a common approach is to randomly shuffle the data, create a set\n",
        "    of batches $B_t$ (without replacement), and then iterate through\n",
        "    them\n",
        "-   Terminology (when relevant)\n",
        "    -   Every iteration of SGD for a given batch is a **step**\n",
        "    -   If you have gone through the entire dataset once, we say that\n",
        "        you have completed an **epoch**\n",
        "-   At the end of an epoch is a good time to log, check the validation\n",
        "    loss, and potentially stop the training\n",
        "\n",
        "## Software Components used in ML\n",
        "\n",
        "Some common software components for optimization are\n",
        "\n",
        "1.  **Autodifferentiation** and libraries of functions provide the\n",
        "    approximation class\n",
        "2.  **Data loaders** which will take care of providing batches to the\n",
        "    optimizers\n",
        "3.  **Optimizers** are typically iterative, have an internal state, and\n",
        "    you can update with one sample of the gradient for that batch\n",
        "4.  **Logging** and visualization tools to track progress because the\n",
        "    optimization process may be slow and you want to do HPO\n",
        "5.  **HPO** software using training, validation, and possibly test loss\n",
        "\n",
        "## Logging and Visualization\n",
        "\n",
        "-   Several tools exist for logging to babysit optimizers, find good\n",
        "    hyperparameters, etc. including\n",
        "    [Tensorboard](https://www.tensorflow.org/tensorboard)\n",
        "    -   But we will use [Weights and Biases](https://wandb.ai/site)\n",
        "        (W&B) because it is a market leader, free for academics and\n",
        "        seems to be the frontrunner\n",
        "-   Many algorithms and frameworks exist for HPO:\n",
        "    -   [Weights and Biases](https://wandb.ai/site) (W&B) has a built-in\n",
        "        HPO framework using random search and bayesian optimization\n",
        "    -   [Optuna](https://optuna.org/) and [Ray\n",
        "        Tune](https://docs.ray.io/en/master/tune/index.html) is a\n",
        "        popular open-source HPO framework\n",
        "    -   [Ray Tune](https://docs.ray.io/en/master/tune/index.html) is a\n",
        "        popular open-source HPO framework\n",
        "-   HPO frameworks will often use the\n",
        "    [command-line](https://github.com/shadawck/awesome-cli-frameworks#python)\n",
        "    to run new jobs. [Python\n",
        "    Fire](https://github.com/google/python-fire)\n",
        "\n",
        "## Broad Frameworks for Machine Learning\n",
        "\n",
        "-   You can just hand-code loops/etc. which seems the best approach for\n",
        "    JAX\n",
        "    -   Even with Pytorch, it isn’t obvious that a framework is better\n",
        "        ex-post, though ex-ante it can help you try different\n",
        "        permutations easily\n",
        "-   [Pytorch Lightning](https://www.pytorchlightning.ai/) is a popular\n",
        "    framework which will formalize the training loops even across\n",
        "    distributed systems and make CLI, HPO, logging, etc. convenient\n",
        "    -   It remains fairly flexible because it is just wrapping Pytorch\n",
        "-   [Keras](https://keras.io/) is a similar framework with the ability\n",
        "    to target multiple backends (e.g., Pytorch, JAX)\n",
        "    -   The challenge is that it is much less flexible for non-typical\n",
        "        research\n",
        "-   [Hydra](https://github.com/facebookresearch/hydra) is a framework\n",
        "    for more serious engineering code\n",
        "\n",
        "# Linear Regression with Pytorch\n",
        "\n",
        "## Linear Regression Examples\n",
        "\n",
        "-   Of course SGD is a terrible way to do a standard linear regression,\n",
        "    but it will help us understand the mechanics with a well-understood\n",
        "    problem\n",
        "-   These examples simulate data for some:\n",
        "    $y_n = x_n \\cdot \\theta + \\sigma \\epsilon$ for\n",
        "    $\\epsilon \\sim N(0,1)$\n",
        "-   They then show various features of the optimization pipeline and\n",
        "    software to implement the LLS ERM objective\n",
        "\n",
        "$$\n",
        "\\min_{\\theta} \\frac{1}{N} \\sum_{n=1}^N \\left[y_n - x_n \\cdot \\theta\\right]^2\n",
        "$$\n",
        "\n",
        "-   Which we note has a finite-sum objective, which lets us use\n",
        "    minibatch SGD with gradient estimates\n",
        "-   Install in your environment with `pip install -r requirements.txt`\n",
        "\n",
        "## Packages\n",
        "\n",
        "-   Before showing all of the variations, here we will implement an\n",
        "    inline SGD version with minibatches"
      ],
      "id": "e031fea0-4a87-4fe6-ac6d-88720be035b5"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader, TensorDataset\n",
        "import equinox as eqx\n",
        "from flax import nnx\n",
        "from jax_dataloader.loaders import DataLoaderJAX"
      ],
      "id": "69f26f62"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Simulate Data\n",
        "\n",
        "$$\n",
        "y \\sim N(x \\cdot \\theta, \\sigma^2)\n",
        "$$"
      ],
      "id": "9a0fe16b-6c61-458f-9761-511eaa60f32d"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor([0.0513, 0.4148]), tensor(-0.0869))"
          ]
        }
      ],
      "source": [
        "N = 500  # samples\n",
        "M = 2\n",
        "sigma = 0.001\n",
        "theta = torch.randn(M)\n",
        "X = torch.randn(N, M)\n",
        "Y = X @ theta + sigma * torch.randn(N)\n",
        "dataset = TensorDataset(X, Y)\n",
        "print(dataset[0]) # returns tuples of (x_n, y_n)"
      ],
      "id": "a7af7dff"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataloaders Provide Batches\n",
        "\n",
        "-   Code which serves up random batches of data for gradient estimates\n",
        "    are called “dataloaders”\n",
        "-   The following code shuffles the data and provides batches of size\n",
        "    `batch_size`\n",
        "-   It returns a [Python\n",
        "    generator](https://docs.python.org/3/howto/functional.html#generator-expressions-and-list-comprehensions)\n",
        "    which can be iterated until it hits the end of the data"
      ],
      "id": "53040b10-d2c7-4a2c-a2f7-b98392088570"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "output-location": "column"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([[-0.3904,  0.8326],\n",
            "        [ 0.0964, -0.3748],\n",
            "        [-0.2077,  0.6708],\n",
            "        [ 1.8944,  0.5919],\n",
            "        [-0.8976,  0.5145],\n",
            "        [ 0.5539,  0.4646],\n",
            "        [ 0.5181,  0.4673],\n",
            "        [-0.7004, -0.0130]]), tensor([-0.7983,  0.2580, -0.5068,  2.1786, -1.3217,  0.5315,  0.4856, -0.8810])]"
          ]
        }
      ],
      "source": [
        "batch_size = 8\n",
        "train_loader = DataLoader(dataset,\n",
        "                batch_size=batch_size,\n",
        "                shuffle=True)\n",
        "# e.g. iterate and get first element\n",
        "print(next(iter(train_loader)))"
      ],
      "id": "cd75bad8"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loss Function for Gradient Descent\n",
        "\n",
        "-   Reminder: need to provide AD-able functions which give a gradient\n",
        "    estimate, not necessarily the objective itself!\n",
        "-   In particular, for LLS we simply can find the MSE between the\n",
        "    prediction and the data for the batch itself"
      ],
      "id": "329338a2-cde6-43a6-a210-5496381bc3c7"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def residuals(model, X, Y):  # batches or full data\n",
        "    Y_hat = model(X).squeeze()\n",
        "    return ((Y_hat - Y) ** 2).mean()"
      ],
      "id": "b4dd5e66"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hypothesis Class\n",
        "\n",
        "-   The “Hypothesis Class” for our ERM approximation is linear in this\n",
        "    case.\n",
        "-   Anything with differentiable parameters is a sub-class of the\n",
        "    `nn.Module` in Pytorch. Special case of Neural Networks\n",
        "-   In this case, we can just use a prebuilt linear approximation from\n",
        "    $\\mathbb{R}^M \\to \\mathbb{R}^1$ without an affine constant term.\n",
        "-   The underlying parameters will have a random initialization, which\n",
        "    becomes **crucial** with overparameterized models (but wouldn’t be\n",
        "    important here)"
      ],
      "id": "daa89986-309b-430c-af58-07f110b83b9c"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear(in_features=2, out_features=1, bias=False)"
          ]
        }
      ],
      "source": [
        "model = nn.Linear(M, 1, bias=False)  # random initialization\n",
        "print(model)"
      ],
      "id": "baa71b81"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optimizer\n",
        "\n",
        "-   First-order optimizers take steps using gradient estimates\n",
        "-   In many ML applications you will want control over the process, so\n",
        "    will manually call the function to collect the gradient estimate\n",
        "    then call the optimizer to take the next step\n",
        "-   Here we will just use SGD with a fixed learning rate\n",
        "-   Note that the optimizer is constructed to look directly at the\n",
        "    `parameters` for your underlying model(s)"
      ],
      "id": "4403577f-c972-4ea5-9725-6cc4394f5352"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "output-location": "column"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SGD (\n",
            "Parameter Group 0\n",
            "    dampening: 0\n",
            "    differentiable: False\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    lr: 0.001\n",
            "    maximize: False\n",
            "    momentum: 0\n",
            "    nesterov: False\n",
            "    weight_decay: 0\n",
            ")"
          ]
        }
      ],
      "source": [
        "optimizer = optim.SGD(\n",
        "    model.parameters(), lr=0.001\n",
        ")\n",
        "print(optimizer)"
      ],
      "id": "bb8311aa"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Loop\n",
        "\n",
        "-   Finally, we can loop for multiple “epochs” of passes through the\n",
        "    data with the dataloader, calling the optimizer to update each time\n",
        "-   The `for ... in train_loader:` will repeat until the end of the data\n",
        "    and continue to the next epoch (i.e., pass through data)\n",
        "-   Each batch updates a `step` using the optimizer, which is unaware of\n",
        "    epochs/batches/etc."
      ],
      "id": "194f95a9-2a3d-4535-b43d-bfdae67b7387"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "||theta - theta_hat|| = 2.5889024982461706e-05"
          ]
        }
      ],
      "source": [
        "for epoch in range(300):\n",
        "    for X_batch, Y_batch in train_loader:\n",
        "        optimizer.zero_grad() \n",
        "        loss = residuals(model, X_batch, Y_batch)  # primal\n",
        "        loss.backward()  # backprop/reverse-mode AD\n",
        "        # Now the model.parameters have gradients updated, so...\n",
        "        optimizer.step()  # Update the optimizers internal parameters\n",
        "print(f\"||theta - theta_hat|| = {torch.norm(theta - model.weight.squeeze())}\")        "
      ],
      "id": "3902fd2d"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# More Pytorch Linear Regression Examples\n",
        "\n",
        "## Gradient Descent\n",
        "\n",
        "-   See\n",
        "    [examples/linear_regression_pytorch_gd.py](examples/linear_regression_pytorch_gd.py)\n",
        "-   Simulates data and shows the basic training loop\n",
        "-   Using the “full batch” to calculate the residuals\n",
        "\n",
        "## Stochastic Gradient Descent\n",
        "\n",
        "-   See\n",
        "    [examples/linear_regression_pytorch_sgd.py](examples/linear_regression_pytorch_sgd.py)\n",
        "-   This takes the existing code, but adds in code to calculate gradient\n",
        "    estimates using minibatches\n",
        "-   Note that this is creating batches by shuffling the data and the\n",
        "    going through it `batch_size` chunks at a time\n",
        "-   When it gets to the end of that data, it is the end of the `epoch`\n",
        "\n",
        "## Adam + Bells and Whistles\n",
        "\n",
        "-   See\n",
        "    [examples/linear_regression_pytorch_adam.py](examples/linear_regression_pytorch_adam.py)\n",
        "-   This extends the previous version and adds in the full\n",
        "    train/val/test datasplit\n",
        "    -   The `val_loss` is collected and displayed at the end of each\n",
        "        epoch\n",
        "-   It also shows a learning rate scheduler and a few utilities for\n",
        "    logging and early stopping\n",
        "\n",
        "## Logging with Weights and Biases\n",
        "\n",
        "-   See\n",
        "    [examples/linear_regression_pytorch_logging.py](examples/linear_regression_pytorch_logging.py)\n",
        "\n",
        "-   This adds in support for Weights and Biases, and also demonstrates\n",
        "    the use of a custom `nn.Module` for the hypothesis class\n",
        "\n",
        "    1.  Go to [wandb.ai](https://wandb.ai/) and create an account,\n",
        "        ideally linked to your github\n",
        "    2.  Ensure you have installed the packages with\n",
        "        `pip install -r requirements.txt`\n",
        "    3.  Run `wandb login` in terminal to connect to your account\n",
        "\n",
        "-   You will then be able to run these files and see results on\n",
        "    [wandb.ai](https://wandb.ai/)\n",
        "\n",
        "## Pytorch Lightning\n",
        "\n",
        "-   See\n",
        "    [examples/linear_regression_pytorch_lightning.py](examples/linear_regression_pytorch_lightning.py)\n",
        "-   See\n",
        "    [examples/linear_regression_pytorch_lightning_defaults.yaml](examples/linear_regression_pytorch_lightning_defaults.yaml)\n",
        "    for default HPO and parameters\n",
        "-   This is using many features in [Pytorch\n",
        "    Lightning](https://www.pytorchlightning.ai/) to simplify the code\n",
        "-   The optimizer, stopping rules, logging, learning rate scheduler,\n",
        "    etc. are all handled by the framework and can be configured in that\n",
        "    file or on the CLI.\n",
        "-   Can change values on commandline to override the `yaml` file,\n",
        "\n",
        "``` bash\n",
        "python lectures/examples/linear_regression_pytorch_lightning.py --optimizer.lr=0.0001 --model.N=500\n",
        "python lectures/examples/linear_regression_pytorch_lightning.py --trainer.max_epochs=500\n",
        "```\n",
        "\n",
        "## Sweeps with W&B\n",
        "\n",
        "-   See\n",
        "    [examples/linear_regression_pytorch_sweep.yaml](lectures/examples/linear_regression_pytorch_sweep.yaml)\n",
        "    for a [sweep file](https://docs.wandb.ai/guides/sweeps) which\n",
        "    provides a HPO experiment to run and log\n",
        "    -   Executes variations on `--model.batch_size=32` and\n",
        "        `--model.lr=0.001` etc\n",
        "    -   Tries to choose them to minimize the `val_loss` as a HPO\n",
        "        objective\n",
        "    -   First, create the sweep,\n",
        "        `wandb sweep lectures/examples/linear_regression_pytorch_sweep.yaml`\n",
        "    -   Then run `wandb agent <sweep_id>` with returned sweep id\n",
        "    -   Call `wandb agent <sweep_id>` on multiple computers to run in\n",
        "        parallel\n",
        "-   Any CLI implementation can be used with these sorts of frameworks\n",
        "\n",
        "# Linear Regression with Raw JAX\n",
        "\n",
        "## Packages\n",
        "\n",
        "-   `optax` is a common package for ML optimization methods"
      ],
      "id": "9039be15-bd62-4957-acd7-7379bf698903"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import grad, jit, value_and_grad, vmap\n",
        "from jax import random\n",
        "import optax\n",
        "import jax_dataloader as jdl\n",
        "from jax_dataloader.loaders import DataLoaderJAX"
      ],
      "id": "aea3df45"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Simulate Data\n",
        "\n",
        "-   Few differences here, except for manual use of the `key`\n",
        "-   Remember that if you use the same `key` you get the same value.\n",
        "-   See [JAX\n",
        "    docs](https://jax.readthedocs.io/en/latest/jax-101/05-random-numbers.html)\n",
        "    for more details"
      ],
      "id": "d0be04a2-e4fb-40cd-b8fa-16db5efda20a"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "N = 500  # samples\n",
        "M = 2\n",
        "sigma = 0.001\n",
        "key = random.PRNGKey(42)\n",
        "# Pattern: split before using key, replace name \"key\"\n",
        "key, *subkey = random.split(key, num=4)\n",
        "theta = random.normal(subkey[0], (M,))\n",
        "X = random.normal(subkey[1], (N, M))\n",
        "Y = X @ theta + sigma * random.normal(subkey[2], (N,))  # Adding noise"
      ],
      "id": "915fc1ee"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataloaders Provide Batches\n",
        "\n",
        "-   For more complicated data (e.g. images, text) JAX can use other\n",
        "    packages, but it doesn’t have a canonical dataloader at this point\n",
        "-   But in this case we can manually create this, using\n",
        "    [`yield`](https://docs.python.org/3/howto/functional.html#generators)"
      ],
      "id": "53fb9c34-4e05-4a4e-84f5-6ee6cc7a4019"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "output-location": "column"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Array([[ 0.05907545, -1.7277497 ],\n",
            "       [-1.3816313 ,  0.33074763],\n",
            "       [ 0.76224667, -0.07191363],\n",
            "       [-0.46871892,  0.24884424]], dtype=float32), Array([-1.3571022 ,  0.07165897,  0.0480412 ,  0.13437417], dtype=float32))"
          ]
        }
      ],
      "source": [
        "def data_loader(key, X, Y, batch_size):\n",
        "    N = X.shape[0]\n",
        "    assert N == Y.shape[0]\n",
        "    indices = jnp.arange(N)\n",
        "    indices = random.permutation(key, indices)\n",
        "    # Loop over batches and yield\n",
        "    for i in range(0, N, batch_size):\n",
        "        b_indices = indices[i:i + batch_size]\n",
        "        yield X[b_indices], Y[b_indices]\n",
        "# e.g. iterate and get first element\n",
        "dl_test = data_loader(key, X, Y, 4)\n",
        "print(next(iter(dl_test)))"
      ],
      "id": "50b3fa8f"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hypothesis Class\n",
        "\n",
        "-   The “Hypothesis Class” for our ERM approximation is linear in this\n",
        "    case\n",
        "-   JAX is functional and non-mutating, so you must write stateless code\n",
        "-   We will move towards a more general class with the `equinox`\n",
        "    package, but for now we will implement the model with the parameters\n",
        "    directly\n",
        "-   The underlying parameters will have a random initialization, which\n",
        "    becomes **crucial** with overparameterized models (but wouldn’t be\n",
        "    important here)"
      ],
      "id": "a40ce0f0-e138-46e8-9ab5-c9af3568722c"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "theta_0 = [ 1.7535115  -0.07298409], theta = [0.1378821  0.79073715]"
          ]
        }
      ],
      "source": [
        "def predict(theta, X):\n",
        "    return jnp.matmul(X, theta) #or jnp.dot(X, theta)\n",
        "\n",
        "# Need to randomize our own theta_0 parameters\n",
        "key, subkey = random.split(key)\n",
        "theta_0 = random.normal(subkey, (M,))\n",
        "print(f\"theta_0 = {theta_0}, theta = {theta}\")"
      ],
      "id": "78ac1dd7"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loss Function for Gradient Descent\n",
        "\n",
        "-   Reminder: need to provide AD-able functions which give a gradient\n",
        "    estimate, not necessarily the objective itself!\n",
        "-   In particular, for LLS we simply can find the MSE between the\n",
        "    prediction and the data for the batch itself\n",
        "-   For now, we are passing the `params` rather than the `model` itself"
      ],
      "id": "72eed72b-f486-4598-9e0c-a81fd319b737"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "def vectorized_residuals(params, X, Y):\n",
        "    Y_hat = predict(params, X)\n",
        "    return jnp.mean((Y_hat - Y) ** 2)"
      ],
      "id": "2d9ca55c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optimizer\n",
        "\n",
        "-   The `optimizer.init(theta_0)` provides the initial state for the\n",
        "    iterations\n",
        "-   With SGD it is empty, but with momentum/etc. it will have internal\n",
        "    state"
      ],
      "id": "3255b68a-a349-4950-baf7-d6010e0447c1"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "output-location": "column"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimizer state:(EmptyState(), EmptyState())"
          ]
        }
      ],
      "source": [
        "lr = 0.001\n",
        "batch_size = 16\n",
        "num_epochs = 201\n",
        "\n",
        "# optax.adam(lr) is worse here\n",
        "optimizer = optax.sgd(lr)\n",
        "opt_state = optimizer.init(theta_0)\n",
        "print(f\"Optimizer state:{opt_state}\")\n",
        "params = theta_0 # initial condition"
      ],
      "id": "00c96302"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Using Optimizer for a Step\n",
        "\n",
        "-   Here we write a (compiled) utility function which:\n",
        "    1.  Calculates the loss and gradient estimates for the batch\n",
        "    2.  Updates the optimizer state\n",
        "    3.  Applies the updates to the parameters\n",
        "    4.  Returns the updated parameters, optimizer state, and loss\n",
        "-   The reason to set this up as a function is to maintain JAXs “pure”\n",
        "    style"
      ],
      "id": "97bd4e24-a750-46c0-9f85-a7af0b3c9a24"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "@jax.jit\n",
        "def make_step(params, opt_state, X, Y):\n",
        "  loss_value, grads = jax.value_and_grad(vectorized_residuals)(params, X, Y)\n",
        "  updates, opt_state = optimizer.update(grads, opt_state, params)\n",
        "  params = optax.apply_updates(params, updates)\n",
        "  return params, opt_state, loss_value"
      ],
      "id": "b0c285ec"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Loop Version 1\n",
        "\n",
        "-   Note that unlike Pytorch the gradients are passed as parameters"
      ],
      "id": "59a8c9d3-94cc-4026-8432-c67b012a304c"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0,||theta - theta_hat|| = 1.714521050453186\n",
            "Epoch 100,||theta - theta_hat|| = 0.0020757941529154778\n",
            "Epoch 200,||theta - theta_hat|| = 6.367397145368159e-05\n",
            "||theta - theta_hat|| = 6.367397145368159e-05"
          ]
        }
      ],
      "source": [
        "for epoch in range(num_epochs):\n",
        "    key, subkey = random.split(key) # changing key for shuffling each epoch\n",
        "    train_loader = data_loader(subkey, X, Y, batch_size)\n",
        "    for X_batch, Y_batch in train_loader:\n",
        "        params, opt_state, train_loss = make_step(params, opt_state, X_batch, Y_batch)  \n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"Epoch {epoch},||theta - theta_hat|| = {jnp.linalg.norm(theta - params)}\")\n",
        "\n",
        "print(f\"||theta - theta_hat|| = {jnp.linalg.norm(theta - params)}\")"
      ],
      "id": "e091e201"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Auto-Vectorizing\n",
        "\n",
        "-   In the above case the `vectorized_residuals` was able to use a\n",
        "    directly vectorized function.\n",
        "-   However in many cases it will be more convenient to write code for a\n",
        "    single element of the finite-sum objectives\n",
        "-   Now we will rewrite our objective to demonstrate how to use `vmap`"
      ],
      "id": "dee8da07-8a24-4375-aee8-c418861c80da"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.021030858\n",
            "3.5461223"
          ]
        }
      ],
      "source": [
        "def residual(theta, x, y):\n",
        "    y_hat = predict(theta, x)\n",
        "    return (y_hat - y) ** 2\n",
        "\n",
        "@jit\n",
        "def residuals(theta, X, Y):\n",
        "    # Use vmap, fixing the 1st argument\n",
        "    batched_residuals = jax.vmap(residual, in_axes=(None, 0, 0))\n",
        "    return jnp.mean(batched_residuals(theta, X, Y))\n",
        "print(residual(theta_0, X[0], Y[0]))\n",
        "print(residuals(theta_0, X, Y))"
      ],
      "id": "43846107"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## New Step and Initialization\n",
        "\n",
        "-   This simply changes the function used for the `value_and_grad` call\n",
        "    to use the new `residuals` function and resets our optimizer"
      ],
      "id": "cbe8bdca-1a0d-4ee4-abce-721cf6d9bc48"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "@jax.jit\n",
        "def make_step(params, opt_state, X, Y):     \n",
        "  loss_value, grads = jax.value_and_grad(residuals)(params, X, Y)\n",
        "  updates, opt_state = optimizer.update(grads, opt_state, params)\n",
        "  params = optax.apply_updates(params, updates)\n",
        "  return params, opt_state, loss_value\n",
        "optimizer = optax.sgd(lr) # better than optax.adam here\n",
        "opt_state = optimizer.init(theta_0)\n",
        "params = theta_0"
      ],
      "id": "73c459bd"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Loop Version 2\n",
        "\n",
        "-   Otherwise the training loop is the same"
      ],
      "id": "194afe3e-fd2a-4a05-9e57-6c4b5501e1ac"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0,||theta - theta_hat|| = 1.7061582803726196\n",
            "Epoch 100,||theta - theta_hat|| = 0.002027335576713085\n",
            "Epoch 200,||theta - theta_hat|| = 6.216356268851086e-05\n",
            "||theta - theta_hat|| = 6.216356268851086e-05"
          ]
        }
      ],
      "source": [
        "for epoch in range(num_epochs):\n",
        "    key, subkey = random.split(key) # changing key for shuffling each epoch\n",
        "    train_loader = data_loader(subkey, X, Y, batch_size)\n",
        "    for X_batch, Y_batch in train_loader:\n",
        "        params, opt_state, train_loss = make_step(params, opt_state, X_batch, Y_batch)  \n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"Epoch {epoch},||theta - theta_hat|| = {jnp.linalg.norm(theta - params)}\")\n",
        "\n",
        "print(f\"||theta - theta_hat|| = {jnp.linalg.norm(theta - params)}\")"
      ],
      "id": "5f618447"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## JAX Examples\n",
        "\n",
        "-   See\n",
        "    [examples/linear_regression_jax_sgd.py](examples/linear_regression_jax_sgd.py)\n",
        "    -   This implements the inline code above without the vmap\n",
        "-   See\n",
        "    [examples/linear_regression_jax_vmap.py](examples/linear_regression_jax_vmap.py)\n",
        "    -   This implements the `vmap` as above\n",
        "    -   This also adds in an [learning rate\n",
        "        schedule](https://optax.readthedocs.io/en/latest/api.html#optimizer-schedules)\n",
        "-   See\n",
        "    [examples/linear_regression_jax_nnx.py](examples/linear_regression_jax_nnx.py)\n",
        "    and\n",
        "    [examples/linear_regression_jax_nnx_split.py](examples/linear_regression_jax_nnx_split.py)\n",
        "    for ones using the Flax NNX\n",
        "\n",
        "# Linear Regression with Flax\n",
        "\n",
        "## Flax NNX\n",
        "\n",
        "-   While it seems convenient to work in a functional style, when we\n",
        "    move towards nested, deep approximations it can become cumbersome to\n",
        "    manage the parameters\n",
        "-   [Flax](https://flax.readthedocs.io/en/latest/index.html) is a\n",
        "    package which provides flexible ways to define and work with\n",
        "    function approximations\n",
        "    -   There is a newer (NNX) and older (Linen) interface. Use NNX.\n",
        "-   We will also introduce a DataLoader class to remove boilerplate\n",
        "\n",
        "## Hypothesis Class\n",
        "\n",
        "-   We are moving towards Neural Networks, which are a very broad class\n",
        "    of approximations.\n",
        "-   Here lets just use a linear approximation with no constant term\n",
        "-   As always, the initial randomization will become increasingly\n",
        "    important"
      ],
      "id": "eed22d0f-9208-4edb-84d9-8dbe6f9dd600"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Param(\n",
            "  value=Array([[-0.19977082],\n",
            "         [-0.613821  ]], dtype=float32)\n",
            ")"
          ]
        }
      ],
      "source": [
        "N, M, sigma = 500, 2, 0.001\n",
        "rngs = nnx.Rngs(42)\n",
        "model = nnx.Linear(M, 1, use_bias=False, rngs=rngs)\n",
        "print(model.kernel) # the initial parameters"
      ],
      "id": "8dad4f59"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Residuals Using the “Model”\n",
        "\n",
        "-   The model now contains all of the, potentially nested, parameters\n",
        "    for the approximation class\n",
        "-   It provides call notation to evaluate the function with those\n",
        "    parameters"
      ],
      "id": "ec40de32-7e3c-4704-93ef-7a41ecdc5e2b"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "def residual(model, x, y):\n",
        "    y_hat = model(x)\n",
        "    return (y_hat - y) ** 2\n",
        "\n",
        "def residuals_loss(model, X, Y):\n",
        "    return jnp.mean(jax.vmap(residual, in_axes=(None, 0, 0))(model, X, Y))\n",
        "theta = random.normal(rngs(), (M,))\n",
        "X = random.normal(rngs(), (N, M))\n",
        "Y = X @ theta + sigma * random.normal(rngs(), (N,))"
      ],
      "id": "daa679e9"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Gradients of Models\n",
        "\n",
        "-   As discussed, we can find the gradients of richer objects than just\n",
        "    arrays\n",
        "-   Optimizer updates use perturbations of the underlying PyTree\n",
        "-   Updates can be applied because the type of the gradients matches the\n",
        "    underlying PyTree"
      ],
      "id": "fb288c7f-86e7-4e32-9db2-90622d586c9b"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "output-location": "column"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "State({\n",
            "  'bias': VariableState(\n",
            "    type=Param,\n",
            "    value=None\n",
            "  ),\n",
            "  'kernel': VariableState(\n",
            "    type=Param,\n",
            "    value=Array([[-0.63399523],\n",
            "           [-0.02216433]], dtype=float32)\n",
            "  )\n",
            "})"
          ]
        }
      ],
      "source": [
        "grads = nnx.grad(residuals_loss)(model, X, Y)\n",
        "print(grads)"
      ],
      "id": "bc77782d"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup Optimizer and Training Step\n",
        "\n",
        "-   The DataLoaderJAX will give us sampled batches without custom code\n",
        "-   Note the `@nnx.jit` and `@nnx.vmap` which replace the `@jax.`\n",
        "    versions"
      ],
      "id": "be451f45-aeef-4759-b161-5780b93af439"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "@nnx.jit\n",
        "def train_step(model, optimizer, X, Y):\n",
        "    def loss_fn(model):\n",
        "        return residuals_loss(model, X, Y)\n",
        "    loss, grads = nnx.value_and_grad(loss_fn)(model)\n",
        "    optimizer.update(grads)\n",
        "    return loss\n",
        "optimizer = nnx.Optimizer(model, optax.sgd(0.001))\n",
        "dataset = jdl.ArrayDataset(X, Y) # automatic data loader\n",
        "train_loader = DataLoaderJAX(dataset, batch_size=1024, shuffle=True)"
      ],
      "id": "b5c02453"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run Optimizer with DataLoaderJAX\n",
        "\n",
        "-   Run optimizer and extract the parameters in the `model`"
      ],
      "id": "f2621600-aff0-4aed-a1d3-9e3551d99d01"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0,||theta-theta_hat|| = 0.31769511103630066\n",
            "Epoch 100,||theta-theta_hat|| = 0.2602464258670807\n",
            "Epoch 200,||theta-theta_hat|| = 0.21318809688091278\n",
            "Epoch 300,||theta-theta_hat|| = 0.17464040219783783\n",
            "Epoch 400,||theta-theta_hat|| = 0.1430637687444687\n",
            "||theta - theta_hat|| = 0.11743119359016418"
          ]
        }
      ],
      "source": [
        "for epoch in range(500):\n",
        "    for X_batch, Y_batch in train_loader:\n",
        "        loss = train_step(model, optimizer, X_batch, Y_batch)\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "        norm_diff = jnp.linalg.norm(theta - jnp.squeeze(model.kernel.value))\n",
        "        print(f\"Epoch {epoch},||theta-theta_hat|| = {norm_diff}\")\n",
        "norm_diff = jnp.linalg.norm(theta - jnp.squeeze(model.kernel.value))\n",
        "print(f\"||theta - theta_hat|| = {norm_diff}\")"
      ],
      "id": "78d2761a"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define a Custom Type\n",
        "\n",
        "-   “Neural Networks” are custom types which nest parameterized function\n",
        "    calls\n",
        "-   Nest calls to other `nnx.Module` or create/use differentiable\n",
        "    `nnx.Param`"
      ],
      "id": "ee898a20-8664-468e-9347-5c98d563eebc"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MyLinear(nnx.Module):\n",
        "    def __init__(self, in_size, out_size, rngs):\n",
        "        self.out_size = out_size\n",
        "        self.in_size = in_size\n",
        "        self.kernel = nnx.Param(jax.random.normal(rngs(), (self.out_size, self.in_size)))\n",
        "    # Similar to Pytorch's forward\n",
        "    def __call__(self, x):\n",
        "        return self.kernel @ x\n",
        "\n",
        "model = MyLinear(M, 1, rngs = rngs)"
      ],
      "id": "e9d2debb"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Same Optimization Loop"
      ],
      "id": "24733d6a-4098-47c6-b8ba-6a8caa2a0ec2"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0,||theta-theta_hat|| = 2.498084306716919\n",
            "Epoch 100,||theta-theta_hat|| = 2.0521292686462402\n",
            "Epoch 200,||theta-theta_hat|| = 1.6858018636703491\n",
            "Epoch 300,||theta-theta_hat|| = 1.384881615638733\n",
            "Epoch 400,||theta-theta_hat|| = 1.1376869678497314\n",
            "||theta - theta_hat|| = 0.9364640712738037"
          ]
        }
      ],
      "source": [
        "optimizer = nnx.Optimizer(model, optax.sgd(0.001))\n",
        "for epoch in range(500):\n",
        "    for X_batch, Y_batch in train_loader:\n",
        "        loss = train_step(model, optimizer, X_batch, Y_batch)\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "        norm_diff = jnp.linalg.norm(theta - jnp.squeeze(model.kernel.value))\n",
        "        print(f\"Epoch {epoch},||theta-theta_hat|| = {norm_diff}\")\n",
        "norm_diff = jnp.linalg.norm(theta - jnp.squeeze(model.kernel.value))\n",
        "print(f\"||theta - theta_hat|| = {norm_diff}\")"
      ],
      "id": "7e79836a"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Filtering Transformations\n",
        "\n",
        "-   Much of the NNX package is built around\n",
        "    [filtering](https://flax.readthedocs.io/en/latest/guides/filters_guide.html)\n",
        "    members of the underlying python class\n",
        "-   Within an `nnx.Module` the `nnx.Param` are values which you might\n",
        "    look to differentiate, others are fixed\n",
        "-   Since JAX code is (primarily) “pure” and functional, a key part of\n",
        "    the package is to split and recombine parameters intended for\n",
        "    gradients from those which are not\n",
        "\n",
        "## Splitting into Differentiable Parameters\n",
        "\n",
        "-   For our custom type, the fields are `out_size, in_size, kernel`. We\n",
        "    only want to differentate the `kernel` since wrapped in `nnx.Param`\n",
        "-   To separate out parameters use `nnx.split` and to recombine use\n",
        "    `nnx.merge`"
      ],
      "id": "ef5a4f15-6272-4944-92b6-faed8fd3db28"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "output-location": "column"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GraphDef(\n",
            "  nodedef=NodeDef(\n",
            "    type=MyLinear,\n",
            "    index=0,\n",
            "    attributes=('in_size', 'kernel', 'out_size'),\n",
            "    subgraphs={},\n",
            "    static_fields={\n",
            "      'in_size': 2,\n",
            "      'out_size': 1\n",
            "    },\n",
            "    leaves={\n",
            "      'kernel': 1\n",
            "    },\n",
            "    metadata=<class '__main__.MyLinear'>\n",
            "  ),\n",
            "  index_mapping=None\n",
            ")"
          ]
        }
      ],
      "source": [
        "model = MyLinear(M, 1, rngs = rngs)\n",
        "graphdef, state = nnx.split(model)\n",
        "print(graphdef)"
      ],
      "id": "acdf4097"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Merging\n",
        "\n",
        "-   `graphdef` was the fixed structure, `state` is the differentiable\n",
        "-   Use `nnx.merge` to combine the fixed and differentiable parts"
      ],
      "id": "21e76b95-d16e-4f92-9599-e5108bfc9073"
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "output-location": "column"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "State({\n",
            "  'kernel': VariableState(\n",
            "    type=Param,\n",
            "    value=Array([[1.1462003, 1.0469387]], dtype=float32)\n",
            "  )\n",
            "})\n",
            "MyLinear(\n",
            "  in_size=2,\n",
            "  kernel=Param(\n",
            "    value=Array(shape=(1, 2), dtype=float32)\n",
            "  ),\n",
            "  out_size=1\n",
            ")"
          ]
        }
      ],
      "source": [
        "print(state)\n",
        "# Emulate a \"gradient\" update\n",
        "def apply_fake_gradient(param):\n",
        "    return param + 0.01\n",
        "# Apply \"gradient\" update to tree\n",
        "state_2 = jax.tree_util.tree_map(\n",
        "               apply_fake_gradient, state)\n",
        "# Combine to form a model\n",
        "model_2 = nnx.merge(graphdef, state_2)\n",
        "print(model_2)"
      ],
      "id": "ed42a346"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## More Advanced Optimization Loops\n",
        "\n",
        "-   Filtering is often automated by replacing `jax` with `nnx`\n",
        "    equivalents\n",
        "    -   `nnx.jit, nnx.value_and_grad` etc. automatically filter for\n",
        "        Params\n",
        "-   This process provides some overhead, so for high-speed\n",
        "    [examples](https://github.com/google/flax/blob/main/examples/nnx_toy_examples/01_functional_api.py)\n",
        "    may manually split and merge\n",
        "\n",
        "# Linear Regression with Equinox\n",
        "\n",
        "## Using an Equinox Model\n",
        "\n",
        "-   Another option for NNs is\n",
        "    [Equinox](https://github.com/patrick-kidger/equinox)\n",
        "-   Key to the design of equinox is that if we use that the nested\n",
        "    parameters of an approximation class are PyTrees, then we can find\n",
        "    the derivative with respect to that entire type. See our previous\n",
        "    example on differentiating PyTrees\n",
        "-   Despite its strengths, Equinox has fewer maintainers than Flax NNX\n",
        "    -   Many similarities in the design\n",
        "    -   Has a good ecosystem of more scientific packages, which is a\n",
        "        benefit\n",
        "-   See\n",
        "    [examples/linear_regression_jax_equinox.py](examples/linear_regression_jax_equinox.py)\n",
        "\n",
        "## Equinox Macros\n",
        "\n",
        "-   There are several macros in equinox which make it easier to work\n",
        "    with differentiable PyTrees, all of which have the name `filter_`\n",
        "    -   Loosely: these macros go through the underlying python\n",
        "        datastructures and filter out values into static values vs. ones\n",
        "        which could be perturbed\n",
        "    -   For example, if I have a type with a `jnp.array` for the\n",
        "        differentiable weights and a `int` which stores the number of\n",
        "        hidden dimensions, then the `filter_grad` will flag the\n",
        "        `jnp.array` as differentiable and the `int` will have the\n",
        "        gradient type as `None`\n",
        "-   When in doubt, always replace them. e.g. `@jax.jit` can be replied\n",
        "    with `@equinox.filter_jit`, `jax.vmap` should be replaced with\n",
        "    `eqx.filter_vmap`, etc.\n",
        "\n",
        "## Hypothesis Class\n",
        "\n",
        "-   We are moving towards Neural Networks, which are a very broad class\n",
        "    of approximations.\n",
        "-   Here lets just use a linear approximation with no constant term\n",
        "-   As always, the initial randomization will become increasingly\n",
        "    important"
      ],
      "id": "32a9963e-a383-4b96-92cb-e5481f073a3c"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-0.01143495  0.6449629 ]]"
          ]
        }
      ],
      "source": [
        "key, subkey = random.split(key)\n",
        "model = eqx.nn.Linear(M, 1, use_bias = False, key = subkey)\n",
        "print(model.weight)"
      ],
      "id": "1ccd4295"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Residuals using the Model\n",
        "\n",
        "-   The model now contains all of the, potentially nested, parameters\n",
        "    for the approximation class\n",
        "-   It provides call notation to evaluate the function with those\n",
        "    parameters"
      ],
      "id": "b2176989-3024-4863-9054-d2cea366ceea"
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "def residual(model, x, y):\n",
        "    y_hat = model(x)\n",
        "    return (y_hat - y) ** 2\n",
        "\n",
        "def residuals(model, X, Y):\n",
        "    batched_residuals = vmap(residual, in_axes=(None, 0, 0))\n",
        "    return jnp.mean(batched_residuals(model, X, Y))\n",
        "# Alternatively, could have combined and still jit/grad\n",
        "def residuals_2(model, X, Y):\n",
        "    Y_hat = vmap(model)(X).squeeze()\n",
        "    return jnp.mean((Y - Y_hat) ** 2)"
      ],
      "id": "6d91d369"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Gradients of Models\n",
        "\n",
        "-   As discussed, we can find the gradients of richer objects than just\n",
        "    arrays\n",
        "-   Optimizer updates use perturbations of the underlying PyTree\n",
        "-   Updates can be applied because the type of the gradients matches the\n",
        "    underlying PyTree"
      ],
      "id": "89615f51-9602-4705-b181-80250006471d"
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear(\n",
            "  weight=f32[1,2],\n",
            "  bias=None,\n",
            "  in_features=2,\n",
            "  out_features=1,\n",
            "  use_bias=False\n",
            ")\n",
            "[[-0.22713283  2.4636858 ]]"
          ]
        }
      ],
      "source": [
        "grads = jax.grad(residuals)(model, X, Y)\n",
        "print(grads)\n",
        "print(grads.weight)"
      ],
      "id": "b05eeb45"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup of the Optimizer and `make_step`\n",
        "\n",
        "-   The `make_step` isn’t very different, except for using a few\n",
        "    `equinox` utility functions"
      ],
      "id": "2561640c-3c5e-4d19-8010-dd1aae1cf0d4"
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "lr = 0.001\n",
        "optimizer = optax.sgd(lr)\n",
        "# Needs to remove the non-differentiable parts of the \"model\" object\n",
        "opt_state = optimizer.init(eqx.filter(model,eqx.is_inexact_array))\n",
        "\n",
        "@eqx.filter_jit\n",
        "def make_step(model, opt_state, X, Y):     \n",
        "  loss_value, grads = eqx.filter_value_and_grad(residuals)(model, X, Y)\n",
        "  updates, opt_state = optimizer.update(grads, opt_state, model)\n",
        "  model = eqx.apply_updates(model, updates)\n",
        "  return model, opt_state, loss_value"
      ],
      "id": "3c12ebee"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Loop"
      ],
      "id": "8cba5457-dab7-4f22-a85e-c81910a517a5"
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0,||theta - theta_hat|| = 1.2386810779571533\n",
            "Epoch 100,||theta - theta_hat|| = 0.25654149055480957\n",
            "Epoch 200,||theta - theta_hat|| = 0.05323007330298424\n",
            "||theta - theta_hat|| = 0.0112279849126935"
          ]
        }
      ],
      "source": [
        "num_epochs = 300\n",
        "batch_size = 64\n",
        "key, subkey = random.split(key) # will keep same key for shuffling each epoch\n",
        "for epoch in range(num_epochs):\n",
        "    key, subkey = random.split(key) # changing key for shuffling each epoch\n",
        "    train_loader = data_loader(subkey, X, Y, batch_size)\n",
        "    for X_batch, Y_batch in train_loader:\n",
        "        model, opt_state, train_loss = make_step(model, opt_state, X_batch, Y_batch)    \n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"Epoch {epoch},||theta - theta_hat|| = {jnp.linalg.norm(theta - model.weight)}\")\n",
        "\n",
        "print(f\"||theta - theta_hat|| = {jnp.linalg.norm(theta - model.weight)}\")"
      ],
      "id": "ab423035"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Custom Types\n",
        "\n",
        "-   To prepare for layers of NN, we see that this class could have been\n",
        "    written manually, layering as we see fit\n",
        "-   See\n",
        "    [examples/linear_regression_jax_equinox.py](examples/linear_regression_jax_equinox.py)\n",
        "    for more, but we could have manually created the following"
      ],
      "id": "e99ac66c-bd25-4eb0-bb87-85e13f13a497"
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MyLinear(eqx.Module):\n",
        "    weight: jax.Array\n",
        "    def __init__(self, in_size, out_size, key):\n",
        "        self.weight = jax.random.normal(key, (out_size, in_size))\n",
        "    # Similar to Pytorch's forward\n",
        "    def __call__(self, x):\n",
        "        return self.weight @ x\n",
        "\n",
        "model = MyLinear(M, 1, key = subkey)"
      ],
      "id": "9b6c3568"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": "3"
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  }
}