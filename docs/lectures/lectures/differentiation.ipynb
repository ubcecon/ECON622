{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ECON622: Computational Economics with Data Science Applications\n",
        "\n",
        "Symbolic, Numerical, and Automatic Differentiation\n",
        "\n",
        "Jesse Perla (University of British Columbia)\n",
        "\n",
        "# Overview\n",
        "\n",
        "## Why the Emphasis on Differentiation?\n",
        "\n",
        "-   Modern ML would be impossible without (1) software that makes\n",
        "    calculating gradients easy; and (2) specialized hardware\n",
        "-   Old methods, but flexible software + hardware have radically changed\n",
        "    the scale of problems we can solve\n",
        "-   You simply can’t solve large problems (or sample from\n",
        "    high-dimensional distributions) without gradients, or jacobians of\n",
        "    constraints\n",
        "-   A mental shift was towards “differentiable programming”, i.e. to\n",
        "    treat entire software programs as differentiable, nested functions\n",
        "    -   As long as you have helpful software to manage the bookkeeping\n",
        "    -   You can **differentiate almost anything** continuous, and at\n",
        "        least expectations or distributions of almost anything discrete\n",
        "\n",
        "## Types of Differentiation\n",
        "\n",
        "A few general types of differentiation\n",
        "\n",
        "1.  Numerical Differentiation (i.e., finite differences)\n",
        "\n",
        "2.  Symbolic Differentiation (i.e., chain rule and simplify\n",
        "    subexpressions by hand)\n",
        "\n",
        "3.  Automatic Differentiation (i.e., execute chain rule on computer)\n",
        "\n",
        "    -   Use the chain rule forwards vs. backwards\n",
        "    -   Think matrix-free methods\n",
        "\n",
        "4.  Sparse Differentiation (i.e., use one of the above to calculate\n",
        "    directional derivatives, potentially filling in sparse Jacobians\n",
        "    with fewer passes)\n",
        "\n",
        "# Numerical Derivatives\n",
        "\n",
        "## Finite Differences\n",
        "\n",
        "-   With $f : \\mathbb{R}^N \\to \\mathbb{R}^M$, take $e_i$ as the $i$th\n",
        "    standard basis vector\n",
        "\n",
        "$$\n",
        "\\frac{\\partial f(x)}{\\partial x_i} \\approx \\frac{f(x + \\epsilon e_i) - f(x)}{\\epsilon}\n",
        "$$\n",
        "\n",
        "-   Requires $N$ forward passes for the full $\\nabla f(x)$. Same as\n",
        "    forward-mode AD.\n",
        "-   Good rule of thumb with above is\n",
        "    $\\epsilon = \\sqrt{\\epsilon_{\\text{machine}}}$\n",
        "-   Tough tradeoffs: roundoff vs. truncation errors\n",
        "    -   $\\epsilon$ too small hit machine precision errors, especially\n",
        "        with GPUs\n",
        "    -   $\\epsilon$ too large and the approximation is bad\n",
        "-   Still useful in many cases, especially for sparse problems\n",
        "\n",
        "## More Points for More Accuracy\n",
        "\n",
        "-   Trickier in practice to handle tradeoff than you might expect\n",
        "-   Could use more points which improves accuracy at the cost of more\n",
        "    function evaluations. e.g. 5 point central differences\n",
        "\n",
        "$$\n",
        "f'(x) \\approx \\frac{-f(x-2\\epsilon) + 8f(x-\\epsilon) - 8f(x+\\epsilon) + f(x+2\\epsilon)}{12\\epsilon}\n",
        "$$\n",
        "\n",
        "-   In that case, use $\\epsilon = \\sqrt[4]{\\epsilon_{\\text{machine}}}$\n",
        "\n",
        "# Symbolic Differentiation\n",
        "\n",
        "## Roll up Your Sleeves\n",
        "\n",
        "-   Do it by hand, or use Mathematica/Sympy/etc\n",
        "-   Seems like it should always be better?\n",
        "    -   Often identical to auto-differentiation, though it gives you\n",
        "        more control over algebra with subexpressions. Prone to algebra\n",
        "        or coding errors\n",
        "    -   Substituting expressions could speed things up (or slow things\n",
        "        down)\n",
        "    -   Less overhead than many auto-differentiation methods, which may\n",
        "        lead to better performance. Or may not if you do a different\n",
        "        calculation (e.g. flatten the computational graph)\n",
        "-   Very useful in many cases, even if only for designing new AD\n",
        "    “primitives”\n",
        "\n",
        "## Sub-Expressions and Computational Graphs\n",
        "\n",
        "-   Take $f(g(x), h(g(x)))$. Would you want to substitute/simplify the\n",
        "    gradient?\n",
        "\n",
        "$$\n",
        "f'(x) = g'(x) f_1(g(x), h(g(x))) + g'(x) h'(g(x)) f_2(g(x), h(g(x)))\n",
        "$$"
      ],
      "id": "a6d8e4ec-3c27-4395-8dd6-fdcde07212cd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/html": [
              "</svg>"
            ]
          }
        }
      ],
      "source": [],
      "id": "a6d9c3e5-4279-452f-bfb5-afcd4057ca20"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Automatic Differentiation\n",
        "\n",
        "## Let the Computer Execute the Chain Rule\n",
        "\n",
        "-   Auto-differentiation/differentiable programming works on “computer\n",
        "    programs”. i.e., computational graphs are just functions\n",
        "\n",
        "    1.  Converts the program into a computational graph (i.e., nested\n",
        "        functions)\n",
        "    2.  Apply the chain rule to the computational graph recursively\n",
        "    3.  Provide library of “primitives” where the recursion stops, and\n",
        "        provides registration of new primitives to teach the computer\n",
        "        calculus\n",
        "\n",
        "Finally: many frameworks will compile the resulting sequence of\n",
        "operations to be efficient on a GPU since this is so central to deep\n",
        "learning performance\n",
        "\n",
        "## Forward and Reverse Mode\n",
        "\n",
        "-   The chain rule can be done forwards or backwards\n",
        "-   See\n",
        "    [Wikipedia](https://en.wikipedia.org/wiki/Automatic_differentiation#Forward_and_reverse_accumulation)\n",
        "    for good examples. Intuition for\n",
        "    $f : \\mathbb{R}^N \\to \\mathbb{R}^M$:\n",
        "    -   Forward-Mode: grab one of the $N$ inputs and **wiggle** it to\n",
        "        see impact on all $M$ outputs. Need $N$ passes to get full\n",
        "        Jacobian\n",
        "    -   Reverse-Mode: grab one of the $M$ outputs and **wobble** it to\n",
        "        see impact on all $N$ inputs. Need $M$ passes to get full\n",
        "        Jacobian\n",
        "-   Hence, reverse-mode is good for calculating gradients when $N \\gg M$\n",
        "    (e.g. neural networks). If $M = 1$ gradients are the same complexity\n",
        "    as evaluating the function\n",
        "-   Reverse-mode has significant overhead, so often forward-mode is\n",
        "    preferred even if $N > M$\n",
        "\n",
        "## Forward and Backwards With the Computational Graph\n",
        "\n",
        "-   See\n",
        "    [wikipedia](https://en.wikipedia.org/wiki/Automatic_differentiation#Forward_and_reverse_accumulation)\n",
        "    for classic treatment, and [ProbML:\n",
        "    Introduction](https://probml.github.io/pml-book/book1.html) Section\n",
        "    13.3 for a special case\n",
        "-   Useful to read, but missing key linear algebra interpretations that\n",
        "    are useful for understanding how to adapt AD\n",
        "-   Instead, we will think of AD as linearization/etc. and follow\n",
        "    [ProbML: Advanced\n",
        "    Topics](https://probml.github.io/pml-book/book2.html) and the JAX\n",
        "    documentation\n",
        "    -   While we won’t cover it, this is much more amenable to\n",
        "        higher-order derivatives and perturbations\n",
        "\n",
        "## Reminder: Filling in a Matrix from a Linear Operator\n",
        "\n",
        "-   A standard basis in $\\mathbb{R}^2$ is\n",
        "    $e_1 = \\begin{bmatrix} 1 & 0 \\end{bmatrix}^{\\top}$ and\n",
        "    $e_2 = \\begin{bmatrix} 0 & 1 \\end{bmatrix}^{\\top}$\n",
        "-   Given linear operator $\\mathcal{A} : \\mathbb{R}^2 \\to \\mathbb{R}^3$\n",
        "    and adjoint $\\mathcal{A}^{\\top} : \\mathbb{R}^3 \\to \\mathbb{R}^2$ how\n",
        "    can we get the underlying matrix (i.e. $A$ such that\n",
        "    $\\mathcal{A}(v) = A v$ for all $v\\in\\mathbb{R}^2$)?\n",
        "\n",
        "1.  Use the standard basis vectors $e_1, e_2$ and calculate\n",
        "    $\\mathcal{A}(e_1), \\mathcal{A}(e_2)$\n",
        "\n",
        "    -   Gives two columns of the $A$ matrix, so\n",
        "        $A = \\begin{bmatrix} \\mathcal{A}(e_1) & \\mathcal{A}(e_2) \\end{bmatrix}$\n",
        "\n",
        "2.  Use the standard basis vectors $e_1, e_2, e_3$ (now of\n",
        "    $\\mathbb{R}^3$) and calculate\n",
        "    $\\mathcal{A}^{\\top}(e_1), \\mathcal{A}^{\\top}(e_2), \\mathcal{A}^{\\top}(e_3)$\n",
        "\n",
        "    -   Gives the three columns of $A^{\\top}$,\n",
        "        i.e. $A^{\\top} = \\begin{bmatrix} \\mathcal{A}^{\\top}(e_1) & \\mathcal{A}^{\\top}(e_2) & \\mathcal{A}^{\\top}(e_3) \\end{bmatrix}$\n",
        "\n",
        "## Jacobians and Linearization\n",
        "\n",
        "-   Differentiation linearizes around a point, yielding the Jacobian\n",
        "-   i.e., for $f : \\mathbb{R}^N \\to \\mathbb{R}^M$, then\n",
        "    $x \\to \\partial f(x)$ maps to an $N \\times M$ Jacobian matrix\n",
        "    -   But remember matrix-free linear operators!\n",
        "-   Instead of the Jacobian as a matrix, think of matrix-vector products\n",
        "    and $\\partial f(x) : \\mathbb{R}^N \\to \\mathbb{R}^M$ as a linear\n",
        "    operator\n",
        "    -   Note: $x$ is the linearization point in that notation, not the\n",
        "        argument\n",
        "-   See [ProbML: Advanced\n",
        "    Topics](https://probml.github.io/pml-book/book2.html) Chapter 6\n",
        "\n",
        "## Push-Forwards and JVPs\n",
        "\n",
        "-   Denote the operator, linearized around $x$, and applied to\n",
        "    $v\\in\\mathbb{N}$ as\n",
        "\n",
        "    $$\n",
        "    (x, v) \\mapsto \\partial f(x)[v] \\in \\mathbb{R}^M\n",
        "    $$\n",
        "\n",
        "    -   This is called the “push-forward”. The Jacobian Vector Product\n",
        "        (JVP)\n",
        "    -   i.e. $\\nabla f(x) \\cdot v$, as the product of the jacobian and a\n",
        "        direction\n",
        "\n",
        "-   JAX (and others) will take an $f$ and an $x$ and compile a new\n",
        "    function from $\\mathbb{R}^N$ to $\\mathbb{R}^M$ that calculates\n",
        "    $\\partial f(x)[v]$\n",
        "\n",
        "## Adjoints, Pullbacks, and VJPs\n",
        "\n",
        "-   Just as we can transpose a linear operator, we can transpose the\n",
        "    Jacobian around the linearization point, $x$\n",
        "\n",
        "$$\n",
        "\\partial f(x)^{\\top} : \\mathbb{R}^M \\to \\mathbb{R}^N\n",
        "$$\n",
        "\n",
        "-   Which lets us define the “pullback”:\n",
        "    $(x, u) \\mapsto \\partial f(x)^{\\top}[u] \\in \\mathbb{R}^N$\n",
        "-   Just as with matrix-free linear operators, we can think of this as\n",
        "    an inner product: The Vector Jacobian Product (VJP)\n",
        "-   i.e., $u \\cdot \\nabla f(x)$ or $\\nabla f(x)^{\\top} \\cdot u$ is the\n",
        "    reason for the “adjoint” terminology\n",
        "-   JAX (and others) will take an $f$ and an $x$ and compile a new\n",
        "    function from $\\mathbb{R}^M$ to $\\mathbb{R}^N$ that calculates\n",
        "    $\\partial f(x)^{\\top}[u]$\n",
        "\n",
        "## Example of a Jacobian\n",
        "\n",
        "Let $f : \\mathbb{R}^2 \\to \\mathbb{R}^2$ be defined as\n",
        "\n",
        "$$\n",
        "f(x) \\equiv \\begin{bmatrix}\n",
        "x_1^2 + x_2^2 \\\\\n",
        "x_1 x_2\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Then\n",
        "\n",
        "$$\n",
        "\\nabla f(x) \\equiv \\begin{bmatrix}\n",
        "2 x_1 & 2 x_2 \\\\\n",
        "x_2 & x_1\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "## JVP\n",
        "\n",
        "Let $v = \\begin{bmatrix} 1 & 0 \\end{bmatrix}^{\\top}$, i.e. the $e_1$ in\n",
        "the standard basis then\n",
        "\n",
        "$$\n",
        "\\partial f(x)[v] = \\nabla f(x) \\cdot \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix}\n",
        "2 x_1 \\\\ x_2 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "-   Each gives a column of the Jacobian\n",
        "-   Could use $e_1, \\ldots, e_N$ to get the full Jacobian\n",
        "\n",
        "## VJP\n",
        "\n",
        "-   Let $u = \\begin{bmatrix} 1 & 0 \\end{bmatrix}^{\\top}$, i.e. the $e_1$\n",
        "    in the standard basis then\n",
        "\n",
        "$$\n",
        "\\partial f(x)^{\\top}[u] = \\begin{bmatrix} 1 & 0 \\end{bmatrix} \\cdot \\nabla f(x) = \\begin{bmatrix} 2 x_1 & 2 x_2 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "-   The first row of the Jacobian (or the first column of its transpose)\n",
        "-   Could use $e_1,\\ldots,e_M$ we can get the full Jacobian\n",
        "\n",
        "## Chain Rule for JVP\n",
        "\n",
        "-   Consider $f : \\mathbb{R}^N \\rightarrow \\mathbb{R}^M$ with\n",
        "    $f = c \\circ b \\circ a$\n",
        "\n",
        "$$\n",
        "\\partial f(x) = \\partial c(b(a(x))) \\circ \\partial b(a(x)) \\circ \\partial a(x)\n",
        "$$\n",
        "\n",
        "-   JVP against an input perturbation $v \\in \\mathbb{R}^N$\n",
        "-   Moving inside out because as we perturbing inputs\n",
        "\n",
        "$$\n",
        "\\partial f(x)[v] = \\partial c(b(a(x))) \\left[ \\partial b(a(x))[\\partial a(x)[v]] \\right]\n",
        "$$\n",
        "\n",
        "## Calculation Order for JVP Chain Rule\n",
        "\n",
        "$$\n",
        "\\partial f(x)[v] = \\partial c(b(a(x))) \\left[ \\partial b(a(x))[\\partial a(x)[v]] \\right]\n",
        "$$\n",
        "\n",
        "-   Calculation order inside out, recursively finding linearization\n",
        "    points:\n",
        "\n",
        "    1.  $\\partial a(x)[v]$ and $a(x)$\n",
        "    2.  $\\partial b(a(x))[\\partial a(x)[v]]$ and $b(a(x))$\n",
        "    3.  $\\partial c(b(a(x)))[\\partial b(a(x))[\\partial a(x)[v]]]$ (and\n",
        "        $c(b(a(x)))$ if required)\n",
        "\n",
        "-   Conveniently follows calculating “primal” calculation. Many ways to\n",
        "    do it (e.g. overloading, duals)\n",
        "\n",
        "-   Can calculate the “primal” and the “push-forward” at the same time\n",
        "\n",
        "## Chain Rule for VJP\n",
        "\n",
        "$$\n",
        "\\partial f(x) = \\partial c(b(a(x))) \\circ \\partial b(a(x)) \\circ \\partial a(x)\n",
        "$$\n",
        "\n",
        "-   Take the transpose,\n",
        "\n",
        "$$\n",
        "\\partial f(x)^{\\top} = \\partial a(x)^{\\top} \\circ \\partial b(a(x))^{\\top} \\circ \\partial c(b(a(x)))^{\\top}\n",
        "$$\n",
        "\n",
        "-   In particular, if we multiply by some $u \\in \\mathbb{R}^M$ (i.e.,\n",
        "    $u \\cdot \\nabla f(x)$), we get\n",
        "\n",
        "$$\n",
        "\\partial f(x)^{\\top}[u] = \\partial a(x)^{\\top} \\left[ \\partial b(a(x))^{\\top} \\left[ \\partial c(b(a(x)))^{\\top}[u] \\right] \\right]\n",
        "$$\n",
        "\n",
        "## Calculation Order for VJP Chain Rule\n",
        "\n",
        "$$\n",
        "\\partial f(x)^{\\top}[u] = \\partial a(x)^{\\top} \\left[ \\partial b(a(x))^{\\top} \\left[ \\partial c(b(a(x)))^{\\top}[u] \\right] \\right]\n",
        "$$\n",
        "\n",
        "-   Calculation order outside in (of original):\n",
        "    1.  $a(x), b(a(x)), c(b(a(x)))$ (i.e., the “primal” calculations\n",
        "        required for linearization points)\n",
        "    2.  $\\partial c(b(a(x)))^{\\top}[u]$\n",
        "    3.  $\\partial b(a(x))^{\\top}[\\partial c(b(a(x)))^{\\top}[u]]$\n",
        "    4.  $\\partial a(x)^{\\top}[\\partial b(a(x))^{\\top}[\\partial c(b(a(x)))^{\\top}[u]]]$\n",
        "-   Unlike with JVP, we need the full calculations before going\n",
        "    backwards through the computational graph at the end (i.e.,\n",
        "    “backprop” terminology)\n",
        "\n",
        "## Complexity with Reverse-Mode AD\n",
        "\n",
        "-   In principle for $f : \\mathbb{R}^N \\to \\mathbb{R}$ can calculate\n",
        "    $\\nabla f(x)$ in the same computational order as $f$ itself -\n",
        "    independent of $N$\n",
        "    -   This is a key part of the secret sauce that makes ML possible\n",
        "-   But in practice it isn’t quite so simple\n",
        "    -   Requires storage for entire “primal” graph before going\n",
        "        backwards (unlike forward-mode). Inplace operations in primal\n",
        "        often useless\n",
        "    -   Requires more complicated code to keep track of the steps in the\n",
        "        computational graph, which creates overhead\n",
        "-   This means that often forward-mode will be faster even when $N > M$\n",
        "    (e.g., mabye 50-100 dimensions, but depends)\n",
        "\n",
        "## Sparse Differentiation\n",
        "\n",
        "-   For the full Jacobian a $f : \\mathbb{R}^N \\rightarrow \\mathbb{R}^M$\n",
        "    you need either $N$ forward passes or $M$ backwards passes\n",
        "    -   But if sparse, then maybe could use better directional\n",
        "        derivatives than $e_i$\n",
        "    -   e.g. Tridiagonal matrices can be done with 3 directional\n",
        "        derivatives.\n",
        "-   See\n",
        "    [SparseDiffTools.jl](https://github.com/JuliaDiff/SparseDiffTools.jl)\n",
        "    and use\n",
        "    [FiniteDiff.jl](https://github.com/JuliaDiff/FiniteDiff.jl#tutorial-2-fast-sparse-jacobians)\n",
        "    -   See [sparsejac](https://github.com/mfschubert/sparsejac) for an\n",
        "        experimental version in JAX?\n",
        "-   Finding the right directional derivatives is hard and requires\n",
        "    knowing the sparsity pattern (and solves a problem equivalent to\n",
        "    graph coloring)\n",
        "\n",
        "# AD Implementation and Examples\n",
        "\n",
        "## Software and Implementation\n",
        "\n",
        "-   Tracking the computational graph for reverse-mode is tricky\n",
        "    (especially if there were inplace modifications)\n",
        "    -   Mutating support rare for reverse-mode, functional style typical\n",
        "-   The recursion goes forwards, backwards, or both both ways down the\n",
        "    computational graph until it hits a primitive\n",
        "    -   Recursion stops when it hits a function that has JVP/VJP\n",
        "        implemented\n",
        "-   These are two extreme cases, where in principle you can mix then\n",
        "    (e.g., an internal a function has\n",
        "    $\\mathbb{R}^N \\to \\mathbb{R}^K \\to \\mathbb{R}$, where $K \\gg N$,\n",
        "    then use forward-mode from $K \\to N$ and reverse-mode from\n",
        "    $N \\to 1$)\n",
        "-   Similar methods apply for higher order derivatives,\n",
        "    e.g. Hessian-vector products and taylor series\n",
        "\n",
        "## Pytorch\n",
        "\n",
        "-   See [Probabilistic ML: Chapter 8\n",
        "    Notebook](https://github.com/probml/pyprobml/blob/master/notebooks/book1/08/autodiff_pytorch.ipynb)\n",
        "-   Reverse-mode centric, especially convenient for neural networks but\n",
        "    can be confusing for general functions\n",
        "-   In general, you will find it the most convenient for a standard\n",
        "    supervised learning problems (e.g. neural networks with empirical\n",
        "    risk minimization)\n",
        "-   We will discuss later when we look at ML pipelines"
      ],
      "id": "1e275907-06aa-4d29-b749-da254474cc8d"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch"
      ],
      "id": "78e81028"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example with Pytorch `.backward()`\n",
        "\n",
        "-   Reverse-mode AD passes values with `requires_grad=True`\n",
        "-   Traces the intermediates, and does the AD on `.backward()`"
      ],
      "id": "35b0ed31-2e0e-43d3-8327-6a7952a7f9af"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "output-location": "column"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.0707)\n",
            "tensor(3.) tensor([ 8., -3.])"
          ]
        }
      ],
      "source": [
        "x = torch.tensor(2.0, requires_grad=True)\n",
        "# Trace computations for the \"forward\" pass\n",
        "y = torch.tanh(x)\n",
        "# Do the \"backward\" pass for Reverse-mode AD\n",
        "y.backward()\n",
        "print(x.grad)\n",
        "\n",
        "def f(x, y):\n",
        "    return x**3 + 2 * y[0]**2 - 3 * y[1] + 1\n",
        "\n",
        "x = torch.tensor(1.0, requires_grad=True)\n",
        "y = torch.tensor([2.0, 3.0],\n",
        "                  requires_grad=True)\n",
        "z = f(x, y)\n",
        "z.backward()\n",
        "print(x.grad, y.grad)"
      ],
      "id": "cdd32950"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## JAX\n",
        "\n",
        "-   Very flexible with high level tools (e.g. `grad` is\n",
        "    $\\mathbb{R}^N \\to \\mathbb{R}$ reverse-diff) as well as lower-level\n",
        "    functions to directly use `jvp`, `vjp`, and hessian-vector products\n",
        "-   Emphasizing JAX here because non-trivial algorithms will typically\n",
        "    require more flexibility to scale (e.g., cross-derivatives,\n",
        "    matrix-free, etc.)\n",
        "-   Easier to use for general functions rather than in standard\n",
        "    estimation pipelines\n",
        "-   See [JAX Autodiff\n",
        "    Cookbook](https://jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html)\n",
        "-   See [Probabilistic ML: Chapter 8\n",
        "    Notebook](https://github.com/probml/pyprobml/blob/master/notebooks/book1/08/autodiff_jax.ipynb)\n",
        "-   See [JAX Advanced\n",
        "    Autodiff](https://jax.readthedocs.io/en/latest/jax-101/04-advanced-autodiff.html)\n",
        "\n",
        "## JAX Setup\n",
        "\n",
        "-   From [Autodiff\n",
        "    cookbook](https://jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html)\n",
        "    and [ProbML book 1 chapter\n",
        "    8](https://github.com/probml/pyprobml/blob/master/notebooks/book1/08/autodiff_jax.ipynb)\n",
        "-   Random numbers always require keys, which can be split for\n",
        "    reproducibility\n",
        "-   Use `jax.config.update('jax_enable_x64', True)` for 64bit precision\n",
        "    (default is 32bit)"
      ],
      "id": "f7f8b97e-0c76-423b-9d2f-cbc3658cba30"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "Array([-0.04950034, -0.711568  ], dtype=float32)"
            ]
          }
        }
      ],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "from jax import grad, jit, vmap\n",
        "from jax import random, vjp, jvp\n",
        "key = random.PRNGKey(0)\n",
        "subkey1, subkey2 = random.split(key)\n",
        "random.normal(subkey1, (2,))"
      ],
      "id": "6f3037ed"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## High Level Grad (i.e. Reverse-mode)\n",
        "\n",
        "-   `grad` is the high-level reverse-mode AD function\n",
        "-   Returns a new function, which could be compiled"
      ],
      "id": "e5bf6121-c9f5-4d34-942a-52b02dce4d9c"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.070650816\n",
            "0.070650816\n",
            "4.0"
          ]
        },
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "Array(4., dtype=float32, weak_type=True)"
            ]
          }
        }
      ],
      "source": [
        "grad_tanh = grad(jnp.tanh)\n",
        "print(grad_tanh(2.0))\n",
        "grad_tanh_jit = jit(grad_tanh)\n",
        "print(grad_tanh_jit(2.0))\n",
        "\n",
        "def f(x):\n",
        "    return x**3 + 2 * x**2 - 3 * x + 1\n",
        "print(grad(f)(1.0))\n",
        "@jit\n",
        "def f2(x):\n",
        "    return x**3 + 2 * x**2 - 3 * x + 1\n",
        "grad(f2)(1.0)"
      ],
      "id": "469c869c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fixing an argument"
      ],
      "id": "9b17a377-b8c5-4b62-bac2-977162676b68"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7.0\n",
            "4.0\n",
            "1.0"
          ]
        }
      ],
      "source": [
        "def f3(x, y):\n",
        "    return x**2 + y\n",
        "v, gx = jax.value_and_grad(f3, argnums=0)(2.0, 3.0)\n",
        "print(v)\n",
        "print(gx)\n",
        "\n",
        "gy = grad(f3, argnums=1)(2.0, 3.0)\n",
        "print(gy)"
      ],
      "id": "5f74ba3f"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Full Jacobians (Forward and Reverse)\n",
        "\n",
        "-   Goes through full basis forwards or backwards"
      ],
      "id": "3b439448-72ba-45f3-ada7-fca527752fa1"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True"
          ]
        }
      ],
      "source": [
        "def fun(x):\n",
        "    return jnp.dot(A, x)\n",
        "A = np.random.normal(size=(4, 3))\n",
        "x = np.random.normal(size=(3,))\n",
        "Jf = jax.jacfwd(fun)(x)\n",
        "Jr = jax.jacrev(fun)(x)\n",
        "print(np.allclose(Jf, Jr))"
      ],
      "id": "04187e01"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup for Logistic Regression"
      ],
      "id": "287b2042-cf1f-4291-b834-d61ca9211d67"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sigmoid(x):\n",
        "    return 0.5 * (jnp.tanh(x / 2) + 1)\n",
        "\n",
        "def predict(W, b, inputs):\n",
        "    return sigmoid(jnp.dot(inputs, W) + b)\n",
        "inputs = jnp.array([[0.52, 1.12,  0.77],\n",
        "                   [0.88, -1.08, 0.15],\n",
        "                   [0.52, 0.06, -1.30],\n",
        "                   [0.74, -2.49, 1.39]])\n",
        "targets = jnp.array([True, True, False, True])                   \n",
        "key, W_key, b_key = random.split(key, 3)\n",
        "W = random.normal(W_key, (3,))\n",
        "b = random.normal(b_key, ())                    "
      ],
      "id": "ee9cfe75"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## JVP"
      ],
      "id": "31d1e439-d0d2-440d-91e9-90e86ddb8214"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Array([0.13262254, 0.952067  , 0.6249393 , 0.9980987 ], dtype=float32), Array([ 0.01893247,  0.06917656, -0.00554809,  0.00501772], dtype=float32))"
          ]
        }
      ],
      "source": [
        "f = lambda W: predict(W, b, inputs)\n",
        "key, subkey = random.split(key)\n",
        "v = random.normal(subkey, W.shape)\n",
        "\n",
        "# Push forward\n",
        "y, u = jvp(f, (W,), (v,))\n",
        "print((y, u))"
      ],
      "id": "efaeb908"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## VJP"
      ],
      "id": "cb56b00d-6046-4ed1-bfba-8c88a7d0bb37"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Array([0.13262254, 0.952067  , 0.6249393 , 0.9980987 ], dtype=float32), (Array([-0.05722706, -0.07111154, -0.09522615], dtype=float32),))"
          ]
        }
      ],
      "source": [
        "y, vjp_fun = vjp(f, W)\n",
        "\n",
        "key, subkey = random.split(key)\n",
        "u = random.normal(subkey, y.shape)\n",
        "\n",
        "# Pull back\n",
        "# Note need to call function\n",
        "print((y, vjp_fun(u)))"
      ],
      "id": "9b96b044"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Differentiating PyTrees\n",
        "\n",
        "-   Key JAX feature is “flattening” of nested data\n",
        "-   Works for arbitrarily nested tree structures"
      ],
      "id": "1031353b-dda9-45de-832a-39af6cca2230"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'W': Array([-0.16965583, -0.8774644 , -1.4901346 ], dtype=float32), 'b': Array(-0.29227245, dtype=float32)}"
          ]
        }
      ],
      "source": [
        "def loss2(params_dict):\n",
        "    preds = predict(params_dict['W'], params_dict['b'], inputs)\n",
        "    label_probs = preds * targets + (1 - preds) * (1 - targets)\n",
        "    return -jnp.sum(jnp.log(label_probs))\n",
        "params = {'W': W, 'b': b}\n",
        "print(grad(loss2)(params))"
      ],
      "id": "ff15e442"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Implicit Differentiation and Custom Rules\n",
        "\n",
        "## Primal and JVP/VJP Calculations are Separate\n",
        "\n",
        "-   For a JVP or VJP, we first need to calculate the $f(x)$\n",
        "\n",
        "    -   This could involve complicated algorithms, external libraries,\n",
        "        etc.\n",
        "\n",
        "-   Often madness to descend recursively into primal calculations\n",
        "\n",
        "    -   e.g. if $f(x) = \\cos(x)$ then should it step inside $\\cos(x)$?\n",
        "    -   Alternatively, use the known derivative to find\n",
        "\n",
        "    $$\n",
        "    \\partial f(x)[v] = -\\sin(x) \\cdot v\n",
        "    $$\n",
        "\n",
        "-   AD systems all have a library of these rules, and typically a way to\n",
        "    create new ones for “custom” rules for complicated functions\n",
        "\n",
        "## Custom Rules/Primitives\n",
        "\n",
        "-   Derive the derivative by hand, and register it with the AD system\n",
        "-   See [Matrix\n",
        "    Algebra](https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf)\n",
        "    and [Matrix Derivative\n",
        "    Results](https://people.maths.ox.ac.uk/gilesm/files/AD2008.pdf), and\n",
        "    [ChainRules.jl\n",
        "    Docs](https://juliadiff.org/ChainRulesCore.jl/stable/maths/arrays.html)\n",
        "    for examples\n",
        "-   Derivations for forward-mode is relatively easier using the total\n",
        "    derivative\n",
        "-   Derivations for reverse-mode is difficult.\n",
        "    -   See tricks for reverse using the trace of the Frobenius Inner\n",
        "        product.\n",
        "-   See\n",
        "    [here](https://jax.readthedocs.io/en/latest/notebooks/Custom_derivative_rules_for_Python_code.html)\n",
        "    for JAX implementation\n",
        "\n",
        "## Smooth Matrix Functions\n",
        "\n",
        "-   Consider $f : \\mathbb{R}^{N \\times N} \\to \\mathbb{R}^{N \\times N}$\n",
        "    and following Mathias 1996 and Higham 2008\n",
        "-   Assume a suitably smooth function and a perturbation $\\delta A$,\n",
        "    where we want to calculate the forward-mode\n",
        "    $\\partial f(A)[\\delta A]$\n",
        "-   Then, apply $f(\\cdot)$ to the following $\\mathbb{R}^{2N \\times 2 N}$\n",
        "    block matrix and extract the answer form the upper right corner $$\n",
        "    f\\left(\\begin{bmatrix} A & \\delta A\\\\ 0 & A\\end{bmatrix}\\right) = \\begin{bmatrix} f(A) & \\partial f(A)[\\delta A]\\\\ 0 & f(A)\\end{bmatrix}\n",
        "    $$\n",
        "    -   This is a remarkable result true for any $\\delta A$. Not always\n",
        "        the most efficient way, but very general\n",
        "\n",
        "## Registering JVPs in JAX"
      ],
      "id": "422d2394-4aed-4c70-a83e-56ab9835c366"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.7278922\n",
            "2.7278922 -1.2484405"
          ]
        }
      ],
      "source": [
        "@jax.custom_jvp\n",
        "def f(x, y):\n",
        "  return jnp.sin(x) * y\n",
        "@f.defjvp\n",
        "def f_jvp(primals, tangents):\n",
        "  x, y = primals\n",
        "  x_dot, y_dot = tangents\n",
        "  primal_out = f(x, y)\n",
        "  tangent_out = jnp.cos(x) * x_dot * y + jnp.sin(x) * y_dot\n",
        "  return primal_out, tangent_out\n",
        "\n",
        "print(f(2., 3.))\n",
        "y, y_dot = jvp(f, (2., 3.), (1., 0.)) # perturb x, not y\n",
        "print(y, y_dot)  "
      ],
      "id": "af6c0a99"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Deriving Rules for Matrix Inverse\n",
        "\n",
        "-   Let $f : \\mathbb{R}^{N \\times N} \\to \\mathbb{R}^{N \\times N}$ where\n",
        "    $f(A) = A^{-1} = C$\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "I &= C A\\\\\n",
        "0 &= \\partial C A + C \\partial A \\\\\n",
        "0 &= \\partial C A C + C (\\partial A) C \\\\\n",
        "0 &= \\partial C + C (\\partial A) C \\\\\n",
        "\\partial C &= -C (\\partial A) C \\\\\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "-   So given the $A$, the “primal” can be calculated $C = A^{-1}$ using\n",
        "    the appropriate method (e.g. LU decomposition, cholesky, etc.)\n",
        "-   Then the forward mode AD is just matrix products\n",
        "-   Reverse is harder to derive\n",
        "    ($\\partial A = - C^{\\top} (\\partial C) C^{\\top}$)\n",
        "\n",
        "## Implicit Functions\n",
        "\n",
        "-   The implicit function theorem helps us linearize around a solution\n",
        "-   For example:\n",
        "    -   [Jaxopt](https://jaxopt.github.io)\n",
        "    -   [DSGE\n",
        "        solutions](https://github.com/HighDimensionalEconLab/DifferentiableStateSpaceModels.jl)\n",
        "    -   [Dynamax](https://github.com/probml/dynamax) for Filters and\n",
        "        State Space Models\n",
        "    -   See [Implicit Layers\n",
        "        Tutorial](http://implicit-layers-tutorial.org/implicit_functions/)\n",
        "        for fixed point example and for [differentiable\n",
        "        optimizers](http://implicit-layers-tutorial.org/differentiable_optimization/)\n",
        "\n",
        "## Differentiating a Fixed Point Solution\n",
        "\n",
        "-   Solve primal problem $z^*(a) = f(a, z^*(a))$ for $z^*(a)$ using\n",
        "    Anderson iteration, Newton, etc. fixing $a$. Use implicit function\n",
        "    theorem at $z^* \\equiv z^*(a_0)$ $$\n",
        "    \\frac{\\partial z^*(a)}{\\partial a} = \\left[ I - \\frac{\\partial f(a, z^*)}{\\partial z} \\right]^{-1} \\frac{\\partial f(a, z^*)}{\\partial a}.\n",
        "    $$\n",
        "\n",
        "-   For JVP: $(a, v) \\mapsto \\frac{\\partial z^*(a)}{\\partial a}v$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial z^*(a)}{\\partial a}\\cdot v = \\left[ I - \\frac{\\partial f(a,z^*)}{\\partial z} \\right]^{-1} \\frac{\\partial f(a, z^*)}{\\partial a}\\cdot v\n",
        "$$\n",
        "\n",
        "-   Note that this requires the gradients of $f(a, z)$ using symbolics,\n",
        "    AD, etc.\n",
        "\n",
        "## JAX Packages with Builtin Implicit Differentiation\n",
        "\n",
        "-   Most JAX and Pytorch packages will be built with AD rules"
      ],
      "id": "d8bc3b46-2267-45e8-8b55-bda324dbfb38"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-0.22139914"
          ]
        }
      ],
      "source": [
        "from jaxopt import Bisection\n",
        "@jax.jit\n",
        "def F(x, factor):\n",
        "  return factor * x ** 3 - x - 2\n",
        "\n",
        "def root(factor):\n",
        "  bisec = Bisection(optimality_fun=F, lower=1, upper=2)\n",
        "  return bisec.run(factor=factor).params\n",
        "\n",
        "# Derivative of root with respect to factor at 2.0.\n",
        "print(grad(root)(2.0))"
      ],
      "id": "482af3a3"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": "3"
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  }
}