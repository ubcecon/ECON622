{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ECON622: Computational Economics with Data Science Applications\n",
        "\n",
        "Key Terminology and Concepts in Machine Learning\n",
        "\n",
        "Jesse Perla (University of British Columbia)\n",
        "\n",
        "# Overview\n",
        "\n",
        "## Motivation\n",
        "\n",
        "-   In this lecture we will introduce some key terminology and concepts\n",
        "    in machine learning\n",
        "-   We will map these concepts to terminology in economics and\n",
        "    statistics\n",
        "-   Finally, we will discuss Python Frameworks\n",
        "\n",
        "## Textbooks\n",
        "\n",
        "-   [Probabilistic Machine Learning: An\n",
        "    Introduction](https://probml.github.io/pml-book/book1.html) by Kevin\n",
        "    Murphy\n",
        "-   [Probabilistic Machine Learning: Advanced\n",
        "    Topics](https://probml.github.io/pml-book/book2.html) by Kevin\n",
        "    Murphy\n",
        "-   Both have PDFs of the “draft” available\n",
        "-   Code is available on https://github.com/probml/pyprobml\n",
        "    -   Much in JAX, some in torch/etc.\n",
        "-   For some linear algebra/etc. examples, see the [ECON526\n",
        "    Lectures](https://ubcecon.github.io/ECON526/lectures/lectures/index.html)\n",
        "\n",
        "## “Depth” and Representations\n",
        "\n",
        "-   Could approximate a function $f(X)$ with a “shallow” approximation,\n",
        "    e.g. polynomials of $X$. Alternatively, nest functions $h(\\cdot)$\n",
        "    and $\\phi(\\cdot)$ $$\n",
        "    f(X) \\approx h(\\phi(X))\n",
        "    $$\n",
        "    -   First, the $\\phi(X)$ will transform the state into something\n",
        "        more amenable for the downstream task (e.g. prediction,\n",
        "        classification, etc.)\n",
        "    -   Then the $h(\\cdot)$ maps that transformed state into the output.\n",
        "-   Good $\\phi(\\cdot)$ efficiently calculate $h(\\cdot)$. Often reusable\n",
        "    for other tasks (e.g. $f_2(X) \\approx h_2(\\phi(X))$)\n",
        "-   For simple $X$ we can design them (e.g., take means, logs,\n",
        "    first-differences). But for rich data can we learn them from the\n",
        "    data itself?\n",
        "\n",
        "## Relevant Categories of Machine Learning\n",
        "\n",
        "-   Supervised Learning (e.g., Regression and Classification)\n",
        "-   Unsupervised Learning (e.g., Clustering, Auto-encoders)\n",
        "-   Semi-Supervised Learning (e.g., some observations)\n",
        "-   Reinforcement Learning (e.g., policy/control)\n",
        "-   Generative Models/Bayesian Methods (e.g., diffusions, probabilistic\n",
        "    programming)\n",
        "-   Instance-based learning (e.g., Kernel Methods) and Deep Learning are\n",
        "    somewhat orthogonal\n",
        "\n",
        "## Key Terminology: Features, Labels, and Latents\n",
        "\n",
        "-   **Features** are economists **explanatory or independent\n",
        "    variables**. They have the key source of variation to make\n",
        "    predictions and conduct counterfactuals\n",
        "-   **Labels** correspond to economists **observables or dependent\n",
        "    variables**\n",
        "-   **Latent Variables** are **unobserved variables**, typically sources\n",
        "    of heterogeneity or which may drive both the dependent and\n",
        "    independent variables\n",
        "-   **Feature Engineering** is the process of creating or selecting\n",
        "    features from the data that are more useful for the task at hand\n",
        "\n",
        "## Key Concepts\n",
        "\n",
        "-   20th vs. 21st Century ML\n",
        "-   Stochastic Gradients and Auto-Differentiation\n",
        "-   Implicit and Explicit Regularization\n",
        "-   Inductive/Implicit Bias\n",
        "-   Generalization\n",
        "-   Overfitting and the Bias-Variance Tradeoff\n",
        "-   Test vs. Train vs. Validation Set\n",
        "-   Hyperparameter Optimization\n",
        "-   Representation Learning\n",
        "-   Transfer Learning\n",
        "\n",
        "# Python\n",
        "\n",
        "## Why Python?\n",
        "\n",
        "-   For “modern” ML: **all** the well-supported frameworks are in Python\n",
        "-   In particular, auto-differentiation is central to many ML algorithms\n",
        "-   Why should you avoid Julia/Matlab/R in these cases?\n",
        "    -   Poor AD, especially for reverse-mode\n",
        "    -   Network effects. Very few higher level packages for ML pipeline\n",
        "    -   But Julia dominates for many ML topics (e.g. ODEs) and R is\n",
        "        outstanding for classic ML\n",
        "-   Should you use Python for more things?\n",
        "    -   Maybe, but it is limited and can be slow unless you jump through\n",
        "        hoops\n",
        "    -   Personally, if I have algorithms but no need for AD or\n",
        "        particular packages, Julia is a much better language and less\n",
        "        frustrating\n",
        "\n",
        "## There is No Such Thing as “Python”!\n",
        "\n",
        "-   Many incompatible wrappers around C++ for numerical methods\n",
        "-   Numpy/Scipy is the baseline (a common API)\n",
        "-   Pytorch\n",
        "-   JAX\n",
        "-   Ones to avoid\n",
        "    -   Tensorflow, common in industry but old\n",
        "    -   Numba (for me, reasonable people disagree)\n",
        "\n",
        "## Pytorch\n",
        "\n",
        "-   In recent years, the most flexible and popular ML framework for\n",
        "    researchers\n",
        "-   Key features:\n",
        "    -   Most of the code is for auto-differentiation/GPUs\n",
        "    -   JIT/etc. for GPU and fast kernels for deep learning\n",
        "    -   Neural Network libraries and utilities\n",
        "    -   A good subset of numpy\n",
        "    -   Utilities for ML pipelines optimization/etc.\n",
        "\n",
        "## Pytorch Key Downsides\n",
        "\n",
        "-   Not really for general purpose programming\n",
        "    -   Intended for making auto-differentiation of neural networks\n",
        "        easy, and updating gradients for solvers\n",
        "    -   May be very slow for simple things or ones which don’t involve\n",
        "        high-order AD\n",
        "-   Won’t always have packages you need for general code, and\n",
        "    compatibility is ugly\n",
        "\n",
        "## JAX\n",
        "\n",
        "-   Compiler that enables layered program transformations\n",
        "    1.  `jit` compiler to [XLA](https://www.tensorflow.org/xla/),\n",
        "        including accelerators (e.g. GPUs)\n",
        "    2.  `grad` Auto-differentiation\n",
        "    3.  `vmap` vectorization\n",
        "    4.  Flexibility to add more transformations\n",
        "-   [JAX\n",
        "    PyTrees](https://jax.readthedocs.io/en/latest/jax-101/05.1-pytrees.html)\n",
        "    provide a nested tree structure for compiler passes\n",
        "-   Closer to being a full JIT for general code than pytorch\n",
        "-   For ML, not full-featured like pytorch. Need to shop for other\n",
        "    libraries\n",
        "\n",
        "## JAX Key Downsides\n",
        "\n",
        "-   Tough to trust Google, especially since it is a research project\n",
        "    -   Too ingrained in DeepMind research to disappear, but might have\n",
        "        intermittent support\n",
        "-   Different operating system support is limited, but they are making\n",
        "    progress\n",
        "-   Subset of python. Can’t really use loops, etc. Functional-style\n",
        "    programming\n",
        "    -   Much more restrictive than it seems, and far more restrictive\n",
        "        than pytorch\n",
        "\n",
        "# Python Ecosystem\n",
        "\n",
        "## Environments\n",
        "\n",
        "-   Hate it, but nevertheless [install\n",
        "    conda](https://www.anaconda.com/download)\n",
        "-   **Always** use a virtual environment to hate conda slightly less\n",
        "-   Keep dependencies in `requirements.txt` for `pip`, conda\n",
        "    `environment.yml` if necessary\n",
        "-   To create, activate, and install packages\n",
        "\n",
        "```` markdown\n",
        "```{bash}\n",
        "conda create -n econ622 python=3.11\n",
        "conda activate econ622\n",
        "pip install -r requirements.txt\n",
        "```\n",
        "````\n",
        "\n",
        "-   In vscode, you can go `> Python: Select Interpreter`, choose\n",
        "    `econ622` and it will automatically activate\n",
        "\n",
        "## Development Environment\n",
        "\n",
        "-   pip/anaconda with virtual environments\n",
        "-   Use VSCode for debugging, testing, etc.\n",
        "-   Github Copilot. Install [Github Copilot\n",
        "    Chat](https://docs.github.com/en/copilot/github-copilot-chat/using-github-copilot-chat)\n",
        "-   Format with `black`. See\n",
        "    [here](https://code.visualstudio.com/docs/python/formatting) for\n",
        "    setup\n",
        "\n",
        "## Baseline, Safe Packages to Use\n",
        "\n",
        "-   [Numpy](https://numpy.org/doc/stable/) and\n",
        "    [Scipy](https://docs.scipy.org/doc/scipy/reference/)\n",
        "-   [Pandas](https://pandas.pydata.org/docs/) for dataframes\n",
        "-   [Matplotlib](https://matplotlib.org/stable/contents.html) for\n",
        "    general plotting\n",
        "-   [Seaborn](https://seaborn.pydata.org/) for plotting data\n",
        "-   [Statsmodels](https://www.statsmodels.org/stable/index.html) for\n",
        "    classic econometrics\n",
        "-   [Scikit-learn](https://scikit-learn.org/stable/) for classic ML\n",
        "\n",
        "## Numpy/Scipy Basics\n",
        "\n",
        "-   Using some examples from\n",
        "    [ECON526](https://ubcecon.github.io/ECON526/lectures/lectures/index.html)\n",
        "\n",
        "## General Tools for ML Pipelines\n",
        "\n",
        "-   Logging/visualization: [Weights and Biases](https://wandb.ai/site)\n",
        "    -   Sign up for an account! Built in hyperparameter optimization\n",
        "        tools\n",
        "-   CLI useful for many pipelines and HPO. See\n",
        "    [here](https://github.com/shadawck/awesome-cli-frameworks#python)\n",
        "-   For more end-to-end frameworks for deep-learning\n",
        "    -   [Pytorch Lightning](https://www.pytorchlightning.ai/) is\n",
        "        extremely easy and flexble, eliminating a lot of boilerplate for\n",
        "        CLI, optimizers, GPUs, etc.\n",
        "    -   [Keras](https://keras.io/) is a higher-level framework for deep\n",
        "        learning. Traditionally tensorflow, but now many. Also\n",
        "        [FastAI](https://www.fast.ai/)\n",
        "-   [HuggingFace](https://huggingface.co/) is a great resource for NLP\n",
        "    and transformers\n",
        "-   [Optuna](https://optuna.org/) is a great hyperparameter optimization\n",
        "    framework, etc.\n",
        "\n",
        "# JAX Ecosystem\n",
        "\n",
        "## Examples of Core Transformations\n",
        "\n",
        "From [JAX\n",
        "quickstart](https://jax.readthedocs.io/en/latest/notebooks/quickstart.html)\n",
        "\n",
        "Builtin composable transformations: `jit`, `grad` and `vmap`"
      ],
      "id": "3e003c71-cf6b-4ed0-a332-1fedc7d44a9d"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "from jax import grad, jit, vmap, random"
      ],
      "id": "46639644"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compiling with `jit`"
      ],
      "id": "b2a4717f-f9c1-4f7b-b7bb-0580ce39f5c1"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.44 ms ± 62.6 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
            "551 μs ± 13.6 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)"
          ]
        }
      ],
      "source": [
        "def selu(x, alpha=1.67, lmbda=1.05):\n",
        "  return lmbda * jnp.where(x > 0, x, alpha * jnp.exp(x) - alpha)\n",
        "key = random.PRNGKey(0)  \n",
        "x = random.normal(key, (1000000,))\n",
        "%timeit selu(x).block_until_ready()\n",
        "selu_jit = jit(selu)\n",
        "%timeit selu_jit(x).block_until_ready()"
      ],
      "id": "73ecf171"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Convenience Decorators for `jit`\n",
        "\n",
        "-   Convenience python decorator `@jit`"
      ],
      "id": "77c9a9f8-7700-4a08-b7d7-0c333d8235f2"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "542 μs ± 6.04 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)"
          ]
        }
      ],
      "source": [
        "@jit\n",
        "def selu(x, alpha=1.67, lmbda=1.05):\n",
        "  return lmbda * jnp.where(x > 0, x, alpha * jnp.exp(x) - alpha)\n",
        "%timeit selu(x).block_until_ready()"
      ],
      "id": "47f6dd10"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Differentiation with `grad`"
      ],
      "id": "2de40e0f-f75a-401d-8f47-ee820839f4f1"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.25       0.19661197 0.10499357]"
          ]
        }
      ],
      "source": [
        "def sum_logistic(x):\n",
        "  return jnp.sum(1.0 / (1.0 + jnp.exp(-x)))\n",
        "\n",
        "x_small = jnp.arange(3.)\n",
        "derivative_fn = grad(sum_logistic)\n",
        "print(derivative_fn(x_small))"
      ],
      "id": "d2afa988"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Manual “Batching”/Vectorization\n",
        "\n",
        "Common to run the same function along one dimension of an array"
      ],
      "id": "77077344-42d3-421c-8768-32bfd2dbc8a6"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "698 μs ± 2.71 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)"
          ]
        }
      ],
      "source": [
        "mat = random.normal(key, (150, 100))\n",
        "batched_x = random.normal(key, (10, 100))\n",
        "\n",
        "def f(v):\n",
        "  return jnp.dot(mat, v)\n",
        "def naively_batched_f(v_batched):\n",
        "  return jnp.stack([f(v) for v in v_batched])\n",
        "%timeit naively_batched_f(batched_x).block_until_ready()  "
      ],
      "id": "aa7924a2"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Using `vmap`\n",
        "\n",
        "The\n",
        "[vmap](https://jax.readthedocs.io/en/latest/_autosummary/jax.vmap.html#jax.vmap)\n",
        "applies across a dimension"
      ],
      "id": "68dafffa-bd9c-41c2-a62f-56aa380758aa"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Auto-vectorized with vmap\n",
            "46.3 μs ± 486 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)"
          ]
        }
      ],
      "source": [
        "@jit\n",
        "def vmap_batched_f(v_batched):\n",
        "  return vmap(f)(v_batched)\n",
        "\n",
        "print('Auto-vectorized with vmap')\n",
        "%timeit vmap_batched_f(batched_x).block_until_ready()"
      ],
      "id": "ef3bd05f"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## More `vmap`\n",
        "\n",
        "Can fix dimensions with `in_axes`"
      ],
      "id": "e65b4840-c602-4765-ba3c-180c02e39fcb"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "Array([ 0.,  3.,  6.,  9., 12.], dtype=float32)"
            ]
          }
        }
      ],
      "source": [
        "def f(a, x, y):\n",
        "  return a * x + y\n",
        "a = 2.0\n",
        "x = jnp.arange(5.)\n",
        "y = jnp.arange(5.)\n",
        "vmap(f, in_axes=(None, 0, 0))(a, x, y)"
      ],
      "id": "8f871ac2"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save `vmap` functions\n",
        "\n",
        "Can fix dimensions with `in_axes`"
      ],
      "id": "721c9b8f-2a48-4e28-a75f-bb850ec7e7a4"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "Array([ 0.,  3.,  6.,  9., 12.], dtype=float32)"
            ]
          }
        }
      ],
      "source": [
        "@jax.jit\n",
        "def f(a, x, y):\n",
        "  return a * x + y\n",
        "a = 2.0\n",
        "x = jnp.arange(5.)\n",
        "y = jnp.arange(5.)\n",
        "f_batched = vmap(f, in_axes=(None, 0, 0))\n",
        "f_batched(a, x, y)"
      ],
      "id": "97a2279c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key JAX Neural Network Libraries/Frameworks\n",
        "\n",
        "-   Neural Network Libraries\n",
        "    -   [Flax NNX](https://flax.readthedocs.io/en/latest/index.html)\n",
        "        -   NNX is the new Flax API, Linen the older one\n",
        "        -   Has momentum, supported by google (for now)\n",
        "    -   [Equinox](https://github.com/patrick-kidger/equinox)\n",
        "        -   General, not just neural networks. Similar to NNX\n",
        "    -   [Keras](https://keras.io/) supports JAX (as well as PyTorch, TF,\n",
        "        etc.)\n",
        "\n",
        "## Other ML-oriented Packages\n",
        "\n",
        "-   Tough to keep up, see [Awesome\n",
        "    JAX](https://github.com/n2cholas/awesome-jax)\n",
        "-   [Optax](https://github.com/google-deepmind/optax) for ML-style\n",
        "    optimization\n",
        "-   Checkpointing and serialization:\n",
        "    [obax](https://orbax.readthedocs.io/en/latest/)\n",
        "\n",
        "## More Scientific Computing in JAX\n",
        "\n",
        "-   [jax.scipy](https://jax.readthedocs.io/en/latest/jax.scipy.html)\n",
        "    which is a subset of scipy\n",
        "-   Nonlinear Systems/Least Squares:\n",
        "    [Optimistix](https://github.com/patrick-kidger/optimistix)\n",
        "-   Linear Systems of Equations:\n",
        "    [Lineax](https://docs.kidger.site/lineax/)\n",
        "-   Matrix-free operators for iterative solvers:\n",
        "    [COLA](https://github.com/wilson-labs/cola)\n",
        "-   Differential Equations:\n",
        "    [diffrax](https://github.com/patrick-kidger/diffrax)\n",
        "-   More general optimization and solvers:\n",
        "    [JAXopt](https://jaxopt.github.io/stable/#)\n",
        "-   Interpolation:\n",
        "    [interpax](https://interpax.readthedocs.io/en/latest/?badge=latest)\n",
        "\n",
        "## JAX Challenges\n",
        "\n",
        "-   Basically only pure functional programming\n",
        "    -   No “mutation” of vectors\n",
        "    -   Loops/conditionals are tough\n",
        "    -   Rules for what is `jit`able are tricky\n",
        "-   See [JAX - The Sharp\n",
        "    Bits](https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html)\n",
        "-   May not be faster on CPUs or for “normal” things\n",
        "-   Debugging\n",
        "\n",
        "## PyTrees\n",
        "\n",
        "-   JAX uses a generic [tree\n",
        "    structure](https://jax.readthedocs.io/en/latest/jax-101/05.1-pytrees.html).\n",
        "    Powerful but takes time to understand: Examples from:\n",
        "    [here](https://jax.readthedocs.io/en/latest/pytrees.html) and\n",
        "    [here](https://jax.readthedocs.io/en/latest/_autosummary/jax.vmap.html#jax.vmap)"
      ],
      "id": "a4913313-8407-4fdd-9c2a-178056fe9cd0"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11.0\n",
            "25.0\n",
            "[11. 25.]"
          ]
        }
      ],
      "source": [
        "f = lambda x, y: jnp.vdot(x, y)\n",
        "X = jnp.array([[1.0, 2.0],\n",
        "               [3.0, 4.0]])\n",
        "y = jnp.array([3.0, 4.0])\n",
        "print(f(X[0], y))\n",
        "print(f(X[1], y))\n",
        "\n",
        "mv = vmap(f, in_axes = (\n",
        "  0, # broadcast over 1st index of first argument\n",
        "  None # don't broadcast over anything of second argument\n",
        "  ), out_axes=0)\n",
        "print(mv(X, y))"
      ],
      "id": "58526a17"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## PyTree Example 1\n",
        "\n",
        "The `in_axes` can match more complicated structures"
      ],
      "id": "0de0c5ce-1033-4cf7-94a4-018182499e41"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1. 2. 3. 4. 5.]"
          ]
        }
      ],
      "source": [
        "dct = {'a': 0., 'b': jnp.arange(5.)}\n",
        "def foo(dct, x):\n",
        " return dct['a'] + dct['b'] + x\n",
        "# axes must match shape of the PyTree\n",
        "x = 1.\n",
        "out = vmap(foo, in_axes=(\n",
        "  {'a': None, 'b': 0}, #broadcast over the 'b'\n",
        "  None # no broadcasting over the \"x\"\n",
        "  ))(dct, x)\n",
        "# example now: {'a': 0, 'b': 0} etc.\n",
        "print(out)"
      ],
      "id": "f42b4483"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## PyTree Example 2"
      ],
      "id": "012cc42c-63dd-408e-8aec-884180c9170b"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 6. 10.]"
          ]
        }
      ],
      "source": [
        "dct = {'a': jnp.array([3.0, 5.0]), 'b': jnp.array([2.0, 4.0])}\n",
        "def foo2(dct, x):\n",
        " return dct['a'] + dct['b'] + x\n",
        "# axes must match shape of the PyTree\n",
        "x = 1.\n",
        "out = vmap(foo2, in_axes=(\n",
        "  {'a': 0, 'b': 0}, #broadcast over the 'a' and 'b'\n",
        "  None # no broadcasting over the \"x\"\n",
        "  ))(dct, x)\n",
        "# example now: {'a': 3.0, 'b': 2.0} etc.\n",
        "print(out)"
      ],
      "id": "c00e9a7a"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## PyTree Example 3"
      ],
      "id": "a2d2f3d1-f5f9-49cc-985b-f461877d5fbc"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[16. 17. 18. 19. 20.]"
          ]
        }
      ],
      "source": [
        "dct = {'a': jnp.array([3.0, 5.0]), 'b': jnp.arange(5.)}\n",
        "def foo3(dct, x):\n",
        " return dct['a'][0] * dct['a'][1] + dct['b'] + x\n",
        "# axes must match shape of the PyTree\n",
        "out = vmap(foo3, in_axes=(\n",
        "  {'a': None, 'b': 0}, #broadcast over the 'b'\n",
        "  None # no broadcasting over the \"x\"\n",
        "  ))(dct, x)\n",
        "# example now: {'a': [3.0, 5.0], 'b': 0} etc.\n",
        "print(out)"
      ],
      "id": "7161ec25"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": "3"
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  }
}