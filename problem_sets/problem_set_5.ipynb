{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ECON622: Assignment 5\n",
        "\n",
        "# Packages\n",
        "\n",
        "Add whatever packages you wish here"
      ],
      "id": "eea5fb15-9117-473c-b7be-8108fd46ecca"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy\n",
        "import pandas as pd\n",
        "import torch\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import grad, hessian\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "id": "1e53dc40"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Q1\n",
        "\n",
        "The trace of the Hessian matrix is useful in a variety of applications\n",
        "in statistics, econometrics, and stochastic processes. It can also be\n",
        "used to regularize a loss function.\n",
        "\n",
        "For of a function $f:\\mathbb{R}^N\\to\\mathbb{R}$, denote the Hessian as\n",
        "$\\nabla^2 f(x) \\in \\mathbb{R}^{N\\times N}$.\n",
        "\n",
        "It can be shows that for some mean zero, unit variance random vectors\n",
        "$v\\in\\mathbb{R}^N$ with $\\mathbb{E}(v) = 0$ and\n",
        "$\\mathbb{E}(v v^{\\top}) = I$ the trace of the Hessian fulfills\n",
        "\n",
        "$$\n",
        "\\mathrm{Tr}(\\nabla^2 f(x)) = \\mathbb{E}\\left[v^{\\top} \\nabla^2 f(x)\\, v\\right]\n",
        "$$\n",
        "\n",
        "Which leads to a random algorithm by sampling $M$ vectors\n",
        "$v_1,\\ldots,v_M$ and using the monte-carlo approximation of the\n",
        "expectation, called the [Hutchinson Trace\n",
        "Estimator](https://www.tandfonline.com/doi/abs/10.1080/03610918908812806)\n",
        "\n",
        "$$\n",
        "\\mathrm{Tr}(\\nabla^2 f(x)) \\approx \\frac{1}{M} \\sum_{m=1}^M v_m^{\\top} \\nabla^2 f(x)\\, v_m\n",
        "$$\n",
        "\n",
        "# Q1.1\n",
        "\n",
        "Now, lets take the function $f(x) = \\frac{1}{2}x^{\\top} P x$, which is a\n",
        "quadratic form and where we know that $\\nabla^2 f(x) = P$.\n",
        "\n",
        "The following code finds the trace of the hessian, which is equivalently\n",
        "just the sum of the diagonal of $P$ in this simple function."
      ],
      "id": "0aea80b8-9d02-4961-852a-935356a7153f"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10223.29\n",
            "10223.289"
          ]
        }
      ],
      "source": [
        "key = jax.random.PRNGKey(0)\n",
        "\n",
        "N = 100  # Dimension of the matrix\n",
        "A = jax.random.normal(key, (N, N))\n",
        "# Create a positive-definite matrix P by forming A^T * A\n",
        "P = jnp.dot(A.T, A)\n",
        "def f(x):\n",
        "    return 0.5 * jnp.dot(x.T, jnp.dot(P, x))\n",
        "x = jax.random.normal(key, (N,))\n",
        "print(jnp.trace(jax.hessian(f)(x)))\n",
        "print(jnp.diag(P).sum())"
      ],
      "id": "4178e483"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, instead of calculating the whole Hessian, use a [Hessian-vector\n",
        "product in\n",
        "JAX](https://jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html#hessian-vector-products-using-both-forward-and-reverse-mode)\n",
        "and the approximation above with $M$ draws of random vectors to\n",
        "calculate an approximation of the trace of the Hessian. Increase the\n",
        "numbers of $M$ to see what the variance of the estimator is, comparing\n",
        "to the above closed-form solution for this quadratic.\n",
        "\n",
        "Hint: you will want to do Forward-over-Reverse mode differentiation for\n",
        "this (i.e. the `vjp` gives a pullback function for first derivative,\n",
        "then differentate that new function. Given that it would then be\n",
        "$\\mathbb{R}^N \\to \\mathbb{R}^N$, it makes sense to use forward mode with\n",
        "a `jvp`)"
      ],
      "id": "82404ae5-5ed9-4780-861f-85a5b7db19e8"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ADD CODE HERE"
      ],
      "id": "69cc0bf4"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Q1.2 BONUS\n",
        "\n",
        "If you wish, you can play around with radically increase the size of the\n",
        "`N` and change the function itself. One suggestion is to move towards a\n",
        "sparse or even matrix-free $f(x)$ calculation so that the $P$ doesn’t\n",
        "itself need to materialize."
      ],
      "id": "c5d3aede-d243-4d2f-b24c-521faf38809e"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ADD CODE HERE"
      ],
      "id": "44f18c68"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Q2\n",
        "\n",
        "This section gives some hints on how to setup a differentiable\n",
        "likelihood function with implicit functions\n",
        "\n",
        "## Q2.1\n",
        "\n",
        "The following code uses scipy to find the equilibrium price and demand\n",
        "for some simple supply and demand functions with embedded parameters"
      ],
      "id": "b0a87638-7c98-4b33-af4a-7b362ad00623"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Equilibrium Price: 17.65\n",
            "Equilibrium Quantity: 91.60"
          ]
        }
      ],
      "source": [
        "from scipy.optimize import root_scalar\n",
        "\n",
        "# Define the demand function with power c\n",
        "def demand(P, c_d):\n",
        "    return 100 - 2 * P**c_d\n",
        "\n",
        "# Define the supply function with power f\n",
        "def supply(P, c_s):\n",
        "    return 5 * 3**(c_s * P)\n",
        "\n",
        "# Define the function to find the root of, including c and f\n",
        "def equilibrium(P, c_d, c_s):\n",
        "    return demand(P, c_d) - supply(P, c_s)\n",
        "\n",
        "# Use root_scalar to find the equilibrium price\n",
        "def find_equilibrium(c_d, c_s):\n",
        "    result = root_scalar(equilibrium, args=(c_d, c_s), bracket=[0, 100], method='brentq')\n",
        "    return result.root, demand(result.root, c_d)\n",
        "\n",
        "# Example usage\n",
        "c_d = 0.5\n",
        "c_s = 0.15\n",
        "equilibrium_price, equilibrium_quantity = find_equilibrium(c_d, c_s)\n",
        "print(f\"Equilibrium Price: {equilibrium_price:.2f}\")\n",
        "print(f\"Equilibrium Quantity: {equilibrium_quantity:.2f}\")"
      ],
      "id": "b1076da2"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, convert this to use JAX and one of the JAX packages for finding\n",
        "the root (e.g., in [JAXopt](https://jaxopt.github.io/stable/)). Make\n",
        "sure you can jit the whole `find_equilibrium` function"
      ],
      "id": "bd55f1e9-c03a-474c-b317-bd0c11667e35"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ADD CODE HERE"
      ],
      "id": "ef3a2d54"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Q2.2\n",
        "\n",
        "Now, assume that you get a noisy signal on the price that fulfills that\n",
        "demand system.\n",
        "\n",
        "$$\n",
        "\\hat{p} \\sim \\mathcal{N}(p, \\sigma^2)\n",
        "$$\n",
        "\n",
        "In that case, the log likelihood for the Gaussian is\n",
        "\n",
        "$$\n",
        "\\log \\mathcal{L}(\\hat{p}\\,|\\,c_d, c_s, p) = -\\frac{1}{2} \\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} (\\hat{p} - p)^2\n",
        "$$\n",
        "\n",
        "Or, if $p$ was implicitly defined by the equilibrium conditions as some\n",
        "$p(c_d, c_s)$ from above,\n",
        "\n",
        "$$\n",
        "\\log \\mathcal{L}(\\hat{p}\\,|\\,c_d, c_s) = -\\frac{1}{2} \\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} (\\hat{p} - p(c_d, c_s))^2\n",
        "$$\n",
        "\n",
        "Then for some $\\sigma = 0.01$ we can calculate this log likelihood the\n",
        "above as"
      ],
      "id": "c3587ab8-3324-4446-abf1-552874ddb059"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "np.float64(3.5353586869582854)"
            ]
          }
        }
      ],
      "source": [
        "def log_likelihood(p_hat, c_d, c_s, sigma):\n",
        "    p, x = find_equilibrium(c_d, c_s)\n",
        "    return -0.5 * np.log(2 * np.pi * sigma**2) - 0.5 * (p_hat - p)**2 / sigma**2\n",
        "\n",
        "c_d = 0.5\n",
        "c_s = 0.15\n",
        "sigma = 0.01\n",
        "p, x = find_equilibrium(c_d, c_s) # get the true value for simulation\n",
        "p_hat = p + np.random.normal(0, sigma) # simulate a noisy signal\n",
        "log_likelihood(p_hat, c_d, c_s, sigma)"
      ],
      "id": "9f161b14"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, take this code for the likelihood and convert it to JAX and jit.\n",
        "Use your function from Q2.1"
      ],
      "id": "2343f5b1-3233-4d60-b445-38d26c0ebd6d"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ADD CODE HERE"
      ],
      "id": "6e232ff1"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Q2.3\n",
        "\n",
        "Use the function from the previous part and calculate the gradient with\n",
        "respect to `c_d` and `c_s` using `grad` and JAX. You will probably want\n",
        "to put the `c_d` and `c_s` into a vector as the first argument, or play\n",
        "around with passing in a dictionary and using PyTrees"
      ],
      "id": "b9ebefe3-9a99-4014-b298-0238181ead43"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ADD CODE HERE"
      ],
      "id": "92da2bc0"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Q2.4 BONUS\n",
        "\n",
        "You could try to run maximum likelihood estimation by using a gradient\n",
        "based optimizer in JAX (e.g., ) Typically you wil want to use\n",
        "[JAXopt](https://jaxopt.github.io/stable/) for this instead of the more\n",
        "ML-centric optimizers.\n",
        "\n",
        "If you attempt this: - Consider starting your optimization at the\n",
        "“pseudo-true” values with the `c_s, c_d, sigma` you used to simulate the\n",
        "data and even start with `p_hat = p`. - You may find that it is a little\n",
        "too noisy with only the one observation. If so, you could adapt your\n",
        "likelihood to take a vector of $\\hat{p}$ instead. The likelihood of IID\n",
        "gaussians is a simple variation on the above."
      ],
      "id": "8fc5a3e4-51ca-484a-b249-1186f9516828"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ADD CODE HERE"
      ],
      "id": "f186d8e3"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "<!--\n",
        "## Q3\n",
        "For the LLS examples with Pytorch we added in [linear_regression_pytorch_logging.py](logging https://github.com/ubcecon/ECON622/blob/master/lectures/lectures/examples/linear_regression_pytorch_logging.py)  and a CLI interface - which came for free with pytorch lightning.\n",
        "\n",
        "In this question you will add in some of those features to the [linear_regression_jax_equinox.py](https://github.com/ubcecon/ECON622/blob/master/lectures/lectures/examples/linear_regression_jax_equinox.py) example.\n",
        "\n",
        "## Q3.1\n",
        "Take the `linear_regression_jax_equinox.py` copied below for your convenience and:\n",
        "\n",
        "1. Setup the W&B properly\n",
        "2. Add in logging of the `train_loss` at every step of the optimizer\n",
        "3. Remove the other epoch printing, or try to log an epoch specific `||theta - theta_hat||` if you wish\n",
        "4. Log the end `||theta - theta_hat||` at the end of the training\n",
        "\n",
        "::: {#22661be9 .cell execution_count=11}\n",
        "``` {.python .cell-code}\n",
        "# MODIFY CODE HERE\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import grad, jit, value_and_grad, vmap\n",
        "from jax import random\n",
        "import optax\n",
        "import equinox as eqx\n",
        "\n",
        "N = 500  # samples\n",
        "M = 2\n",
        "sigma = 0.001\n",
        "key = random.PRNGKey(42)\n",
        "# Pattern: split before using key, replace name \"key\"\n",
        "key, *subkey = random.split(key, num=4)\n",
        "theta = random.normal(subkey[0], (M,))\n",
        "X = random.normal(subkey[1], (N, M))\n",
        "Y = X @ theta + sigma * random.normal(subkey[2], (N,))  # Adding noise\n",
        "\n",
        "# Creates an iterable \n",
        "def data_loader(key, X, Y, batch_size):\n",
        "    num_samples = X.shape[0]\n",
        "    assert num_samples == Y.shape[0]\n",
        "    indices = jnp.arange(num_samples)\n",
        "    indices = random.permutation(key, indices)\n",
        "    # Loop over batches and yield\n",
        "    for i in range(0, num_samples, batch_size):\n",
        "        batch_indices = indices[i:i + batch_size]\n",
        "        yield X[batch_indices], Y[batch_indices]\n",
        "\n",
        "\n",
        "# Need to randomize our own theta_0 parameters\n",
        "key, subkey = random.split(key)\n",
        "theta_0 = random.normal(subkey, (M,))\n",
        "print(f\"theta_0 = {theta_0}, theta = {theta}\")\n",
        "\n",
        "# Probably a way to use `vmap` or `eqx.filter_vmap` here as well\n",
        "\n",
        "def residual(model, x, y):\n",
        "    y_hat = model(x)\n",
        "    return (y_hat - y) ** 2\n",
        "\n",
        "def residuals(model, X, Y):\n",
        "    batched_residuals = vmap(residual, in_axes=(None, 0, 0))\n",
        "    return jnp.mean(batched_residuals(model, X, Y))\n",
        "\n",
        "# Alternatively could do something like\n",
        "def residuals_2(model, X, Y):\n",
        "    Y_hat = vmap(model)(X).squeeze()\n",
        "    return jax.numpy.mean((Y - Y_hat) ** 2)\n",
        "\n",
        "# Hypothesis Class: will start with a linear function, which is randomly initialized\n",
        "# model is a variable of all parametesr, and supports model(X) calls\n",
        "key, subkey = random.split(key)\n",
        "model = eqx.nn.Linear(M, 1, use_bias = False, key = subkey)\n",
        "\n",
        "# reinitialize\n",
        "lr = 0.001\n",
        "optimizer = optax.sgd(lr)\n",
        "# Needs to remove the non-differentiable parts of the \"model\" object\n",
        "opt_state = optimizer.init(eqx.filter(model,eqx.is_inexact_array))\n",
        "\n",
        "@eqx.filter_jit\n",
        "def make_step(model, opt_state, X, Y):     \n",
        "  loss_value, grads = eqx.filter_value_and_grad(residuals)(model, X, Y)\n",
        "  updates, opt_state = optimizer.update(grads, opt_state, model)\n",
        "  model = eqx.apply_updates(model, updates)\n",
        "  return model, opt_state, loss_value\n",
        "\n",
        "num_epochs = 300\n",
        "batch_size = 64\n",
        "key, subkey = random.split(key) # will keep same key for shuffling each epoch\n",
        "for epoch in range(num_epochs):\n",
        "    key, subkey = random.split(key) # changing key for shuffling each epoch\n",
        "    train_loader = data_loader(subkey, X, Y, batch_size)\n",
        "    for X_batch, Y_batch in train_loader:\n",
        "        model, opt_state, train_loss = make_step(model, opt_state, X_batch, Y_batch)\n",
        "        # TODO ADD IN LOGGING OF THE train_loss\n",
        "    \n",
        "    # TODO CAN REMOVE THIS ENTIRELY AFTER LOGGING IS WORKING\n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"Epoch {epoch},||theta - theta_hat|| = {jnp.linalg.norm(theta - model.weight)}\")\n",
        "\n",
        "# TODO: LOG THE FINAL VALUE HERE\n",
        "print(f\"||theta - theta_hat|| = {jnp.linalg.norm(theta - model.weight)}\")\n",
        "```\n",
        "\n",
        "::: {.cell-output .cell-output-stdout}\n",
        "```\n",
        "theta_0 = [ 1.7535115  -0.07298409], theta = [0.1378821  0.79073715]\n",
        "Epoch 0,||theta - theta_hat|| = 0.4085429608821869\n",
        "Epoch 100,||theta - theta_hat|| = 0.0743650421500206\n",
        "Epoch 200,||theta - theta_hat|| = 0.01363404467701912\n",
        "||theta - theta_hat|| = 0.002546713687479496\n",
        "```\n",
        ":::\n",
        ":::\n",
        "\n",
        "\n",
        "## Q3.2\n",
        "Now, take the above code and copy it into a file named `linear_regression_jax_cli.py`.\n",
        "\n",
        "Feel free to use use the builtin [Argparse](https://docs.python.org/3/library/argparse.html) or any other [CLI framework](https://github.com/shadawck/awesome-cli-frameworks#python)\n",
        "\n",
        "\n",
        "Regardless of how you do it, here is a suggestion of some steps\n",
        "\n",
        "1. Create a function called ` main_fn(lr: float = 0.001, N: int = 100, ...)` with whatever parameters you want to change as arguments.  The type annotations are optional but useful for some CLI packages\n",
        "2. Move all of your code inside of that function, and get rid of the initialization of those values\n",
        "3. You can test it out by adding the following code and then running the file\n",
        "\n",
        "```\n",
        "if __name__ == '__main__':\n",
        "  main_fn()\n",
        "```\n",
        "\n",
        "\n",
        "To make this CLI-ready, here are two suggested packages:\n",
        "\n",
        "### jsonargparse\n",
        "A package with many features, most of which you wouldn't use directly, is [jsonargparse](https://jsonargparse.readthedocs.io/).  Besides the more advanced features like [configuration files](https://jsonargparse.readthedocs.io/en/v4.26.2/#writing-configuration-files) and the instantiation of classes/etc. as arguments, the main difference will be that it checks the types of arguments and converts them for you using python typehints.  In that case, you can adapt the following code for your linear_regression_jax_cli.py\n",
        "\n",
        "```\n",
        "import jsonargparse\n",
        "def main_fn(lr: float = 0.001, N: int = 100):\n",
        "    print(f\"lr = {lr}, N = {N}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "     jsonargparse.CLI(main_fn)\n",
        "```     \n",
        "\n",
        "\n",
        "If you get this working you may want to consider trying out the configuration file feature, which is a nice way to save your hyperparameters for later use and reproducibility\n",
        "\n",
        "### Python Fire\n",
        "[python-fire](https://github.com/google/python-fire) which is already in your `requirements.txt`  See the [documentation](https://google.github.io/python-fire/guide/) for more.  This is a very lightweight package compared to some of the alternatives.\n",
        "\n",
        "You can adapt the following code\n",
        "\n",
        "```\n",
        "import fire\n",
        "def main_fn(lr: float = 0.001, N: int = 100):\n",
        "    print(f\"lr = {lr}, N = {N}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  fire.Fire(main_fn)\n",
        "```\n",
        "\n",
        "In this case, however, the `float` and `int` do not seem to be enforced by Python Fire, so you may need to cast them directly in your code if things aren't working correctly.\n",
        "\n",
        "### Using your CLI\n",
        "In either case, at that point you should be able to call this with `python linear_regression_jax_cli.py` and have it use all of the default values, `python linear_regression_jax_cli.py --N=200` to change them, etc.\n",
        "\n",
        "Either submit the file as part of the assignment or just paste the code into the notebook\n",
        "\n",
        "## Q3.3 BONUS\n",
        "\n",
        "Given the CLI you can now run a hyperparameter search. For this bonus problem, do a hyperparameter search over the `--lr` argument by following the [W&B documentation](https://docs.wandb.ai/guides/sweeps).\n",
        "\n",
        "To get you started, your sweep yaml might look something like this\n",
        "\n",
        "\n",
        "```gwzzla\n",
        "program: linear_regression_jax_cli.py\n",
        "name: JAX Example\n",
        "project: linear_regression_pytorch\n",
        "description: JAX Sweep\n",
        "method: random\n",
        "parameters:\n",
        "  lr:\n",
        "    min: 0.0001\n",
        "    max: 0.01\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "Here I changed the `method` from bayes to [`random`](https://docs.wandb.ai/guides/sweeps/define-sweep-configuration#method) because otherwise we would need to provide a `metric` to optimize over.  Feel free to adapt any of these settings.\n",
        "\n",
        "If you successfully run a sweep then paste in your own yaml file here, and a screenshot of the W&B dashboard showing something about the sweep results.\n",
        "\n",
        "-->"
      ],
      "id": "9dbd5b68-fb96-4d2e-944b-bceddd59e60f"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": "3"
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  }
}