<!DOCTYPE html>
<html lang="en"><head>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-html/tabby.min.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/light-border.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-html.min.css" rel="stylesheet" data-mode="light">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.3.450">

  <meta name="author" content="Jesse Perla">
  <title>ECON 622 Lectures - ECON622: Computational Economics with Data Science Applications</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="../site_libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="../site_libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
    /* CSS for syntax highlighting */
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      { color: #003b4f; background-color: #f1f3f5; }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span { color: #003b4f; } /* Normal */
    code span.al { color: #ad0000; } /* Alert */
    code span.an { color: #5e5e5e; } /* Annotation */
    code span.at { color: #657422; } /* Attribute */
    code span.bn { color: #ad0000; } /* BaseN */
    code span.bu { } /* BuiltIn */
    code span.cf { color: #003b4f; } /* ControlFlow */
    code span.ch { color: #20794d; } /* Char */
    code span.cn { color: #8f5902; } /* Constant */
    code span.co { color: #5e5e5e; } /* Comment */
    code span.cv { color: #5e5e5e; font-style: italic; } /* CommentVar */
    code span.do { color: #5e5e5e; font-style: italic; } /* Documentation */
    code span.dt { color: #ad0000; } /* DataType */
    code span.dv { color: #ad0000; } /* DecVal */
    code span.er { color: #ad0000; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #ad0000; } /* Float */
    code span.fu { color: #4758ab; } /* Function */
    code span.im { color: #00769e; } /* Import */
    code span.in { color: #5e5e5e; } /* Information */
    code span.kw { color: #003b4f; } /* Keyword */
    code span.op { color: #5e5e5e; } /* Operator */
    code span.ot { color: #003b4f; } /* Other */
    code span.pp { color: #ad0000; } /* Preprocessor */
    code span.sc { color: #5e5e5e; } /* SpecialChar */
    code span.ss { color: #20794d; } /* SpecialString */
    code span.st { color: #20794d; } /* String */
    code span.va { color: #111111; } /* Variable */
    code span.vs { color: #20794d; } /* VerbatimString */
    code span.wa { color: #5e5e5e; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="../site_libs/revealjs/dist/theme/quarto.css">
  <link href="../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-chalkboard/font-awesome/css/all.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-chalkboard/style.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">

  .callout {
    margin-top: 1em;
    margin-bottom: 1em;  
    border-radius: .25rem;
  }

  .callout.callout-style-simple { 
    padding: 0em 0.5em;
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
    display: flex;
  }

  .callout.callout-style-default {
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
  }

  .callout .callout-body-container {
    flex-grow: 1;
  }

  .callout.callout-style-simple .callout-body {
    font-size: 1rem;
    font-weight: 400;
  }

  .callout.callout-style-default .callout-body {
    font-size: 0.9rem;
    font-weight: 400;
  }

  .callout.callout-titled.callout-style-simple .callout-body {
    margin-top: 0.2em;
  }

  .callout:not(.callout-titled) .callout-body {
      display: flex;
  }

  .callout:not(.no-icon).callout-titled.callout-style-simple .callout-content {
    padding-left: 1.6em;
  }

  .callout.callout-titled .callout-header {
    padding-top: 0.2em;
    margin-bottom: -0.2em;
  }

  .callout.callout-titled .callout-title  p {
    margin-top: 0.5em;
    margin-bottom: 0.5em;
  }
    
  .callout.callout-titled.callout-style-simple .callout-content  p {
    margin-top: 0;
  }

  .callout.callout-titled.callout-style-default .callout-content  p {
    margin-top: 0.7em;
  }

  .callout.callout-style-simple div.callout-title {
    border-bottom: none;
    font-size: .9rem;
    font-weight: 600;
    opacity: 75%;
  }

  .callout.callout-style-default  div.callout-title {
    border-bottom: none;
    font-weight: 600;
    opacity: 85%;
    font-size: 0.9rem;
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-default div.callout-content {
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-simple .callout-icon::before {
    height: 1rem;
    width: 1rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 1rem 1rem;
  }

  .callout.callout-style-default .callout-icon::before {
    height: 0.9rem;
    width: 0.9rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 0.9rem 0.9rem;
  }

  .callout-title {
    display: flex
  }
    
  .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  .callout.no-icon::before {
    display: none !important;
  }

  .callout.callout-titled .callout-body > .callout-content > :last-child {
    margin-bottom: 0.5rem;
  }

  .callout.callout-titled .callout-icon::before {
    margin-top: .5rem;
    padding-right: .5rem;
  }

  .callout:not(.callout-titled) .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  /* Callout Types */

  div.callout-note {
    border-left-color: #4582ec !important;
  }

  div.callout-note .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEU0lEQVRYCcVXTWhcVRQ+586kSUMMxkyaElstCto2SIhitS5Ek8xUKV2poatCcVHtUlFQk8mbaaziwpWgglJwVaquitBOfhQXFlqlzSJpFSpIYyXNjBNiTCck7x2/8/LeNDOZxDuEkgOXe++553zfefee+/OYLOXFk3+1LLrRdiO81yNqZ6K9cG0P3MeFaMIQjXssE8Z1JzLO9ls20MBZX7oG8w9GxB0goaPrW5aNMp1yOZIa7Wv6o2ykpLtmAPs/vrG14Z+6d4jpbSKuhdcSyq9wGMPXjonwmESXrriLzFGOdDBLB8Y6MNYBu0dRokSygMA/mrun8MGFN3behm6VVAwg4WR3i6FvYK1T7MHo9BK7ydH+1uurECoouk5MPRyVSBrBHMYwVobG2aOXM07sWrn5qgB60rc6mcwIDJtQrnrEr44kmy+UO9r0u9O5/YbkS9juQckLed3DyW2XV/qWBBB3ptvI8EUY3I9p/67OW+g967TNr3Sotn3IuVlfMLVnsBwH4fsnebJvyGm5GeIUA3jljERmrv49SizPYuq+z7c2H/jlGC+Ghhupn/hcapqmcudB9jwJ/3jvnvu6vu5lVzF1fXyZuZZ7U8nRmVzytvT+H3kilYvH09mLWrQdwFSsFEsxFVs5fK7A0g8gMZjbif4ACpKbjv7gNGaD8bUrlk8x+KRflttr22JEMRUbTUwwDQScyzPgedQHZT0xnx7ujw2jfVfExwYHwOsDTjLdJ2ebmeQIlJ7neo41s/DrsL3kl+W2lWvAga0tR3zueGr6GL78M3ifH0rGXrBC2aAR8uYcIA5gwV8zIE8onoh8u0Fca/ciF7j1uOzEnqcIm59sEXoGc0+z6+H45V1CvAvHcD7THztu669cnp+L0okAeIc6zjbM/24LgGM1gZk7jnRu1aQWoU9sfUOuhrmtaPIO3YY1KLLWZaEO5TKUbMY5zx8W9UJ6elpLwKXbsaZ4EFl7B4bMtDv0iRipKoDQT2sNQI9b1utXFdYisi+wzZ/ri/1m7QfDgEuvgUUEIJPq3DhX/5DWNqIXDOweC2wvIR90Oq3lDpdMIgD2r0dXvGdsEW5H6x6HLRJYU7C69VefO1x8Gde1ZFSJLfWS1jbCnhtOPxmpfv2LXOA2Xk2tvnwKKPFuZ/oRmwBwqRQDcKNeVQkYcOjtWVBuM/JuYw5b6isojIkYxyYAFn5K7ZBF10fea52y8QltAg6jnMqNHFBmGkQ1j+U43HMi2xMar1Nv0zGsf1s8nUsmUtPOOrbFIR8bHFDMB5zL13Gmr/kGlCkUzedTzzmzsaJXhYawnA3UmARpiYj5ooJZiUoxFRtK3X6pgNPv+IZVPcnwbOl6f+aBaO1CNvPW9n9LmCp01nuSaTRF2YxHqZ8DYQT6WsXT+RD6eUztwYLZ8rM+rcPxamv1VQzFUkzFXvkiVrySGQgJNvXHJAxiU3/NwiC03rSf05VBaPtu/Z7/B8Yn/w7eguloAAAAAElFTkSuQmCC');
  }

  div.callout-note.callout-style-default .callout-title {
    background-color: #dae6fb
  }

  div.callout-important {
    border-left-color: #d9534f !important;
  }

  div.callout-important .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEKklEQVRYCcVXTWhcVRS+575MJym48A+hSRFr00ySRQhURRfd2HYjk2SSTokuBCkU2o0LoSKKraKIBTcuFCoidGFD08nkBzdREbpQ1EDNIv8qSGMFUboImMSZd4/f9zJv8ibJMC8xJQfO3HPPPef7zrvvvnvviIkpC9nsw0UttFunbUhpFzFtarSd6WJkStVMw5xyVqYTvkwfzuf/5FgtkVoB0729j1rjXwThS7Vio+Mo6DNnvLfahoZ+i/o32lULuJ3NNiz7q6+pyAUkJaFF6JwaM2lUJlV0MlnQn5aTRbEu0SEqHUa0A4AdiGuB1kFXRfVyg5d87+Dg4DL6m2TLAub60ilj7A1Ec4odSAc8X95sHh7+ZRPCFo6Fnp7HfU/fBng/hi10CjCnWnJjsxvDNxWw0NfV6Rv5GgP3I3jGWXumdTD/3cbEOP2ZbOZp69yniG3FQ9z1jD7bnBu9Fc2tKGC2q+uAJOQHBDRiZX1x36o7fWBs7J9ownbtO+n0/qWkvW7UPIfc37WgT6ZGR++EOJyeQDSb9UB+DZ1G6DdLDzyS+b/kBCYGsYgJbSQHuThGKRcw5xdeQf8YdNHsc6ePXrlSYMBuSIAFTGAtQo+VuALo4BX83N190NWZWbynBjhOHsmNfFWLeL6v+ynsA58zDvvAC8j5PkbOcXCMg2PZFk3q8MjI7WAG/Dp9AwP7jdGBOOQkAvlFUB+irtm16I1Zw9YBcpGTGXYmk3kQIC/Cds55l+iMI3jqhjAuaoe+am2Jw5GT3Nbz3CkE12NavmzN5+erJW7046n/CH1RO/RVa8lBLozXk9uqykkGAyRXLWlLv5jyp4RFsG5vGVzpDLnIjTWgnRy2Rr+tDKvRc7Y8AyZq10jj8DqXdnIRNtFZb+t/ZRtXcDiVnzpqx8mPcDWxgARUqx0W1QB9MeUZiNrV4qP+Ehc+BpNgATsTX8ozYKL2NtFYAHc84fG7ndxUPr+AR/iQSns7uSUufAymwDOb2+NjK27lEFocm/EE2WpyIy/Hi66MWuMKJn8RvxIcj87IM5Vh9663ziW36kR0HNenXuxmfaD8JC7tfKbrhFr7LiZCrMjrzTeGx+PmkosrkNzW94ObzwocJ7A1HokLolY+AvkTiD/q1H0cN48c5EL8Crkttsa/AXQVDmutfyku0E7jShx49XqV3MFK8IryDhYVbj7Sj2P2eBxwcXoe8T8idsKKPRcnZw1b+slFTubwUwhktrfnAt7J++jwQtLZcm3sr9LQrjRzz6cfMv9aLvgmnAGvpoaGLxM4mAEaLV7iAzQ3oU0IvD5x9ix3yF2RAAuYAOO2f7PEFWCXZ4C9Pb2UsgDeVnFSpbFK7/IWu7TPTvBqzbGdCHOJQSxiEjt6IyZmxQyEJHv6xyQsYk//moVFsN2zP6fRImjfq7/n/wFDguUQFNEwugAAAABJRU5ErkJggg==');
  }

  div.callout-important.callout-style-default .callout-title {
    background-color: #f7dddc
  }

  div.callout-warning {
    border-left-color: #f0ad4e !important;
  }

  div.callout-warning .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAETklEQVRYCeVWW2gcVRg+58yaTUnizqbipZeX4uWhBEniBaoUX1Ioze52t7sRq6APio9V9MEaoWlVsFasRq0gltaAPuxms8lu0gcviE/FFOstVbSIxgcv6SU7EZqmdc7v9+9mJtNks51NTUH84ed889/PP+cmxP+d5FIbMJmNbpREu4WUkiTtCicKny0l1pIKmBzovF2S+hIJHX8iEu3hZJ5lNZGqyRrGSIQpq15AzF28jgpeY6yk6GVdrfFqdrD6Iw+QlB8g0YS2g7dyQmXM/IDhBhT0UCiRf59lfqmmDvzRt6kByV/m4JjtzuaujMUM2c5Z2d6JdKrRb3K2q6mA+oYVz8JnDdKPmmNthzkAk/lN63sYPgevrguc72aZX/L9C6x09GYyxBgCX4NlvyGUHOKELlm5rXeR1kchuChJt4SSwyddZRXgvwMGvYo4QSlk3/zkHD8UHxwVJA6zjZZqP8v8kK8OWLnIZtLyCAJagYC4rTGW/9Pqj92N/c+LUaAj27movwbi19tk/whRCIE7Q9vyI6yvRpftAKVTdUjOW40X3h5OXsKCdmFcx0xlLJoSuQngnrJe7Kcjm4OMq9FlC7CMmScQANuNvjfP3PjGXDBaUQmbp296S5L4DrpbrHN1T87ZVEZVCzg1FF0Ft+dKrlLukI+/c9ENo+TvlTDbYFvuKPtQ9+l052rXrgKoWkDAFnvh0wTOmYn8R5f4k/jN/fZiCM1tQx9jQQ4ANhqG4hiL0qIFTGViG9DKB7GYzgubnpofgYRwO+DFjh0Zin2m4b/97EDkXkc+f6xYAPX0KK2I/7fUQuwzuwo/L3AkcjugPNixC8cHf0FyPjWlItmLxWw4Ou9YsQCr5fijMGoD/zpdRy95HRysyXA74MWOnscpO4j2y3HAVisw85hX5+AFBRSHt4ShfLFkIMXTqyKFc46xdzQM6XbAi702a7sy04J0+feReMFKp5q9esYLCqAZYw/k14E/xcLLsFElaornTuJB0svMuJINy8xkIYuL+xPAlWRceH6+HX7THJ0djLUom46zREu7tTkxwmf/FdOZ/sh6Q8qvEAiHpm4PJ4a/doJe0gH1t+aHRgCzOvBvJedEK5OFE5jpm4AGP2a8Dxe3gGJ/pAutug9Gp6he92CsSsWBaEcxGx0FHytmIpuqGkOpldqNYQK8cSoXvd+xLxXADw0kf6UkJNFtdo5MOgaLjiQOQHcn+A6h5NuL2s0qsC2LOM75PcF3yr5STuBSAcGG+meA14K/CI21HcS4LBT6tv0QAh8Dr5l93AhZzG5ZJ4VxAqdZUEl9z7WJ4aN+svMvwHHL21UKTd1mqvChH7/Za5xzXBBKrUcB0TQ+Ulgkfbi/H/YT5EptrGzsEK7tR1B7ln9BBwckYfMiuSqklSznIuoIIOM42MQO+QnduCoFCI0bpkzjCjddHPN/F+2Yu+sd9bKNpVwHhbS3LluK/0zgfwD0xYI5dXuzlQAAAABJRU5ErkJggg==');
  }

  div.callout-warning.callout-style-default .callout-title {
    background-color: #fcefdc
  }

  div.callout-tip {
    border-left-color: #02b875 !important;
  }

  div.callout-tip .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAADr0lEQVRYCe1XTWgTQRj9ZjZV8a9SPIkKgj8I1bMHsUWrqYLVg4Ue6v9BwZOxSYsIerFao7UiUryIqJcqgtpimhbBXoSCVxUFe9CTiogUrUp2Pt+3aUI2u5vdNh4dmMzOzHvvezuz8xNFM0mjnbXaNu1MvFWRXkXEyE6aYOYJpdW4IXuA4r0fo8qqSMDBU0v1HJUgVieAXxzCsdE/YJTdFcVIZQNMyhruOMJKXYFoLfIfIvVIMWdsrd+Rpd86ZmyzzjJmLStqRn0v8lzkb4rVIXvnpScOJuAn2ACC65FkPzEdEy4TPWRLJ2h7z4cArXzzaOdKlbOvKKX25Wl00jSnrwVxAg3o4dRxhO13RBSdNvH0xSARv3adTXbBdTf64IWO2vH0LT+cv4GR1DJt+DUItaQogeBX/chhbTBxEiZ6gftlDNXTrvT7co4ub5A6gp9HIcHvzTa46OS5fBeP87Qm0fQkr4FsYgVQ7Qg+ZayaDg9jhg1GkWj8RG6lkeSacrrHgDaxdoBiZPg+NXV/KifMuB6//JmYH4CntVEHy/keA6x4h4CU5oFy8GzrBS18cLJMXcljAKB6INjWsRcuZBWVaS3GDrqB7rdapVIeA+isQ57Eev9eCqzqOa81CY05VLd6SamW2wA2H3SiTbnbSxmzfp7WtKZkqy4mdyAlGx7ennghYf8voqp9cLSgKdqNfa6RdRsAAkPwRuJZNbpByn+RrJi1RXTwdi8RQF6ymDwGMAtZ6TVE+4uoKh+MYkcLsT0Hk8eAienbiGdjJHZTpmNjlbFJNKDVAp2fJlYju6IreQxQ08UJDNYdoLSl6AadO+fFuCQqVMB1NJwPm69T04Wv5WhfcWyfXQB+wXRs1pt+nCknRa0LVzSA/2B+a9+zQJadb7IyyV24YAxKp2Jqs3emZTuNnKxsah+uabKbMk7CbTgJx/zIgQYErIeTKRQ9yD9wxVof5YolPHqaWo7TD6tJlh7jQnK5z2n3+fGdggIOx2kaa2YI9QWarc5Ce1ipNWMKeSG4DysFF52KBmTNMmn5HqCFkwy34rDg05gDwgH3bBi+sgFhN/e8QvRn8kbamCOhgrZ9GJhFDgfcMHzFb6BAtjKpFhzTjwv1KCVuxHvCbsSiEz4CANnj84cwHdFXAbAOJ4LTSAawGWFn5tDhLMYz6nWeU2wJfIhmIJBefcd/A5FWQWGgrWzyORZ3Q6HuV+Jf0Bj+BTX69fm1zWgK7By1YTXchFDORywnfQ7GpzOo6S+qECrsx2ifVQAAAABJRU5ErkJggg==');
  }

  div.callout-tip.callout-style-default .callout-title {
    background-color: #ccf1e3
  }

  div.callout-caution {
    border-left-color: #fd7e14 !important;
  }

  div.callout-caution .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAACV0lEQVRYCdVWzWoUQRCuqp2ICBLJXgITZL1EfQDBW/bkzUMUD7klD+ATSHBEfAIfQO+iXsWDxJsHL96EHAwhgzlkg8nBg25XWb0zIb0zs9muYYWkoKeru+vn664fBqElyZNuyh167NXJ8Ut8McjbmEraKHkd7uAnAFku+VWdb3reSmRV8PKSLfZ0Gjn3a6Xlcq9YGb6tADjn+lUfTXtVmaZ1KwBIvFI11rRXlWlatwIAAv2asaa9mlB9wwygiDX26qaw1yYPzFXg2N1GgG0FMF8Oj+VIx7E/03lHx8UhvYyNZLN7BwSPgekXXLribw7w5/c8EF+DBK5idvDVYtEEwMeYefjjLAdEyQ3M9nfOkgnPTEkYU+sxMq0BxNR6jExrAI31H1rzvLEfRIdgcv1XEdj6QTQAS2wtstEALLG1yEZ3QhH6oDX7ExBSFEkFINXH98NTrme5IOaaA7kIfiu2L8A3qhH9zRbukdCqdsA98TdElyeMe5BI8Rs2xHRIsoTSSVFfCFCWGPn9XHb4cdobRIWABNf0add9jakDjQJpJ1bTXOJXnnRXHRf+dNL1ZV1MBRCXhMbaHqGI1JkKIL7+i8uffuP6wVQAzO7+qVEbF6NbS0LJureYcWXUUhH66nLR5rYmva+2tjRFtojkM2aD76HEGAD3tPtKM309FJg5j/K682ywcWJ3PASCcycH/22u+Bh7Aa0ehM2Fu4z0SAE81HF9RkB21c5bEn4Dzw+/qNOyXr3DCTQDMBOdhi4nAgiFDGCinIa2owCEChUwD8qzd03PG+qdW/4fDzjUMcE1ZpIAAAAASUVORK5CYII=');
  }

  div.callout-caution.callout-style-default .callout-title {
    background-color: #ffe5d0
  }

  </style>
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
  <style>
  .reveal .custom3070 > div.column:first-child {
    width: 30%;
  }
  .reveal .custom3070 div.column:not(:first-child) {
    width: 70%;
  }
  .reveal .custom4060 > div.column:first-child {
    width: 40%;
  }
  .reveal .custom4060 div.column:not(:first-child) {
    width: 60%;
  }
  </style>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">ECON622: Computational Economics with Data Science Applications</h1>
  <p class="subtitle">Optimization for Machine Learning</p>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Jesse Perla 
</div>
<div class="quarto-title-author-email">
<a href="mailto:jesse.perla@ubc.ca">jesse.perla@ubc.ca</a>
</div>
        <p class="quarto-title-affiliation">
            University of British Columbia
          </p>
    </div>
</div>

</section><section id="TOC">
<nav role="doc-toc"> 
<h2 id="toc-title">Table of contents</h2>
<ul>
<li><a href="#/overview" id="/toc-overview">Overview</a></li>
<li><a href="#/optimization-crash-course" id="/toc-optimization-crash-course">Optimization Crash Course</a></li>
<li><a href="#/stochastic-optimization" id="/toc-stochastic-optimization">Stochastic Optimization</a></li>
<li><a href="#/training-loops" id="/toc-training-loops">Training Loops</a></li>
<li><a href="#/detailed-pytorch-linear-regression-example" id="/toc-detailed-pytorch-linear-regression-example">Detailed Pytorch Linear Regression Example</a></li>
<li><a href="#/more-pytorch-linear-regression-examples" id="/toc-more-pytorch-linear-regression-examples">More Pytorch Linear Regression Examples</a></li>
<li><a href="#/detailed-jax-linear-regression-example" id="/toc-detailed-jax-linear-regression-example">Detailed JAX Linear Regression Example</a></li>
<li><a href="#/jax-linear-regression-with-equinox" id="/toc-jax-linear-regression-with-equinox">JAX Linear Regression with Equinox</a></li>
</ul>
</nav>
</section>
<section>
<section id="overview" class="title-slide slide level1 center">
<h1>Overview</h1>

</section>
<section id="summary" class="slide level2">
<h2>Summary</h2>
<ul>
<li>This lecture continues from the previous lecture on gradients to further explore optimization methods in machine learning, and discusses training pipelines and tooling</li>
<li>Primary reference materials are:
<ul>
<li><a href="https://probml.github.io/pml-book/book1.html">ProbML Book 1: Introduction</a></li>
<li><a href="https://probml.github.io/pml-book/book2.html">ProbML Book 2: Advanced Topics</a> including Section 6.3</li>
<li><a href="https://www.cs.ubc.ca/~schmidtm/Courses/LecturesOnML/">Mark Schmidt’s ML Lecture Notes</a></li>
</ul></li>
<li>We will also give a sense of a standard machine learning pipeline of training, validation, and test data and discuss generalization, logging, etc.</li>
</ul>
</section>
<section id="why-the-emphasis-on-optimization-and-gradients" class="slide level2">
<h2>Why the Emphasis on Optimization and Gradients?</h2>
<ul>
<li>A huge number of algorithms for economists can be written as optimization problems (e.g., MLE, interpolation) or as something similar in spirit (e.g.&nbsp;Bayesian Sampling, Reinforcement Learning)</li>
<li>Previous lectures on AD showed ways to find VJPs for extremely complicated functions. <strong>Differentiate everything, no excuses!</strong></li>
<li>In practice, <strong>all</strong> problems with high-dimensions parameters or latents require gradients for algorithms to be feasible</li>
<li>We will soon take a further step: <strong>are (unbiased) estimates of the gradient good enough for many algorithms?</strong></li>
</ul>
</section></section>
<section>
<section id="optimization-crash-course" class="title-slide slide level1 center">
<h1>Optimization Crash Course</h1>

</section>
<section id="optimization-methods" class="slide level2">
<h2>Optimization Methods</h2>
<ul>
<li>Learning continuous optimization methods is an enormous project</li>
<li>See referenced materials and lecture notes</li>
<li>Here we will give an overview of some key concepts</li>
<li>Be warned! The details matter, so more study is required if you want to use these methods in practice</li>
</ul>
</section>
<section id="crash-course-in-unconstrained-optimization" class="slide level2">
<h2>Crash Course in Unconstrained Optimization</h2>
<p><span class="math display">\[
\min_{\theta} \mathcal{L}(\theta)
\]</span></p>
<p>Will briefly introduce</p>
<ul>
<li>First-order methods</li>
<li>Second-order methods</li>
<li>Preconditioning</li>
<li>Momentum</li>
<li>Regularization</li>
</ul>
</section>
<section id="first-order-methods" class="slide level2">
<h2>First-Order Methods</h2>
<ul>
<li>See <a href="https://probml.github.io/pml-book/book1.html">ProbML Book 1</a> Section 8.2</li>
<li>Armed with reverse-mode AD for <span class="math inline">\(\mathcal{L} : \mathbb{R}^N \to \mathbb{R}\)</span> we can calculate <span class="math inline">\(\nabla \mathcal{L}(\theta)\)</span> with the same computational order as <span class="math inline">\(\mathcal{L}(\theta)\)</span></li>
<li>Furthermore, given JVPs we know we can calculate these objective functions for extremely complicated functions (e.g., nested fixed points, and implicit functions)</li>
<li>Iterative: take <span class="math inline">\(\theta_0\)</span> and provide <span class="math inline">\(\theta_t \to \theta_{t+1}\)</span>
<ul>
<li>May converge to a stationary point (hopefully close to a global argmin)</li>
<li>If it doesn’t converge, the solution may still be an argmin</li>
<li>See references for details on convergence for convex and non-convex problems</li>
</ul></li>
</ul>
</section>
<section id="gradient-descent" class="slide level2">
<h2>Gradient Descent</h2>
<ul>
<li>See <a href="https://www.cs.ubc.ca/~schmidtm/Courses/340-F22/L13.pdf">Mark Schmidt’s Notes</a></li>
<li>Gradient descent takes <span class="math inline">\(\theta_0\)</span>, and stepsize <span class="math inline">\(\eta_t\)</span> and iterates until <span class="math inline">\(\nabla \mathcal{L}(\theta_t)\)</span> is small, or <span class="math inline">\(\theta_t\)</span> stationary</li>
</ul>
<p><span class="math display">\[
\theta_{t+1} = \theta_t - \eta_t \nabla \mathcal{L}(\theta_t)
\]</span></p>
<ul>
<li>It is the simplest “first-order” method (i.e., ones using just the gradient of <span class="math inline">\(\mathcal{L}\)</span>)</li>
<li>Will call <span class="math inline">\(\eta_t\)</span> a “learning rate schedule”</li>
<li>Think of line-search methods as choosing the stepsize <span class="math inline">\(\eta_t\)</span> optimally. Useful as well for economists, even if used infrequently in M</li>
</ul>
</section>
<section id="when-and-where-does-this-converge" class="slide level2">
<h2>When and Where Does This Converge?</h2>
<ul>
<li><p>Skipping a million details, see <a href="https://probml.github.io/pml-book/book1.html">ProbML Book 1</a> Section 8.2.2 and <a href="https://www.cs.ubc.ca/~schmidtm/Courses/340-F22/L14.pdf">Mark Schmidt’s basic</a> and <a href="https://www.cs.ubc.ca/~schmidtm/Courses/5XX-S22/S1.pdf">more advanced notes</a></p></li>
<li><p>For strictly convex problems this converges to the global minima, though sufficient conditions include Robbins-Monro <span class="math inline">\(\lim_{T\to \infty} \eta_T \to 0\)</span> and</p>
<p><span class="math display">\[
\lim_{T\to\infty}\frac{\sum_{t=1}^T \eta_t}{\sum_{t=1}^T \eta_t^2} = 0
\]</span></p></li>
<li><p>For problems that not globally convex this may go to local optima, but if the function is locally strictly convex then it will converge to a local optima</p></li>
<li><p>For other types of functions (e.g., <a href="https://en.wikipedia.org/wiki/Invex_function">invex</a>) it may still converge to the “right” solution in some important sense</p></li>
</ul>
</section>
<section id="preconditioned-gradient-descent" class="slide level2">
<h2>Preconditioned Gradient Descent</h2>
<ul>
<li><p>As we saw analyzing LLS, badly conditioned problems converge slowly with iterative methods</p></li>
<li><p>We can precondition a problem as we did with linear systems, and it has the same stationary point</p></li>
<li><p>Choose some <span class="math inline">\(C_t\)</span> for preconditioned gradient descent <span class="math display">\[
\theta_{t+1} = \theta_t - \eta_t C_t \nabla \mathcal{L}(\theta_t)
\]</span></p></li>
<li><p>We saw before that the Hessian tells us the geometry, so the optimal preconditioner must be related to <span class="math inline">\(\nabla^2 \mathcal{L}(\theta_t)\)</span></p></li>
</ul>
</section>
<section id="second-order-methods" class="slide level2">
<h2>Second-Order Methods</h2>
<ul>
<li>See <a href="https://probml.github.io/pml-book/book1.html">ProbML Book 1</a> Section 8.3 and <a href="https://www.cs.ubc.ca/~schmidtm/Courses/5XX-S22/S2.pdf">Mark Schmidt’s Notes</a></li>
<li>Adapt <span class="math inline">\(\eta_t C_t\)</span> to use the Hessian (e.g., Newton’s Method)</li>
</ul>
<p><span class="math display">\[
\theta_{t+1} = \theta_t - \eta_t \left[\nabla^2 \mathcal{L}(\theta_t) \right]^{-1}\nabla \mathcal{L}(\theta_t)
\]</span></p>
<ul>
<li>Second order methods are rarer because the calculating the Hessian is no longer the same computational order as <span class="math inline">\(\mathcal{L}(\theta)\)</span></li>
<li>See <a href="https://probml.github.io/pml-book/book1.html">ProbML Book 1</a> Section 8.3.2 for info on Quasi-Newtonian methods which approximation Hessian using gradients like BFGS</li>
</ul>
</section>
<section id="momentum" class="slide level2">
<h2>Momentum</h2>
<ul>
<li>See <a href="https://probml.github.io/pml-book/book1.html">ProbML Book 1</a> Section 8.2.4 and <a href="https://www.cs.ubc.ca/~schmidtm/Courses/5XX-S22/S2.pdf">Mark Schmidt’s Notes</a></li>
<li>Can use “momentum”, which speeds up convergence, helps avoid local optima, and moves fast in flat regions</li>
<li>Momentum will be a common feature of many ML optimizers (e.g.&nbsp;Adam, RMSProp, etc.) as it helps with heavily non-convex problems</li>
<li>A classic method is called Nesterov Accelerated Gradient (NAG), which is a modification of gradient descent for some <span class="math inline">\(\beta_t\in (0,1)\)</span> (e.g., <span class="math inline">\(0.9\)</span>)</li>
</ul>
<p><span class="math display">\[
\begin{aligned}
\hat{\theta}_{t+1} &amp;= \theta_t + \beta_t(\theta_t - \theta_{t-1})\\
\theta_{t+1} &amp;= \hat{\theta}_{t+1} - \eta_t \nabla \mathcal{L}(\hat{\theta}_{t+1})
\end{aligned}
\]</span></p>
</section>
<section id="does-uniqueness-matter" class="slide level2">
<h2>Does Uniqueness Matter?</h2>
<ul>
<li>Remember from our previous lecture on Sobolev norms and regularization that we care about functions, not parameters.</li>
<li>Consider when <span class="math inline">\(\theta\)</span> is used as parameters for a function (e.g.&nbsp;<span class="math inline">\(\hat{f}_{\theta}\)</span>)
<ul>
<li>Then what does a lack of convergence of the <span class="math inline">\(\theta_t\)</span> or multiplicity with multiple <span class="math inline">\(\theta\)</span> solutions mean?</li>
<li>Maybe nothing! If <span class="math inline">\(||\hat{f}_{\theta_0} - \hat{f}_{\theta_1}||_S\)</span> is small, then the functions themselves may be in the same equivalence class. Depends on the norm, of course.</li>
</ul></li>
<li>This topic will be discussed when we consider double-descent curves, but the punchline for now is that the training/optimization is a means to an end (i.e., generalization) and not an end in itself.</li>
</ul>
</section>
<section id="regularization" class="slide level2">
<h2>Regularization</h2>
<ul>
<li>See <a href="https://www.cs.ubc.ca/~schmidtm/Courses/340-F22/L16.pdf">Mark Schmidt’s Notes</a>. For LLS this is the ridge regression</li>
<li>We discussed regularization as a way to deal with multiplicity</li>
</ul>
<p><span class="math display">\[
\min_{\theta}\left[\mathcal{L}(\theta) + \frac{\alpha}{2} ||\theta||^2\right]
\]</span></p>
<ul>
<li>Gradient descent becomes (called “weight decay” in ML, and “ridge regression” if objective is LLS)</li>
</ul>
<p><span class="math display">\[
\theta_{t+1} = \theta_t - \eta_t \left[\nabla \mathcal{L}(\theta_t) + \alpha \theta_t\right]
\]</span></p>
<ul>
<li>Mapping of regularized <span class="math inline">\(\theta_t\)</span> to a <span class="math inline">\(f_{\theta_t}\)</span> is subtle if nonlinear</li>
</ul>
</section></section>
<section>
<section id="stochastic-optimization" class="title-slide slide level1 center">
<h1>Stochastic Optimization</h1>

</section>
<section id="are-gradients-really-that-cheap-to-calculate" class="slide level2">
<h2>Are Gradients Really that Cheap to Calculate?</h2>
<ul>
<li>Consider that the objective often involves data (or grid points for interpolation)
<ul>
<li>Denote <span class="math inline">\(x_n\)</span>, and observables <span class="math inline">\(y_n\)</span> for <span class="math inline">\(n=1, \ldots N\)</span></li>
</ul></li>
<li>With VJPs, the computational order of <span class="math inline">\(\nabla_{\theta} \mathcal{L}(\theta;\{x_n, y_n\}_{n=1}^N)\)</span> may be the same as that of <span class="math inline">\(\mathcal{L}\)</span> itself</li>
<li>However, keep in mind that reverse-mode requires storing the intermediate values in the “primal” calculation (i.e., <span class="math inline">\(\mathcal{L}(\theta;\{x_n, y_n\}_{n=1}^N)\)</span>)
<ul>
<li>Hence, the memory requirements grow with <span class="math inline">\(N\)</span></li>
<li>This may be a big problem for large datasets or complicated calculations, especially with GPUs which have more limited memory</li>
</ul></li>
</ul>
</section>
<section id="do-we-need-the-full-gradient" class="slide level2">
<h2>Do We Need the Full Gradient?</h2>
<ul>
<li><p>In practice, it is impossible to calculate the full gradient for large datasets</p></li>
<li><p>In GD, the gradient provided the direction of steepest descent</p></li>
<li><p>Consider an algorithm with a <span class="math inline">\(g_t\)</span> as an unbiased estimate of the gradient</p>
<p><span class="math display">\[
\begin{aligned}
\theta_{t+1} = \theta_t - \eta_t g_t\\
\mathbb{E}[g_t] = \nabla \mathcal{L}(\theta_t)
\end{aligned}
\]</span></p>
<ul>
<li>Make the <span class="math inline">\(\eta_t\)</span> smaller to deal with noise if this is high-variance</li>
<li>Choose <span class="math inline">\(g_t\)</span> to be far cheaper to calculate than <span class="math inline">\(\nabla \mathcal{L}(\theta_t)\)</span></li>
</ul></li>
<li><p>Will turn out that this also adds additional regularization, which helps with generalization</p></li>
</ul>
</section>
<section id="stochastic-optimization-1" class="slide level2">
<h2>Stochastic Optimization</h2>
<ul>
<li>To formalize: Up until now our optimizers have been “deterministic”</li>
<li>Now we introduce a source of randomness <span class="math inline">\(z \sim q_{\theta}(z)\)</span>, i.e.&nbsp;it might depend on the estimated parameters <span class="math inline">\(\theta\)</span> later with RL/etc.
<ul>
<li><span class="math inline">\(z\)</span> could be a source of uncertainty in the environment</li>
<li><span class="math inline">\(z\)</span> could involve latent variables</li>
<li><span class="math inline">\(z\)</span> could come from randomness in the optimization process (e.g., using subsets of data to form <span class="math inline">\(g_t\)</span>)</li>
</ul></li>
<li>Denote expectations using this distribution as <span class="math inline">\(\mathbb{E}_{q_{\theta}(z)}\)</span></li>
<li>For now, drop the dependence on <span class="math inline">\(\theta\)</span> for simplicity, though it becomes crucial for understanding reinforcement learning/etc.</li>
</ul>
</section>
<section id="stochastic-objective" class="slide level2">
<h2>Stochastic Objective</h2>
<ul>
<li>The full optimization problem is then to minimize this stochastic objective</li>
</ul>
<p><span class="math display">\[
\min_{\theta}\overbrace{\mathbb{E}_{q(z)} \tilde{\mathcal{L}}(\theta, z)}^{\equiv \mathcal{L}(\theta)}
\]</span></p>
<ul>
<li>Under appropriate regularity conditions, could use GD on this objective</li>
</ul>
<p><span class="math display">\[
\nabla \mathcal{L}(\theta) = \mathbb{E}_{q(z)}\left[\nabla \tilde{\mathcal{L}}(\theta, z)\right]
\]</span></p>
<ul>
<li>But in practice, it is rare that we can marginalize out the <span class="math inline">\(z\)</span></li>
</ul>
</section>
<section id="unbiased-draws-from-the-gradient" class="slide level2">
<h2>Unbiased Draws from the Gradient</h2>
<ul>
<li>Assume we can sample <span class="math inline">\(z_t \sim q(z)\)</span> IID</li>
<li>Then with enough regularity the gradient using just <span class="math inline">\(z_t\)</span> is unbiased</li>
</ul>
<p><span class="math display">\[
\mathbb{E}_{q(z)}\left[  \nabla \tilde{\mathcal{L}}(\theta_t, z_t) \right] = \nabla \mathcal{L}(\theta_t)
\]</span></p>
<ul>
<li>That is, on average <span class="math inline">\(\nabla \tilde{\mathcal{L}}(\theta_t, z_t)\)</span> is in the right direction for minimizing <span class="math inline">\(\mathcal{L}(\theta_t)\)</span></li>
<li>This basic approach of finding unbiased estimators of the gradient (and finding ways to lower the variance) is at the heart of most ML optimization algorithms</li>
</ul>
</section>
<section id="stochastic-gradient-descent" class="slide level2">
<h2>Stochastic Gradient Descent</h2>
<ul>
<li>See <a href="https://probml.github.io/pml-book/book1.html">ProbML Book 1</a> Section 8.4, <a href="https://probml.github.io/pml-book/book2.html">ProbML Book 2</a> Section 6.3, and <a href="https://www.cs.ubc.ca/~schmidtm/Courses/340-F22/L23.pdf">Mark Schmidt’s Notes</a></li>
<li>Given the previous slide, given IID samples <span class="math inline">\(z_t \sim q\)</span>, the gradient is unbiased and we have the simplest version of stochastic gradient descent (SGD)</li>
</ul>
<p><span class="math display">\[
\theta_{t+1} = \theta_t - \eta_t \nabla \tilde{\mathcal{L}}(\theta_t, z_t)
\]</span></p>
<ul>
<li>Which converges to the minima of <span class="math inline">\(\min_{\theta} \mathcal{L}(\theta)\)</span> under appropriate conditions</li>
<li>We can layer on all of the other features we discussed (e.g., momentum, preconditioning, etc) with SGD, but some become especially important (e.g.&nbsp;the <span class="math inline">\(\eta_t\)</span> schedule)</li>
</ul>
</section>
<section id="finite-sum-objectives" class="slide level2">
<h2>Finite-Sum Objectives</h2>
<ul>
<li><p>Consider a special case of the loss function which is the sum of <span class="math inline">\(N\)</span> terms. For example with empirical risk minimization used in LLS/etc.</p>
<ul>
<li><span class="math inline">\(z_n \equiv (x_n, y_n)\)</span> are typically data, observables, or grid points</li>
<li><span class="math inline">\(\ell(\theta, x_n, y_n)\)</span> is a loss function for a single data point (e.g., forecasting using some <span class="math inline">\(f_{\theta}\)</span>)</li>
</ul>
<p><span class="math display">\[
\mathcal{L}(\theta) = \frac{1}{N}\sum_{n=1}^N \tilde{\mathcal{L}}(\theta, z_n) \equiv \frac{1}{N}\sum_{n=1}^N \ell(\theta, x_n, y_n)
\]</span></p>
<ul>
<li>For example, LLS is <span class="math inline">\(\ell(\theta, x_n, y_n) = ||y_n - \theta \cdot x_n||^2_2\)</span></li>
</ul></li>
<li><p>In this case, the randomness of <span class="math inline">\(z_t\)</span> is which data point is chosen</p></li>
</ul>
</section>
<section id="sgd-for-finite-sum-objectives" class="slide level2">
<h2>SGD for Finite-Sum Objectives</h2>
<ul>
<li>Hence consider sampling <span class="math inline">\(z_t \equiv (x_t, y_t)\)</span> from our data.
<ul>
<li>In principle, IID with replacement</li>
</ul></li>
<li>Then run SGD on one data point at a time</li>
</ul>
<p><span class="math display">\[
\theta_{t+1} = \theta_t - \eta_t \nabla_{\theta} \ell(\theta_t, x_t, y_t)
\]</span></p>
<ul>
<li>This may converges to the minima of <span class="math inline">\(\mathcal{L}(\theta)\)</span>, and potentially the storage requirements for calculations the gradient are radically reduced</li>
<li>You can guess that the <span class="math inline">\(\eta_t\)</span> parameter is especially sensitive to the variance of the gradient estimate</li>
</ul>
</section>
<section id="decrease-variance-with-multiple-draws" class="slide level2">
<h2>Decrease Variance with Multiple Draws</h2>
<ul>
<li>With a single draw, the variance of the gradient estimate may be high</li>
</ul>
<p><span class="math display">\[
\mathbb{E}\left[\nabla_{\theta} \ell(\theta_t, x_t, y_t)- \nabla \mathcal{L}(\theta_t)\right]^2
\]</span></p>
<ul>
<li>One tool to decrease the variance is just more monte-carlo draws. With finite-sum objectives draw <span class="math inline">\(B \subseteq \{1,\ldots N\}\)</span> indices</li>
</ul>
<p><span class="math display">\[
\frac{1}{|B|}\sum_{n \in B} \nabla_{\theta} \ell(\theta_t, x_n, y_n)
\]</span></p>
<ul>
<li>Classic SGD: <span class="math inline">\(|B|=1\)</span>; GD: <span class="math inline">\(B = \{1, \ldots N\}\)</span> and in between is called “minibatch SGD”. Usually minibatch is implied with “SGD”</li>
</ul>
</section>
<section id="minibatch-sgd" class="slide level2">
<h2>Minibatch SGD</h2>
<ul>
<li><p>Algorithm is to draw <span class="math inline">\(B_t\)</span> indices at each step and execute SGD <span class="math display">\[
\begin{aligned}
g_t \equiv \frac{1}{|B_t|}\sum_{n \in B_t} \nabla_{\theta} \ell(\theta_t, x_n, y_n)\\
\theta_{t+1} = \theta_t - \eta_t g_t
\end{aligned}
\]</span></p></li>
<li><p>Note that we never need to calculate <span class="math inline">\(\mathcal{L}(\theta_t)\)</span> directly, so can write our code to all operate on batches <span class="math inline">\(B_t\)</span></p></li>
<li><p>Then layer other tricks on top (e.g., momentum, preconditioning, etc.)</p>
<ul>
<li>In principle you could also use minibatch with second-order or quasi-newtonian methods but much rarer</li>
</ul></li>
</ul>
</section>
<section id="choosing-batches" class="slide level2">
<h2>Choosing Batches</h2>
<ul>
<li>Choosing the <span class="math inline">\(B_t\)</span> process may be tricky. You could sample from <span class="math inline">\(\{1,\ldots N\}\)</span>
<ul>
<li>with replacement</li>
<li>without replacement</li>
<li>without replacement after shuffling the data, and then ensure you have gone through all of the data before repeating</li>
<li>etc.</li>
</ul></li>
<li>Just remember the goal: variance reduction on gradient estimates</li>
<li>You want it to be unbiased in principle (consider partitioning the data into batches and operating sequentially?)</li>
<li>More art than science in many cases, because it requires many priors</li>
</ul>
</section>
<section id="grad-student-descent" class="slide level2">
<h2>“Grad Student Descent”</h2>
<ul>
<li>This is how virtually all deep learning works. Just swap SGD with slightly fancier algorithms using momentum, tinker with parameters, etc.</li>
<li>In practice, all of these optimizer settings (e.g., how large for <span class="math inline">\(|B_t|\)</span>, <span class="math inline">\(\eta_t\)</span>, convergence criteria, etc.) are fragile and require a lot of tuning
<ul>
<li>Part of a a process called <strong>hyperparameter optimization (HPO)</strong> where you try to find the best non-model parameters for your goals</li>
<li>Same issue with all numerical methods in economics (e.g.&nbsp;convergence criteria of fixed point iteration, initial conditions)</li>
</ul></li>
<li>The concern is not just that it is time-consuming for researchers (and ML “Grad Students”), but that it is easy for priors to sneak in and bias results</li>
</ul>
</section>
<section id="what-was-our-goal" class="slide level2">
<h2>What was our Goal?</h2>
<ul>
<li>We will address this more formally next lecture, but it is worth stepping back to think about our goals. Loosely:
<ul>
<li>If we are solving an empirical risk minimization problem (like regressions, etc.) or interpolation, then our goal is to use the “data” to find a function <span class="math inline">\(\hat{f}_{\theta}\)</span> that is close to the “true” function <span class="math inline">\(f^*\)</span></li>
</ul></li>
<li>Fitting <span class="math inline">\(\hat{f}_{\theta}\)</span> is easy, but we want it to <strong>generalize</strong> within the true distribution
<ul>
<li>But we don’t know that distribution (hence the “empirical”)</li>
<li>So a typical approach is to emulate this by splitting the data we have</li>
<li>But HPO is dangerous because if we are not careful we can “contaminate” our process for finding <span class="math inline">\(\hat{f}_{\theta}\)</span> using some of the data we intend to check it with. Which might lead to overfitting/etc.</li>
</ul></li>
</ul>
</section></section>
<section>
<section id="training-loops" class="title-slide slide level1 center">
<h1>Training Loops</h1>

</section>
<section id="splitting-the-data" class="slide level2">
<h2>Splitting the Data</h2>
<p>A standard way to do this for Empirical Risk Minimization/Regressions/etc. is to split it into three parts:</p>
<ol type="1">
<li><strong>Training</strong> data used in fitting our approximations
<ul>
<li>This is just a means to an end in ML and economics</li>
</ul></li>
<li><strong>Validation</strong> data used for HPO and checking convergence criteria
<ul>
<li>Be cautious to avoid using it for training</li>
</ul></li>
<li><strong>Test</strong> data used to evaluate the generalization performance
<ul>
<li>Ensure we don’t accidentally use it in training or validation</li>
</ul></li>
</ol>
<p>Not all problems will have this structure, and not all with have validation data.</p>
</section>
<section id="why-separate-validation-and-test" class="slide level2">
<h2>Why Separate Validation and Test?</h2>
<ul>
<li>As we will see in deep learning, with massive over-parameterization you typically can interpolate all of the training data.
<ul>
<li>Minimizing training loss is a means to an end, which usually ends at zero</li>
</ul></li>
<li>The validation data might be used to check stopping criteria by checking how well the approximation generalizes to data outside of training</li>
<li>But if we are using it for a stopping criteria or HPO, then is is <strong>contaminated</strong>!
<ul>
<li>Distorts our picture of generalization if we combine it into test data</li>
</ul></li>
</ul>
</section>
<section id="what-about-interpolation-problems" class="slide level2">
<h2>What about Interpolation Problems?</h2>
<ul>
<li>When simply trying to find interpolating functions which solve functional equations, the risk of prior contamination is less clear</li>
<li>However, you may still want to separate out validation and test grid points because any data you use for HPO or convergence criteria can’t be used to understand generalization.</li>
<li>For example consider:
<ol type="1">
<li>Fit until “training” loss is zero</li>
<li>Keep running stochastic optimizer until “validation” loss is zero</li>
</ol></li>
<li>In that case, it crudely interpolating the validation data, which makes it equivalent to training data? Not useful for generalization
<ul>
<li>May find that the model generalized better if you <strong>stopped earlier</strong></li>
</ul></li>
</ul>
</section>
<section id="level-of-abstraction-for-optimizers" class="slide level2">
<h2>Level of Abstraction for Optimizers</h2>
<ul>
<li>While you can setup a standard optimization objective and optimizer, most ML frameworks work at a lower level</li>
<li>The key reasons are that:
<ul>
<li>Minibatching (usually just called “batches”) requires more flexibility in implementation to be efficient</li>
<li>Stopping criteria is more complicated with highly overparameterized models</li>
<li>Logging and validation logic requires more flexibility</li>
<li>Often you will want to take a snapshot of the current best solution and continue later for refinement (or to solve in parallel)</li>
</ul></li>
</ul>
</section>
<section id="steps-and-epochs" class="slide level2">
<h2>Steps and Epochs</h2>
<ul>
<li>There is a great deal of flexibility in how you setup the optimizer</li>
<li>But a common approach is to randomly shuffle the data, create a set of batches <span class="math inline">\(B_t\)</span> (without replacement), and then iterate through them</li>
<li>Terminology (when relevant)
<ul>
<li>Every iteration of SGD for a given batch is a <strong>step</strong></li>
<li>If you have gone through the entire dataset once, we say that you have completed an <strong>epoch</strong></li>
</ul></li>
<li>At the end of an epoch is a good time to log, check the validation loss, and potentially stop the training</li>
</ul>
</section>
<section id="software-components-used-in-ml" class="slide level2">
<h2>Software Components used in ML</h2>
<p>Some common software components for optimization are</p>
<ol type="1">
<li><strong>Autodifferentiation</strong> and libraries of functions provide the approximation class</li>
<li><strong>Data loaders</strong> which will take care of providing batches to the optimizers</li>
<li><strong>Optimizers</strong> are typically iterative, have an internal state, and you can update with one sample of the gradient for that batch</li>
<li><strong>Logging</strong> and visualization tools to track progress because the optimization process may be slow and you want to do HPO</li>
<li><strong>HPO</strong> software using training, validation, and possibly test loss</li>
</ol>
</section>
<section id="logging-and-visualization" class="slide level2">
<h2>Logging and Visualization</h2>
<ul>
<li>Several tools exist for logging to babysit optimizers, find good hyperparameters, etc. including <a href="https://www.tensorflow.org/tensorboard">Tensorboard</a>
<ul>
<li>But we will use <a href="https://wandb.ai/site">Weights and Biases</a> (W&amp;B) because it is a market leader, free for academics and seems to be the frontrunner</li>
</ul></li>
<li>Many algorithms and frameworks exist for HPO:
<ul>
<li><a href="https://wandb.ai/site">Weights and Biases</a> (W&amp;B) has a built-in HPO framework using random search and bayesian optimization</li>
<li><a href="https://optuna.org/">Optuna</a> and <a href="https://docs.ray.io/en/master/tune/index.html">Ray Tune</a> is a popular open-source HPO framework</li>
<li><a href="https://docs.ray.io/en/master/tune/index.html">Ray Tune</a> is a popular open-source HPO framework</li>
</ul></li>
<li>HPO frameworks will often use the <a href="https://github.com/shadawck/awesome-cli-frameworks#python">command-line</a> to run new jobs. <a href="https://github.com/google/python-fire">Python Fire</a></li>
</ul>
</section>
<section id="broad-frameworks-for-machine-learning" class="slide level2">
<h2>Broad Frameworks for Machine Learning</h2>
<ul>
<li>You can just hand-code loops/etc. which seems the best approach for JAX
<ul>
<li>Even with Pytorch, it isn’t obvious that a framework is better ex-post, though ex-ante it can help you try different permutations easily</li>
</ul></li>
<li><a href="https://www.pytorchlightning.ai/">Pytorch Lightning</a> is a popular framework which will formalize the training loops even across distributed systems and make CLI, HPO, logging, etc. convenient
<ul>
<li>It remains fairly flexible because it is just wrapping Pytorch</li>
</ul></li>
<li><a href="https://keras.io/">Keras</a> is a similar framework with the ability to target multiple backends (e.g., Pytorch, JAX)
<ul>
<li>The challenge is that it is much less flexible for non-typical research</li>
</ul></li>
<li><a href="https://github.com/facebookresearch/hydra">Hydra</a> is a framework for more serious engineering code</li>
</ul>
</section></section>
<section>
<section id="detailed-pytorch-linear-regression-example" class="title-slide slide level1 center">
<h1>Detailed Pytorch Linear Regression Example</h1>

</section>
<section id="linear-regression-examples" class="slide level2">
<h2>Linear Regression Examples</h2>
<ul>
<li>Of course SGD is a terrible way to do a standard linear regression, but it will help us understand the mechanics with a well-understood problem</li>
<li>These examples simulate data for some: <span class="math inline">\(y_n = x_n \cdot \theta + \sigma \epsilon\)</span> for <span class="math inline">\(\epsilon \sim N(0,1)\)</span></li>
<li>They then show various features of the optimization pipeline and software to implement the LLS ERM objective</li>
</ul>
<p><span class="math display">\[
\min_{\theta} \frac{1}{N} \sum_{n=1}^N \left[y_n - x_n \cdot \theta\right]^2
\]</span></p>
<ul>
<li>Which we note has a finite-sum objective, which lets us use minibatch SGD with gradient estimates</li>
<li>Install in your environment with <code>pip install -r requirements.txt</code></li>
</ul>
</section>
<section id="packages" class="slide level2">
<h2>Packages</h2>
<ul>
<li>Before showing all of the variations, here we will implement an inline SGD version with minibatches</li>
</ul>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a><span class="im">import</span> torch</span>
<span id="cb1-2"><a href="#cb1-2"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb1-3"><a href="#cb1-3"></a><span class="im">import</span> torch.optim <span class="im">as</span> optim</span>
<span id="cb1-4"><a href="#cb1-4"></a><span class="im">import</span> equinox <span class="im">as</span> eqx</span>
<span id="cb1-5"><a href="#cb1-5"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> TensorDataset, DataLoader, TensorDataset</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="simulate-data" class="slide level2">
<h2>Simulate Data</h2>
<p><span class="math display">\[
y \sim N(x \cdot \theta, \sigma^2)
\]</span></p>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1"></a>N <span class="op">=</span> <span class="dv">500</span>  <span class="co"># samples</span></span>
<span id="cb2-2"><a href="#cb2-2"></a>M <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb2-3"><a href="#cb2-3"></a>sigma <span class="op">=</span> <span class="fl">0.001</span></span>
<span id="cb2-4"><a href="#cb2-4"></a>theta <span class="op">=</span> torch.randn(M)</span>
<span id="cb2-5"><a href="#cb2-5"></a>X <span class="op">=</span> torch.randn(N, M)</span>
<span id="cb2-6"><a href="#cb2-6"></a>Y <span class="op">=</span> X <span class="op">@</span> theta <span class="op">+</span> sigma <span class="op">*</span> torch.randn(N)</span>
<span id="cb2-7"><a href="#cb2-7"></a>dataset <span class="op">=</span> TensorDataset(X, Y)</span>
<span id="cb2-8"><a href="#cb2-8"></a><span class="bu">print</span>(dataset[<span class="dv">0</span>]) <span class="co"># returns tuples of (x_n, y_n)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>(tensor([-0.3331,  0.3385]), tensor(0.9287))</code></pre>
</div>
</div>
</section>
<section id="dataloaders-provide-batches" class="slide level2">
<h2>Dataloaders Provide Batches</h2>
<ul>
<li>Code which serves up random batches of data for gradient estimates are called “dataloaders”</li>
<li>The following code shuffles the data and provides batches of size <code>batch_size</code></li>
<li>It returns a <a href="https://docs.python.org/3/howto/functional.html#generator-expressions-and-list-comprehensions">Python generator</a> which can be iterated until it hits the end of the data</li>
</ul>
<div class="cell columns column-output-location" data-execution_count="4">
<div class="column">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1"></a>batch_size <span class="op">=</span> <span class="dv">8</span></span>
<span id="cb4-2"><a href="#cb4-2"></a>train_loader <span class="op">=</span> DataLoader(dataset,</span>
<span id="cb4-3"><a href="#cb4-3"></a>                batch_size<span class="op">=</span>batch_size,</span>
<span id="cb4-4"><a href="#cb4-4"></a>                shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb4-5"><a href="#cb4-5"></a><span class="co"># e.g. iterate and get first element</span></span>
<span id="cb4-6"><a href="#cb4-6"></a><span class="bu">print</span>(<span class="bu">next</span>(<span class="bu">iter</span>(train_loader)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div><div class="column">
<div class="cell-output cell-output-stdout">
<pre><code>[tensor([[ 1.0887, -2.0147],
        [-0.9769,  0.9183],
        [ 0.3217, -0.2553],
        [ 0.2292, -1.2586],
        [-0.2504, -0.7905],
        [-0.4938, -0.8292],
        [-0.3105,  0.2425],
        [-0.8839, -0.0147]]), tensor([-3.7268,  2.6681, -0.8417, -1.4195, -0.0978,  0.3626,  0.8096,  1.7694])]</code></pre>
</div>
</div>
</div>
</section>
<section id="loss-function-for-gradient-descent" class="slide level2">
<h2>Loss Function for Gradient Descent</h2>
<ul>
<li>Reminder: need to provide AD-able functions which give a gradient estimate, not necessarily the objective itself!</li>
<li>In particular, for LLS we simply can find the MSE between the prediction and the data for the batch itself</li>
</ul>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1"></a><span class="kw">def</span> residuals(model, X, Y):  <span class="co"># batches or full data</span></span>
<span id="cb6-2"><a href="#cb6-2"></a>    Y_hat <span class="op">=</span> model(X).squeeze()</span>
<span id="cb6-3"><a href="#cb6-3"></a>    <span class="cf">return</span> ((Y_hat <span class="op">-</span> Y) <span class="op">**</span> <span class="dv">2</span>).mean()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="hypothesis-class" class="slide level2">
<h2>Hypothesis Class</h2>
<ul>
<li>The “Hypothesis Class” for our ERM approximation is linear in this case.</li>
<li>Anything with differentiable parameters is a sub-class of the <code>nn.Module</code> in Pytorch. Special case of Neural Networks</li>
<li>In this case, we can just use a prebuilt linear approximation from <span class="math inline">\(\mathbb{R}^M \to \mathbb{R}^1\)</span> without an affine constant term.</li>
<li>The underlying parameters will have a random initialization, which becomes <strong>crucial</strong> with overparameterized models (but wouldn’t be important here)</li>
</ul>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1"></a>model <span class="op">=</span> nn.Linear(M, <span class="dv">1</span>, bias<span class="op">=</span><span class="va">False</span>)  <span class="co"># random initialization</span></span>
<span id="cb7-2"><a href="#cb7-2"></a><span class="bu">print</span>(model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Linear(in_features=2, out_features=1, bias=False)</code></pre>
</div>
</div>
</section>
<section id="optimizer" class="slide level2">
<h2>Optimizer</h2>
<ul>
<li>First-order optimizers take steps using gradient estimates</li>
<li>In many ML applications you will want control over the process, so will manually call the function to collect the gradient estimate then call the optimizer to take the next step</li>
<li>Here we will just use SGD with a fixed learning rate</li>
<li>Note that the optimizer is constructed to look directly at the <code>parameters</code> for your underlying model(s)</li>
</ul>
<div class="cell columns column-output-location" data-execution_count="7">
<div class="column">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1"></a>optimizer <span class="op">=</span> optim.SGD(</span>
<span id="cb9-2"><a href="#cb9-2"></a>    model.parameters(), lr<span class="op">=</span><span class="fl">0.001</span></span>
<span id="cb9-3"><a href="#cb9-3"></a>)</span>
<span id="cb9-4"><a href="#cb9-4"></a><span class="bu">print</span>(optimizer)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div><div class="column">
<div class="cell-output cell-output-stdout">
<pre><code>SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 0.001
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
)</code></pre>
</div>
</div>
</div>
</section>
<section id="training-loop" class="slide level2">
<h2>Training Loop</h2>
<ul>
<li>Finally, we can loop for multiple “epochs” of passes through the data with the dataloader, calling the optimizer to update each time</li>
<li>The <code>for ... in train_loader:</code> will repeat until the end of the data and continue to the next epoch (i.e., pass through data)</li>
<li>Each batch updates a <code>step</code> using the optimizer, which is unaware of epochs/batches/etc.</li>
</ul>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">300</span>):</span>
<span id="cb11-2"><a href="#cb11-2"></a>    <span class="cf">for</span> X_batch, Y_batch <span class="kw">in</span> train_loader:</span>
<span id="cb11-3"><a href="#cb11-3"></a>        optimizer.zero_grad() </span>
<span id="cb11-4"><a href="#cb11-4"></a>        loss <span class="op">=</span> residuals(model, X_batch, Y_batch)  <span class="co"># primal</span></span>
<span id="cb11-5"><a href="#cb11-5"></a>        loss.backward()  <span class="co"># backprop/reverse-mode AD</span></span>
<span id="cb11-6"><a href="#cb11-6"></a>        <span class="co"># Now the model.parameters have gradients updated, so...</span></span>
<span id="cb11-7"><a href="#cb11-7"></a>        optimizer.step()  <span class="co"># Update the optimizers internal parameters</span></span>
<span id="cb11-8"><a href="#cb11-8"></a><span class="bu">print</span>(<span class="ss">f"||theta - theta_hat|| = </span><span class="sc">{</span>torch<span class="sc">.</span>norm(theta <span class="op">-</span> model.weight.squeeze())<span class="sc">}</span><span class="ss">"</span>)        </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>||theta - theta_hat|| = 3.726587237906642e-05</code></pre>
</div>
</div>
</section></section>
<section>
<section id="more-pytorch-linear-regression-examples" class="title-slide slide level1 center">
<h1>More Pytorch Linear Regression Examples</h1>

</section>
<section id="gradient-descent-1" class="slide level2">
<h2>Gradient Descent</h2>
<ul>
<li>See <a href="examples/linear_regression_pytorch_gd.py">examples/linear_regression_pytorch_gd.py</a></li>
<li>Simulates data and shows the basic training loop</li>
<li>Using the “full batch” to calculate the residuals</li>
</ul>
</section>
<section id="stochastic-gradient-descent-1" class="slide level2">
<h2>Stochastic Gradient Descent</h2>
<ul>
<li>See <a href="examples/linear_regression_pytorch_sgd.py">examples/linear_regression_pytorch_sgd.py</a></li>
<li>This takes the existing code, but adds in code to calculate gradient estimates using minibatches</li>
<li>Note that this is creating batches by shuffling the data and the going through it <code>batch_size</code> chunks at a time</li>
<li>When it gets to the end of that data, it is the end of the <code>epoch</code></li>
</ul>
</section>
<section id="adam-bells-and-whistles" class="slide level2">
<h2>Adam + Bells and Whistles</h2>
<ul>
<li>See <a href="examples/linear_regression_pytorch_adam.py">examples/linear_regression_pytorch_adam.py</a></li>
<li>This extends the previous version and adds in the full train/val/test datasplit
<ul>
<li>The <code>val_loss</code> is collected and displayed at the end of each epoch</li>
</ul></li>
<li>It also shows a learning rate scheduler and a few utilities for logging and early stopping</li>
</ul>
</section>
<section id="logging-with-weights-and-biases" class="slide level2">
<h2>Logging with Weights and Biases</h2>
<ul>
<li><p>See <a href="examples/linear_regression_pytorch_logging.py">examples/linear_regression_pytorch_logging.py</a></p></li>
<li><p>This adds in support for Weights and Biases, and also demonstrates the use of a custom <code>nn.Module</code> for the hypothesis class</p>
<ol type="1">
<li>Go to <a href="https://wandb.ai/">wandb.ai</a> and create an account, ideally linked to your github</li>
<li>Ensure you have installed the packages with <code>pip install -r requirements.txt</code></li>
<li>Run <code>wandb login</code> in terminal to connect to your account</li>
</ol></li>
<li><p>You will then be able to run these files and see results on <a href="https://wandb.ai/">wandb.ai</a></p></li>
</ul>
</section>
<section id="pytorch-lightning" class="slide level2">
<h2>Pytorch Lightning</h2>
<ul>
<li>See <a href="examples/linear_regression_pytorch_lightning.py">examples/linear_regression_pytorch_lightning.py</a></li>
<li>See <a href="examples/linear_regression_pytorch_lightning_defaults.yaml">examples/linear_regression_pytorch_lightning_defaults.yaml</a> for default HPO and parameters</li>
<li>This is using many features in <a href="https://www.pytorchlightning.ai/">Pytorch Lightning</a> to simplify the code</li>
<li>The optimizer, stopping rules, logging, learning rate scheduler, etc. are all handled by the framework and can be configured in that file or on the CLI.</li>
<li>Can change values on commandline to override the <code>yaml</code> file,</li>
</ul>
<div class="sourceCode" id="cb13"><pre class="sourceCode numberSource bash number-lines code-with-copy"><code class="sourceCode bash"><span id="cb13-1"><a href="#cb13-1"></a><span class="ex">python</span> lectures/examples/linear_regression_pytorch_lightning.py <span class="at">--optimizer.lr</span><span class="op">=</span>0.0001 <span class="at">--model.N</span><span class="op">=</span>500</span>
<span id="cb13-2"><a href="#cb13-2"></a><span class="ex">python</span> lectures/examples/linear_regression_pytorch_lightning.py <span class="at">--trainer.max_epochs</span><span class="op">=</span>500</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="sweeps-with-wb" class="slide level2">
<h2>Sweeps with W&amp;B</h2>
<ul>
<li>See <a href="lectures/examples/linear_regression_pytorch_sweep.yaml">examples/linear_regression_pytorch_sweep.yaml</a> for a <a href="https://docs.wandb.ai/guides/sweeps">sweep file</a> which provides a HPO experiment to run and log
<ul>
<li>Executes variations on <code>--model.batch_size=32</code> and <code>--model.lr=0.001</code> etc</li>
<li>Tries to choose them to minimize the <code>val_loss</code> as a HPO objective</li>
<li>First, create the sweep, <code>wandb sweep lectures/examples/linear_regression_pytorch_sweep.yaml</code></li>
<li>Then run <code>wandb agent &lt;sweep_id&gt;</code> with returned sweep id</li>
<li>Call <code>wandb agent &lt;sweep_id&gt;</code> on multiple computers to run in parallel</li>
</ul></li>
<li>Any CLI implementation can be used with these sorts of frameworks</li>
</ul>
</section></section>
<section>
<section id="detailed-jax-linear-regression-example" class="title-slide slide level1 center">
<h1>Detailed JAX Linear Regression Example</h1>

</section>
<section id="packages-1" class="slide level2">
<h2>Packages</h2>
<ul>
<li><code>optax</code> is a common package for ML optimization methods</li>
</ul>
<div class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1"></a><span class="im">import</span> jax</span>
<span id="cb14-2"><a href="#cb14-2"></a><span class="im">import</span> jax.numpy <span class="im">as</span> jnp</span>
<span id="cb14-3"><a href="#cb14-3"></a><span class="im">from</span> jax <span class="im">import</span> grad, jit, value_and_grad, vmap</span>
<span id="cb14-4"><a href="#cb14-4"></a><span class="im">from</span> jax <span class="im">import</span> random</span>
<span id="cb14-5"><a href="#cb14-5"></a><span class="im">import</span> optax</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="simulate-data-1" class="slide level2">
<h2>Simulate Data</h2>
<ul>
<li>Few differences here, except for manual use of the <code>key</code></li>
<li>Remember that if you use the same <code>key</code> you get the same value.</li>
<li>See <a href="https://jax.readthedocs.io/en/latest/jax-101/05-random-numbers.html">JAX docs</a> for more details</li>
</ul>
<div class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1"></a>N <span class="op">=</span> <span class="dv">500</span>  <span class="co"># samples</span></span>
<span id="cb15-2"><a href="#cb15-2"></a>M <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb15-3"><a href="#cb15-3"></a>sigma <span class="op">=</span> <span class="fl">0.001</span></span>
<span id="cb15-4"><a href="#cb15-4"></a>key <span class="op">=</span> random.PRNGKey(<span class="dv">42</span>)</span>
<span id="cb15-5"><a href="#cb15-5"></a><span class="co"># Pattern: split before using key, replace name "key"</span></span>
<span id="cb15-6"><a href="#cb15-6"></a>key, <span class="op">*</span>subkey <span class="op">=</span> random.split(key, num<span class="op">=</span><span class="dv">4</span>)</span>
<span id="cb15-7"><a href="#cb15-7"></a>theta <span class="op">=</span> random.normal(subkey[<span class="dv">0</span>], (M,))</span>
<span id="cb15-8"><a href="#cb15-8"></a>X <span class="op">=</span> random.normal(subkey[<span class="dv">1</span>], (N, M))</span>
<span id="cb15-9"><a href="#cb15-9"></a>Y <span class="op">=</span> X <span class="op">@</span> theta <span class="op">+</span> sigma <span class="op">*</span> random.normal(subkey[<span class="dv">2</span>], (N,))  <span class="co"># Adding noise</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="dataloaders-provide-batches-1" class="slide level2">
<h2>Dataloaders Provide Batches</h2>
<ul>
<li>For more complicated data (e.g.&nbsp;images, text) JAX can use other packages, but it doesn’t have a canonical dataloader at this point</li>
<li>But in this case we can manually create this, using <a href="https://docs.python.org/3/howto/functional.html#generators"><code>yield</code></a></li>
</ul>
<div class="cell columns column-output-location" data-execution_count="11">
<div class="column">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1"></a><span class="kw">def</span> data_loader(key, X, Y, batch_size):</span>
<span id="cb16-2"><a href="#cb16-2"></a>    N <span class="op">=</span> X.shape[<span class="dv">0</span>]</span>
<span id="cb16-3"><a href="#cb16-3"></a>    <span class="cf">assert</span> N <span class="op">==</span> Y.shape[<span class="dv">0</span>]</span>
<span id="cb16-4"><a href="#cb16-4"></a>    indices <span class="op">=</span> jnp.arange(N)</span>
<span id="cb16-5"><a href="#cb16-5"></a>    indices <span class="op">=</span> random.permutation(key, indices)</span>
<span id="cb16-6"><a href="#cb16-6"></a>    <span class="co"># Loop over batches and yield</span></span>
<span id="cb16-7"><a href="#cb16-7"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, N, batch_size):</span>
<span id="cb16-8"><a href="#cb16-8"></a>        b_indices <span class="op">=</span> indices[i:i <span class="op">+</span> batch_size]</span>
<span id="cb16-9"><a href="#cb16-9"></a>        <span class="cf">yield</span> X[b_indices], Y[b_indices]</span>
<span id="cb16-10"><a href="#cb16-10"></a><span class="co"># e.g. iterate and get first element</span></span>
<span id="cb16-11"><a href="#cb16-11"></a>dl_test <span class="op">=</span> data_loader(key, X, Y, <span class="dv">4</span>)</span>
<span id="cb16-12"><a href="#cb16-12"></a><span class="bu">print</span>(<span class="bu">next</span>(<span class="bu">iter</span>(dl_test)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div><div class="column">
<div class="cell-output cell-output-stdout">
<pre><code>(Array([[ 0.05907545, -1.7277497 ],
       [-1.3816313 ,  0.33074763],
       [ 0.76224667, -0.07191363],
       [-0.46871892,  0.24884424]], dtype=float32), Array([-1.3571022 ,  0.07165897,  0.0480412 ,  0.13437417], dtype=float32))</code></pre>
</div>
</div>
</div>
</section>
<section id="hypothesis-class-1" class="slide level2">
<h2>Hypothesis Class</h2>
<ul>
<li>The “Hypothesis Class” for our ERM approximation is linear in this case</li>
<li>JAX is functional and non-mutating, so you must write stateless code</li>
<li>We will move towards a more general class with the <code>equinox</code> package, but for now we will implement the model with the parameters directly</li>
<li>The underlying parameters will have a random initialization, which becomes <strong>crucial</strong> with overparameterized models (but wouldn’t be important here)</li>
</ul>
<div class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1"></a><span class="kw">def</span> predict(theta, X):</span>
<span id="cb18-2"><a href="#cb18-2"></a>    <span class="cf">return</span> jnp.matmul(X, theta) <span class="co">#or jnp.dot(X, theta)</span></span>
<span id="cb18-3"><a href="#cb18-3"></a></span>
<span id="cb18-4"><a href="#cb18-4"></a><span class="co"># Need to randomize our own theta_0 parameters</span></span>
<span id="cb18-5"><a href="#cb18-5"></a>key, subkey <span class="op">=</span> random.split(key)</span>
<span id="cb18-6"><a href="#cb18-6"></a>theta_0 <span class="op">=</span> random.normal(subkey, (M,))</span>
<span id="cb18-7"><a href="#cb18-7"></a><span class="bu">print</span>(<span class="ss">f"theta_0 = </span><span class="sc">{</span>theta_0<span class="sc">}</span><span class="ss">, theta = </span><span class="sc">{</span>theta<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>theta_0 = [ 1.7535115  -0.07298409], theta = [0.1378821  0.79073715]</code></pre>
</div>
</div>
</section>
<section id="loss-function-for-gradient-descent-1" class="slide level2">
<h2>Loss Function for Gradient Descent</h2>
<ul>
<li>Reminder: need to provide AD-able functions which give a gradient estimate, not necessarily the objective itself!</li>
<li>In particular, for LLS we simply can find the MSE between the prediction and the data for the batch itself</li>
<li>For now, we are passing the <code>params</code> rather than the <code>model</code> itself</li>
</ul>
<div class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1"></a><span class="kw">def</span> vectorized_residuals(params, X, Y):</span>
<span id="cb20-2"><a href="#cb20-2"></a>    Y_hat <span class="op">=</span> predict(params, X)</span>
<span id="cb20-3"><a href="#cb20-3"></a>    <span class="cf">return</span> jnp.mean((Y_hat <span class="op">-</span> Y) <span class="op">**</span> <span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="optimizer-1" class="slide level2">
<h2>Optimizer</h2>
<ul>
<li>The <code>optimizer.init(theta_0)</code> provides the initial state for the iterations</li>
<li>With SGD it is empty, but with momentum/etc. it will have internal state</li>
</ul>
<div class="cell columns column-output-location" data-execution_count="14">
<div class="column">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1"></a>lr <span class="op">=</span> <span class="fl">0.001</span></span>
<span id="cb21-2"><a href="#cb21-2"></a>batch_size <span class="op">=</span> <span class="dv">16</span></span>
<span id="cb21-3"><a href="#cb21-3"></a>num_epochs <span class="op">=</span> <span class="dv">201</span></span>
<span id="cb21-4"><a href="#cb21-4"></a></span>
<span id="cb21-5"><a href="#cb21-5"></a><span class="co"># optax.adam(lr) is worse here</span></span>
<span id="cb21-6"><a href="#cb21-6"></a>optimizer <span class="op">=</span> optax.sgd(lr)</span>
<span id="cb21-7"><a href="#cb21-7"></a>opt_state <span class="op">=</span> optimizer.init(theta_0)</span>
<span id="cb21-8"><a href="#cb21-8"></a><span class="bu">print</span>(<span class="ss">f"Optimizer state:</span><span class="sc">{</span>opt_state<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb21-9"><a href="#cb21-9"></a>params <span class="op">=</span> theta_0 <span class="co"># initial condition</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div><div class="column">
<div class="cell-output cell-output-stdout">
<pre><code>Optimizer state:(EmptyState(), EmptyState())</code></pre>
</div>
</div>
</div>
</section>
<section id="using-optimizer-for-a-step" class="slide level2">
<h2>Using Optimizer for a Step</h2>
<ul>
<li>Here we write a (compiled) utility function which:
<ol type="1">
<li>Calculates the loss and gradient estimates for the batch</li>
<li>Updates the optimizer state</li>
<li>Applies the updates to the parameters</li>
<li>Returns the updated parameters, optimizer state, and loss</li>
</ol></li>
<li>The reason to set this up as a function is to maintain JAXs “pure” style</li>
</ul>
<div class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1"></a><span class="at">@jax.jit</span></span>
<span id="cb23-2"><a href="#cb23-2"></a><span class="kw">def</span> make_step(params, opt_state, X, Y):</span>
<span id="cb23-3"><a href="#cb23-3"></a>  loss_value, grads <span class="op">=</span> jax.value_and_grad(vectorized_residuals)(params, X, Y)</span>
<span id="cb23-4"><a href="#cb23-4"></a>  updates, opt_state <span class="op">=</span> optimizer.update(grads, opt_state, params)</span>
<span id="cb23-5"><a href="#cb23-5"></a>  params <span class="op">=</span> optax.apply_updates(params, updates)</span>
<span id="cb23-6"><a href="#cb23-6"></a>  <span class="cf">return</span> params, opt_state, loss_value</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="training-loop-version-1" class="slide level2">
<h2>Training Loop Version 1</h2>
<ul>
<li>Note that unlike Pytorch the gradients are passed as parameters</li>
</ul>
<div class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(num_epochs):</span>
<span id="cb24-2"><a href="#cb24-2"></a>    key, subkey <span class="op">=</span> random.split(key) <span class="co"># changing key for shuffling each epoch</span></span>
<span id="cb24-3"><a href="#cb24-3"></a>    train_loader <span class="op">=</span> data_loader(subkey, X, Y, batch_size)</span>
<span id="cb24-4"><a href="#cb24-4"></a>    <span class="cf">for</span> X_batch, Y_batch <span class="kw">in</span> train_loader:</span>
<span id="cb24-5"><a href="#cb24-5"></a>        params, opt_state, train_loss <span class="op">=</span> make_step(params, opt_state, X_batch, Y_batch)  </span>
<span id="cb24-6"><a href="#cb24-6"></a>    <span class="cf">if</span> epoch <span class="op">%</span> <span class="dv">100</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb24-7"><a href="#cb24-7"></a>        <span class="bu">print</span>(<span class="ss">f"Epoch </span><span class="sc">{</span>epoch<span class="sc">}</span><span class="ss">,||theta - theta_hat|| = </span><span class="sc">{</span>jnp<span class="sc">.</span>linalg<span class="sc">.</span>norm(theta <span class="op">-</span> params)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb24-8"><a href="#cb24-8"></a></span>
<span id="cb24-9"><a href="#cb24-9"></a><span class="bu">print</span>(<span class="ss">f"||theta - theta_hat|| = </span><span class="sc">{</span>jnp<span class="sc">.</span>linalg<span class="sc">.</span>norm(theta <span class="op">-</span> params)<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 0,||theta - theta_hat|| = 1.714521050453186
Epoch 100,||theta - theta_hat|| = 0.0020757941529154778
Epoch 200,||theta - theta_hat|| = 6.367397145368159e-05
||theta - theta_hat|| = 6.367397145368159e-05</code></pre>
</div>
</div>
</section>
<section id="auto-vectorizing" class="slide level2">
<h2>Auto-Vectorizing</h2>
<ul>
<li>In the above case the <code>vectorized_residuals</code> was able to use a directly vectorized function.</li>
<li>However in many cases it will be more convenient to write code for a single element of the finite-sum objectives</li>
<li>Now we will rewrite our objective to demonstrate how to use <code>vmap</code></li>
</ul>
<div class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1"></a><span class="kw">def</span> residual(theta, x, y):</span>
<span id="cb26-2"><a href="#cb26-2"></a>    y_hat <span class="op">=</span> predict(theta, x)</span>
<span id="cb26-3"><a href="#cb26-3"></a>    <span class="cf">return</span> (y_hat <span class="op">-</span> y) <span class="op">**</span> <span class="dv">2</span></span>
<span id="cb26-4"><a href="#cb26-4"></a></span>
<span id="cb26-5"><a href="#cb26-5"></a><span class="at">@jit</span></span>
<span id="cb26-6"><a href="#cb26-6"></a><span class="kw">def</span> residuals(theta, X, Y):</span>
<span id="cb26-7"><a href="#cb26-7"></a>    <span class="co"># Use vmap, fixing the 1st argument</span></span>
<span id="cb26-8"><a href="#cb26-8"></a>    batched_residuals <span class="op">=</span> jax.vmap(residual, in_axes<span class="op">=</span>(<span class="va">None</span>, <span class="dv">0</span>, <span class="dv">0</span>))</span>
<span id="cb26-9"><a href="#cb26-9"></a>    <span class="cf">return</span> jnp.mean(batched_residuals(theta, X, Y))</span>
<span id="cb26-10"><a href="#cb26-10"></a><span class="bu">print</span>(residual(theta_0, X[<span class="dv">0</span>], Y[<span class="dv">0</span>]))</span>
<span id="cb26-11"><a href="#cb26-11"></a><span class="bu">print</span>(residuals(theta_0, X, Y))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>0.021030858
3.546122</code></pre>
</div>
</div>
</section>
<section id="new-step-and-initialization" class="slide level2">
<h2>New Step and Initialization</h2>
<ul>
<li>This simply changes the function used for the <code>value_and_grad</code> call to use the new <code>residuals</code> function and resets our optimizer</li>
</ul>
<div class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1"></a><span class="at">@jax.jit</span></span>
<span id="cb28-2"><a href="#cb28-2"></a><span class="kw">def</span> make_step(params, opt_state, X, Y):     </span>
<span id="cb28-3"><a href="#cb28-3"></a>  loss_value, grads <span class="op">=</span> jax.value_and_grad(residuals)(params, X, Y)</span>
<span id="cb28-4"><a href="#cb28-4"></a>  updates, opt_state <span class="op">=</span> optimizer.update(grads, opt_state, params)</span>
<span id="cb28-5"><a href="#cb28-5"></a>  params <span class="op">=</span> optax.apply_updates(params, updates)</span>
<span id="cb28-6"><a href="#cb28-6"></a>  <span class="cf">return</span> params, opt_state, loss_value</span>
<span id="cb28-7"><a href="#cb28-7"></a>optimizer <span class="op">=</span> optax.sgd(lr) <span class="co"># better than optax.adam here</span></span>
<span id="cb28-8"><a href="#cb28-8"></a>opt_state <span class="op">=</span> optimizer.init(theta_0)</span>
<span id="cb28-9"><a href="#cb28-9"></a>params <span class="op">=</span> theta_0</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="training-loop-version-2" class="slide level2">
<h2>Training Loop Version 2</h2>
<ul>
<li>Otherwise the training loop is the same</li>
</ul>
<div class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(num_epochs):</span>
<span id="cb29-2"><a href="#cb29-2"></a>    key, subkey <span class="op">=</span> random.split(key) <span class="co"># changing key for shuffling each epoch</span></span>
<span id="cb29-3"><a href="#cb29-3"></a>    train_loader <span class="op">=</span> data_loader(subkey, X, Y, batch_size)</span>
<span id="cb29-4"><a href="#cb29-4"></a>    <span class="cf">for</span> X_batch, Y_batch <span class="kw">in</span> train_loader:</span>
<span id="cb29-5"><a href="#cb29-5"></a>        params, opt_state, train_loss <span class="op">=</span> make_step(params, opt_state, X_batch, Y_batch)  </span>
<span id="cb29-6"><a href="#cb29-6"></a>    <span class="cf">if</span> epoch <span class="op">%</span> <span class="dv">100</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb29-7"><a href="#cb29-7"></a>        <span class="bu">print</span>(<span class="ss">f"Epoch </span><span class="sc">{</span>epoch<span class="sc">}</span><span class="ss">,||theta - theta_hat|| = </span><span class="sc">{</span>jnp<span class="sc">.</span>linalg<span class="sc">.</span>norm(theta <span class="op">-</span> params)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb29-8"><a href="#cb29-8"></a></span>
<span id="cb29-9"><a href="#cb29-9"></a><span class="bu">print</span>(<span class="ss">f"||theta - theta_hat|| = </span><span class="sc">{</span>jnp<span class="sc">.</span>linalg<span class="sc">.</span>norm(theta <span class="op">-</span> params)<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 0,||theta - theta_hat|| = 1.7061582803726196
Epoch 100,||theta - theta_hat|| = 0.002027335576713085
Epoch 200,||theta - theta_hat|| = 6.216356268851086e-05
||theta - theta_hat|| = 6.216356268851086e-05</code></pre>
</div>
</div>
</section>
<section id="jax-examples" class="slide level2">
<h2>JAX Examples</h2>
<ul>
<li>See <a href="examples/linear_regression_jax_sgd.py">examples/linear_regression_jax_sgd.py</a>
<ul>
<li>This implements the inline code above without the vmap</li>
</ul></li>
<li>See <a href="examples/linear_regression_jax_vmap.py">examples/linear_regression_jax_vmap.py</a>
<ul>
<li>This implements the <code>vmap</code> as above</li>
<li>This also adds in an <a href="https://optax.readthedocs.io/en/latest/api.html#optimizer-schedules">learning rate schedule</a></li>
</ul></li>
<li>See <a href="examples/linear_regression_jax_equinox.py">examples/linear_regression_jax_equinox.py</a>
<ul>
<li>See the following example for more</li>
</ul></li>
</ul>
</section></section>
<section>
<section id="jax-linear-regression-with-equinox" class="title-slide slide level1 center">
<h1>JAX Linear Regression with Equinox</h1>

</section>
<section id="using-an-equinox-model" class="slide level2">
<h2>Using an Equinox Model</h2>
<ul>
<li>While it seems convenient to work in a functional style, when we move towards nested, deep approximations it can become cumbersome</li>
<li>An excellent package to both define hypothesis classes and to work with their gradients is <a href="https://github.com/patrick-kidger/equinox">equinox</a></li>
<li>Key to the design of equinox is that if we use that the nested parameters of an approximation class are PyTrees, then we can find the derivative with respect to that entire type. See our previous example on differentiating PyTrees</li>
</ul>
</section>
<section id="equinox-macros" class="slide level2">
<h2>Equinox Macros</h2>
<ul>
<li>There are several macros in equinox which make it easier to work with differentiable PyTrees, all of which have the name <code>filter_</code>
<ul>
<li>Loosely: these macros go through the underlying python datastructures and filter out values into static values vs.&nbsp;ones which could be perturbed</li>
<li>For example, if I have a type with a <code>jnp.array</code> for the differentiable weights and a <code>int</code> which stores the number of hidden dimensions, then the <code>filter_grad</code> will flag the <code>jnp.array</code> as differentiable and the <code>int</code> will have the gradient type as <code>None</code></li>
</ul></li>
<li>When in doubt, always replace them. e.g.&nbsp;<code>@jax.jit</code> can be replied with <code>@equinox.filter_jit</code>, <code>jax.vmap</code> should be replaced with <code>eqx.filter_vmap</code>, etc.</li>
</ul>
</section>
<section id="hypothesis-class-2" class="slide level2">
<h2>Hypothesis Class</h2>
<ul>
<li>We are moving towards Neural Networks, which are a very broad class of approximations.</li>
<li>Here lets just use a linear approximation with no constant term</li>
<li>As always, the initial randomization will become increasingly important</li>
</ul>
<div class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1"></a>key, subkey <span class="op">=</span> random.split(key)</span>
<span id="cb31-2"><a href="#cb31-2"></a>model <span class="op">=</span> eqx.nn.Linear(M, <span class="dv">1</span>, use_bias <span class="op">=</span> <span class="va">False</span>, key <span class="op">=</span> subkey)</span>
<span id="cb31-3"><a href="#cb31-3"></a><span class="bu">print</span>(model.weight)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[[-0.01143495  0.6449629 ]]</code></pre>
</div>
</div>
</section>
<section id="residuals-using-the-model" class="slide level2">
<h2>Residuals using the Model</h2>
<ul>
<li>The model now contains all of the, potentially nested, parameters for the approximation class</li>
<li>It provides call notation to evaluate the function with those parameters</li>
</ul>
<div class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1"></a><span class="kw">def</span> residual(model, x, y):</span>
<span id="cb33-2"><a href="#cb33-2"></a>    y_hat <span class="op">=</span> model(x)</span>
<span id="cb33-3"><a href="#cb33-3"></a>    <span class="cf">return</span> (y_hat <span class="op">-</span> y) <span class="op">**</span> <span class="dv">2</span></span>
<span id="cb33-4"><a href="#cb33-4"></a></span>
<span id="cb33-5"><a href="#cb33-5"></a><span class="kw">def</span> residuals(model, X, Y):</span>
<span id="cb33-6"><a href="#cb33-6"></a>    batched_residuals <span class="op">=</span> vmap(residual, in_axes<span class="op">=</span>(<span class="va">None</span>, <span class="dv">0</span>, <span class="dv">0</span>))</span>
<span id="cb33-7"><a href="#cb33-7"></a>    <span class="cf">return</span> jnp.mean(batched_residuals(model, X, Y))</span>
<span id="cb33-8"><a href="#cb33-8"></a><span class="co"># Alternatively, could have combined and still jit/grad</span></span>
<span id="cb33-9"><a href="#cb33-9"></a><span class="kw">def</span> residuals_2(model, X, Y):</span>
<span id="cb33-10"><a href="#cb33-10"></a>    Y_hat <span class="op">=</span> vmap(model)(X).squeeze()</span>
<span id="cb33-11"><a href="#cb33-11"></a>    <span class="cf">return</span> jnp.mean((Y <span class="op">-</span> Y_hat) <span class="op">**</span> <span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="gradients-of-models" class="slide level2">
<h2>Gradients of Models</h2>
<ul>
<li>As discussed, we can find the gradients of richer objects than just arrays</li>
<li>Optimizer updates use perturbations of the underlying PyTree</li>
<li>Updates can be applied because the type of the gradients matches the underlying PyTree</li>
</ul>
<div class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1"></a>grads <span class="op">=</span> jax.grad(residuals)(model, X, Y)</span>
<span id="cb34-2"><a href="#cb34-2"></a><span class="bu">print</span>(grads)</span>
<span id="cb34-3"><a href="#cb34-3"></a><span class="bu">print</span>(grads.weight)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Linear(
  weight=f32[1,2],
  bias=None,
  in_features=2,
  out_features=1,
  use_bias=False
)
[[-0.30091333 -0.31419277]]</code></pre>
</div>
</div>
</section>
<section id="setup-of-the-optimizer-and-make_step" class="slide level2">
<h2>Setup of the Optimizer and <code>make_step</code></h2>
<ul>
<li>The <code>make_step</code> isn’t very different, except for using a few <code>equinox</code> utility functions</li>
</ul>
<div class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1"></a>lr <span class="op">=</span> <span class="fl">0.001</span></span>
<span id="cb36-2"><a href="#cb36-2"></a>optimizer <span class="op">=</span> optax.sgd(lr)</span>
<span id="cb36-3"><a href="#cb36-3"></a><span class="co"># Needs to remove the non-differentiable parts of the "model" object</span></span>
<span id="cb36-4"><a href="#cb36-4"></a>opt_state <span class="op">=</span> optimizer.init(eqx.<span class="bu">filter</span>(model,eqx.is_inexact_array))</span>
<span id="cb36-5"><a href="#cb36-5"></a></span>
<span id="cb36-6"><a href="#cb36-6"></a><span class="at">@eqx.filter_jit</span></span>
<span id="cb36-7"><a href="#cb36-7"></a><span class="kw">def</span> make_step(model, opt_state, X, Y):     </span>
<span id="cb36-8"><a href="#cb36-8"></a>  loss_value, grads <span class="op">=</span> eqx.filter_value_and_grad(residuals)(model, X, Y)</span>
<span id="cb36-9"><a href="#cb36-9"></a>  updates, opt_state <span class="op">=</span> optimizer.update(grads, opt_state, model)</span>
<span id="cb36-10"><a href="#cb36-10"></a>  model <span class="op">=</span> eqx.apply_updates(model, updates)</span>
<span id="cb36-11"><a href="#cb36-11"></a>  <span class="cf">return</span> model, opt_state, loss_value</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="training-loop-1" class="slide level2">
<h2>Training Loop</h2>
<div class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1"></a>num_epochs <span class="op">=</span> <span class="dv">300</span></span>
<span id="cb37-2"><a href="#cb37-2"></a>batch_size <span class="op">=</span> <span class="dv">64</span></span>
<span id="cb37-3"><a href="#cb37-3"></a>key, subkey <span class="op">=</span> random.split(key) <span class="co"># will keep same key for shuffling each epoch</span></span>
<span id="cb37-4"><a href="#cb37-4"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(num_epochs):</span>
<span id="cb37-5"><a href="#cb37-5"></a>    key, subkey <span class="op">=</span> random.split(key) <span class="co"># changing key for shuffling each epoch</span></span>
<span id="cb37-6"><a href="#cb37-6"></a>    train_loader <span class="op">=</span> data_loader(subkey, X, Y, batch_size)</span>
<span id="cb37-7"><a href="#cb37-7"></a>    <span class="cf">for</span> X_batch, Y_batch <span class="kw">in</span> train_loader:</span>
<span id="cb37-8"><a href="#cb37-8"></a>        model, opt_state, train_loss <span class="op">=</span> make_step(model, opt_state, X_batch, Y_batch)    </span>
<span id="cb37-9"><a href="#cb37-9"></a>    <span class="cf">if</span> epoch <span class="op">%</span> <span class="dv">100</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb37-10"><a href="#cb37-10"></a>        <span class="bu">print</span>(<span class="ss">f"Epoch </span><span class="sc">{</span>epoch<span class="sc">}</span><span class="ss">,||theta - theta_hat|| = </span><span class="sc">{</span>jnp<span class="sc">.</span>linalg<span class="sc">.</span>norm(theta <span class="op">-</span> model.weight)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb37-11"><a href="#cb37-11"></a></span>
<span id="cb37-12"><a href="#cb37-12"></a><span class="bu">print</span>(<span class="ss">f"||theta - theta_hat|| = </span><span class="sc">{</span>jnp<span class="sc">.</span>linalg<span class="sc">.</span>norm(theta <span class="op">-</span> model.weight)<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 0,||theta - theta_hat|| = 0.20521600544452667
Epoch 100,||theta - theta_hat|| = 0.038740210235118866
Epoch 200,||theta - theta_hat|| = 0.0073235719464719296
||theta - theta_hat|| = 0.0013871045084670186</code></pre>
</div>
</div>
</section>
<section id="custom-types" class="slide level2">
<h2>Custom Types</h2>
<ul>
<li>To prepare for layers of NN, we see that this class could have been written manually, layering as we see fit</li>
<li>See <a href="examples/linear_regression_jax_equinox.py">examples/linear_regression_jax_equinox.py</a> for more, but we could have manually created the following</li>
</ul>
<div class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1"></a><span class="kw">class</span> MyLinear(eqx.Module):</span>
<span id="cb39-2"><a href="#cb39-2"></a>    weight: jax.Array</span>
<span id="cb39-3"><a href="#cb39-3"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, in_size, out_size, key):</span>
<span id="cb39-4"><a href="#cb39-4"></a>        <span class="va">self</span>.weight <span class="op">=</span> jax.random.normal(key, (out_size, in_size))</span>
<span id="cb39-5"><a href="#cb39-5"></a>    <span class="co"># Similar to Pytorch's forward</span></span>
<span id="cb39-6"><a href="#cb39-6"></a>    <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, x):</span>
<span id="cb39-7"><a href="#cb39-7"></a>        <span class="cf">return</span> <span class="va">self</span>.weight <span class="op">@</span> x</span>
<span id="cb39-8"><a href="#cb39-8"></a></span>
<span id="cb39-9"><a href="#cb39-9"></a>model <span class="op">=</span> MyLinear(M, <span class="dv">1</span>, key <span class="op">=</span> subkey)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>

<div class="footer footer-default">

</div>
</section></section>
    </div>
  </div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="../site_libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="../site_libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-chalkboard/plugin.js"></script>
  <script src="../site_libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="../site_libs/revealjs/plugin/notes/notes.js"></script>
  <script src="../site_libs/revealjs/plugin/search/search.js"></script>
  <script src="../site_libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="../site_libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': false,
'smaller': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleChalkboard(event)\"><kbd>b</kbd> Toggle Chalkboard</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleNotesCanvas(event)\"><kbd>c</kbd> Toggle Notes Canvas</a></li>\n<li class=\"slide-tool-item\" data-item=\"6\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.downloadDrawings(event)\"><kbd>d</kbd> Download Drawings</a></li>\n<li class=\"slide-tool-item\" data-item=\"7\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'chalkboard': {"buttons":true},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: false,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1280,

        height: 720,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, RevealChalkboard, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
    window.document.addEventListener("DOMContentLoaded", function (event) {
      const toggleBodyColorMode = (bsSheetEl) => {
        const mode = bsSheetEl.getAttribute("data-mode");
        const bodyEl = window.document.querySelector("body");
        if (mode === "dark") {
          bodyEl.classList.add("quarto-dark");
          bodyEl.classList.remove("quarto-light");
        } else {
          bodyEl.classList.add("quarto-light");
          bodyEl.classList.remove("quarto-dark");
        }
      }
      const toggleBodyColorPrimary = () => {
        const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
        if (bsSheetEl) {
          toggleBodyColorMode(bsSheetEl);
        }
      }
      toggleBodyColorPrimary();  
      const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
      tabsets.forEach(function(tabset) {
        const tabby = new Tabby('#' + tabset.id);
      });
      const isCodeAnnotation = (el) => {
        for (const clz of el.classList) {
          if (clz.startsWith('code-annotation-')) {                     
            return true;
          }
        }
        return false;
      }
      const clipboard = new window.ClipboardJS('.code-copy-button', {
        text: function(trigger) {
          const codeEl = trigger.previousElementSibling.cloneNode(true);
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
        }
      });
      clipboard.on('success', function(e) {
        // button target
        const button = e.trigger;
        // don't keep focus
        button.blur();
        // flash "checked"
        button.classList.add('code-copy-button-checked');
        var currentTitle = button.getAttribute("title");
        button.setAttribute("title", "Copied!");
        let tooltip;
        if (window.bootstrap) {
          button.setAttribute("data-bs-toggle", "tooltip");
          button.setAttribute("data-bs-placement", "left");
          button.setAttribute("data-bs-title", "Copied!");
          tooltip = new bootstrap.Tooltip(button, 
            { trigger: "manual", 
              customClass: "code-copy-button-tooltip",
              offset: [0, -8]});
          tooltip.show();    
        }
        setTimeout(function() {
          if (tooltip) {
            tooltip.hide();
            button.removeAttribute("data-bs-title");
            button.removeAttribute("data-bs-toggle");
            button.removeAttribute("data-bs-placement");
          }
          button.setAttribute("title", currentTitle);
          button.classList.remove('code-copy-button-checked');
        }, 1000);
        // clear code selection
        e.clearSelection();
      });
      function tippyHover(el, contentFn) {
        const config = {
          allowHTML: true,
          content: contentFn,
          maxWidth: 500,
          delay: 100,
          arrow: false,
          appendTo: function(el) {
              return el.closest('section.slide') || el.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'light-border',
          placement: 'bottom-start'
        };
          config['offset'] = [0,0];
          config['maxWidth'] = 700;
        window.tippy(el, config); 
      }
      const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
      for (var i=0; i<noterefs.length; i++) {
        const ref = noterefs[i];
        tippyHover(ref, function() {
          // use id or data attribute instead here
          let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
          try { href = new URL(href).hash; } catch {}
          const id = href.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          return note.innerHTML;
        });
      }
      const findCites = (el) => {
        const parentEl = el.parentElement;
        if (parentEl) {
          const cites = parentEl.dataset.cites;
          if (cites) {
            return {
              el,
              cites: cites.split(' ')
            };
          } else {
            return findCites(el.parentElement)
          }
        } else {
          return undefined;
        }
      };
      var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
      for (var i=0; i<bibliorefs.length; i++) {
        const ref = bibliorefs[i];
        const citeInfo = findCites(ref);
        if (citeInfo) {
          tippyHover(citeInfo.el, function() {
            var popup = window.document.createElement('div');
            citeInfo.cites.forEach(function(cite) {
              var citeDiv = window.document.createElement('div');
              citeDiv.classList.add('hanging-indent');
              citeDiv.classList.add('csl-entry');
              var biblioDiv = window.document.getElementById('ref-' + cite);
              if (biblioDiv) {
                citeDiv.innerHTML = biblioDiv.innerHTML;
              }
              popup.appendChild(citeDiv);
            });
            return popup.innerHTML;
          });
        }
      }
    });
    </script>
    

</body></html>