{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ECON622: Computational Economics with Data Science Applications\n",
        "\n",
        "Direct Methods and Matrix Factorizations\n",
        "\n",
        "Jesse Perla (University of British Columbia)\n",
        "\n",
        "# Overview\n",
        "\n",
        "## Motivation\n",
        "\n",
        "-   In preparation for the ML lectures we cover some core numerical\n",
        "    linear algebra concepts\n",
        "-   Many of these are directly useful\n",
        "    -   e.g. solving large systems of equations or as building blocks in\n",
        "        bigger algorithms\n",
        "\n",
        "## Packages and Materials\n",
        "\n",
        "-   See [QuantEcon Numerical Linear\n",
        "    Algebra](https://julia.quantecon.org/tools_and_techniques/numerical_linear_algebra.html)\n",
        "    and associated notebooks"
      ],
      "id": "73bd79ac-e898-4344-9947-69862b8a3eed"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "using LinearAlgebra, Statistics, BenchmarkTools, SparseArrays, Random\n",
        "using Plots\n",
        "Random.seed!(42);  # seed random numbers for reproducibility"
      ],
      "id": "e5186e0e"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Complexity\n",
        "\n",
        "## Basic Computational Complexity\n",
        "\n",
        "**Big-O Notation**\n",
        "\n",
        "For a function $f(N)$ and a positive constant $C$, we say $f(N)$ is\n",
        "$O(g(N))$, if there exist positive constants $C$ and $N_0$ such that:\n",
        "\n",
        "$$\n",
        "0 \\leq f(N) \\leq C \\cdot g(N) \\quad \\text{for all } N \\geq N_0\n",
        "$$\n",
        "\n",
        "-   Often crucial to know how problems scale asymptotically (as\n",
        "    $N\\to\\infty$)\n",
        "-   Caution! This is only an asymptotic limit, and can be misleading for\n",
        "    small $N$\n",
        "    -   $f_1(N) = N^3 + N$ is $O(N^3)$\n",
        "    -   $f_2(N) = 1000 N^2 + 3 N$ is $O(N^2)$\n",
        "    -   For roughly $N>1000$ use $f_2$ algorithm, otherwise $f_1$\n",
        "\n",
        "## Examples of Computational Complexity\n",
        "\n",
        "-   Simple examples:\n",
        "    -   $x \\cdot y = \\sum_{n=1}^N x_n y_n$ is $O(N)$ since it requires\n",
        "        $N$ multiplications and additions\n",
        "    -   $A x$ for $A\\in\\mathbb{R}^{N\\times N},x\\in\\mathbb{R}^N$ is\n",
        "        $O(N^2)$ since it requires $N$ dot products, each $O(N)$\n",
        "\n",
        "## Computational Complexity\n",
        "\n",
        "Ask yourself whether the following is a **computationally expensive**\n",
        "operation as the matrix **size increases**\n",
        "\n",
        "-   Multiplying two matrices?\n",
        "    -   *Answer*: It depends. Multiplying two diagonal matrices is\n",
        "        trivial.\n",
        "-   Solving a linear system of equations?\n",
        "    -   *Answer*: It depends. If the matrix is the identity, the\n",
        "        solution is the vector itself.\n",
        "-   Finding the eigenvalues of a matrix?\n",
        "    -   *Answer*: It depends. The eigenvalues of a triangular matrix are\n",
        "        the diagonal elements.\n",
        "\n",
        "## Numerical Precision\n",
        "\n",
        "**Machine Epsilon**\n",
        "\n",
        "For a given datatype, $\\epsilon$ is defined as\n",
        "$\\epsilon = \\min_{\\delta > 0} \\left\\{ \\delta : 1 + \\delta > 1 \\right\\}$\n",
        "\n",
        "-   Computers have finite precision. 64-bit typical, but 32-bit on GPUs"
      ],
      "id": "53df70b9-2ef5-43d7-a97c-ab4b1c1c2388"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "machine epsilon for float64 = 2.220446049250313e-16\n",
            "1 + eps/2 == 1? true\n",
            "machine epsilon for float32 = 1.1920929e-7"
          ]
        }
      ],
      "source": [
        "println(\"machine epsilon for float64 = $(eps())\")\n",
        "println(\"1 + eps/2 == 1? $(1.0 + 1.1e-16 == 1.0)\")\n",
        "println(\"machine epsilon for float32 = $(eps(Float32))\")"
      ],
      "id": "18897037"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Matrix Structure\n",
        "\n",
        "## Matrix Structure\n",
        "\n",
        "-   A key principle is to ensure you don’t lose “structure”\n",
        "    -   e.g. if sparse, operations should keep it sparse if possible\n",
        "    -   If triangular, then use appropriate algorithms instead of\n",
        "        converting back to a dense matrix\n",
        "-   Key structure is:\n",
        "    -   Symmetry, diagonal, tridiagonal, banded, sparse,\n",
        "        positive-definite\n",
        "-   The worse operations for losing structure are matrix multiplication\n",
        "    and inversion\n",
        "\n",
        "## Example Losing Sparsity\n",
        "\n",
        "-   Here the density increases substantially"
      ],
      "id": "27177e91-5d0b-46ab-8f98-f1cd57ffc93b"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nnz(A) = 46\n",
            "nnz(invA) = 100"
          ]
        }
      ],
      "source": [
        "A = sprand(10, 10, 0.45)  # random sparse 10x10, 45 percent filled with non-zeros\n",
        "\n",
        "@show nnz(A)  # counts the number of non-zeros\n",
        "invA = sparse(inv(Array(A)))  # Julia won't invert sparse, so convert to dense with Array.\n",
        "@show nnz(invA);"
      ],
      "id": "d6d1df18"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Losing Tridiagonal Structure\n",
        "\n",
        "-   An even more extreme example. Tridiagonal has roughly $3N$ nonzeros.\n",
        "    Inverses are dense $N^2$"
      ],
      "id": "7a86e729-254a-498b-8764-c53db7e10304"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "5×5 Matrix{Float64}:\n",
              "  1.29099      -0.327957     0.0416667  -0.00537634   0.000672043\n",
              " -0.163978      1.31183     -0.166667    0.0215054   -0.00268817\n",
              "  0.0208333    -0.166667     1.29167    -0.166667     0.0208333\n",
              " -0.00268817    0.0215054   -0.166667    1.31183     -0.163978\n",
              "  0.000672043  -0.00537634   0.0416667  -0.327957     1.29099"
            ]
          }
        }
      ],
      "source": [
        "N = 5\n",
        "A = Tridiagonal([fill(0.1, N - 2); 0.2], fill(0.8, N), [0.2; fill(0.1, N - 2)])\n",
        "inv(A)"
      ],
      "id": "a8b219be"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Forming the Covariance and/or Gram Matrix\n",
        "\n",
        "-   Another common example is $A' A$"
      ],
      "id": "0bcd0b50-7e60-4f30-8949-5f542d7e47e2"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nnz(A) / 20 ^ 2 = 0.34\n",
            "nnz(A' * A) / 21 ^ 2 = 0.9229024943310657"
          ]
        }
      ],
      "source": [
        "A = sprand(20, 21, 0.3)\n",
        "@show nnz(A) / 20^2\n",
        "@show nnz(A' * A) / 21^2;"
      ],
      "id": "81d19e8d"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Specialized Algorithms\n",
        "\n",
        "Besides sparsity/storage, the real loss is you miss out on algorithms.\n",
        "For example, lets setup the benchmarking code"
      ],
      "id": "ca85603e-c150-4884-b84f-78c223e4eaf3"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "benchmark_solve (generic function with 1 method)"
            ]
          }
        }
      ],
      "source": [
        "using BenchmarkTools\n",
        "function benchmark_solve(A, b)\n",
        "    println(\"A\\\\b for typeof(A) = $(string(typeof(A)))\")\n",
        "    @btime $A \\ $b\n",
        "end"
      ],
      "id": "c0af6fdf"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compare Dense vs. Sparse vs. Tridiagonal"
      ],
      "id": "231f2db2-4fda-468e-a352-0a1eb8db64c2"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A\\b for typeof(A) = Tridiagonal{Float64, Vector{Float64}}\n",
            "  18.100 μs (8 allocations: 47.72 KiB)\n",
            "A\\b for typeof(A) = SparseMatrixCSC{Float64, Int64}\n",
            "  501.800 μs (79 allocations: 1.03 MiB)\n",
            "A\\b for typeof(A) = Matrix{Float64}\n",
            "  12.282 ms (4 allocations: 7.64 MiB)"
          ]
        }
      ],
      "source": [
        "N = 1000\n",
        "b = rand(N)\n",
        "A = Tridiagonal([fill(0.1, N - 2); 0.2], fill(0.8, N), [0.2; fill(0.1, N - 2)])\n",
        "A_sparse = sparse(A)  # sparse but losing tridiagonal structure\n",
        "A_dense = Array(A)    # dropping the sparsity structure, dense 1000x1000\n",
        "\n",
        "# benchmark solution to system A x = b\n",
        "benchmark_solve(A, b)\n",
        "benchmark_solve(A_sparse, b)\n",
        "benchmark_solve(A_dense, b);"
      ],
      "id": "a717e8ed"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Triangular Matrices and Back/Forward Substitution\n",
        "\n",
        "-   A key example of a better algorithm is for triangular matrices\n",
        "-   Upper or lower triangular matrices can be solved in $O(N^2)$ instead\n",
        "    of $O(N^3)$"
      ],
      "id": "eddd74a3-9193-4d5c-9b0f-3d4a8cb5981a"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "3-element Vector{Float64}:\n",
              " 0.0\n",
              " 0.0\n",
              " 0.3333333333333333"
            ]
          }
        }
      ],
      "source": [
        "b = [1.0, 2.0, 3.0]\n",
        "U = UpperTriangular([1.0 2.0 3.0;\n",
        "                     0.0 5.0 6.0;\n",
        "                     0.0 0.0 9.0])\n",
        "U \\ b"
      ],
      "id": "96e288ab"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Backwards Substitution Example\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "U x &= b\\\\\n",
        "U &\\equiv \\begin{bmatrix}\n",
        "3 & 1 \\\\\n",
        "0 & 2 \\\\\n",
        "\\end{bmatrix}, \\quad b = \\begin{bmatrix}\n",
        "7 \\\\\n",
        "2 \\\\\n",
        "\\end{bmatrix}\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "Solving bottom row for $x_2$\n",
        "\n",
        "$$\n",
        "2 x_2 = 2,\\quad x_2 = 1\n",
        "$$\n",
        "\n",
        "Move up a row, solving for $x_1$, substituting for $x_2$\n",
        "\n",
        "$$\n",
        "3 x_1 + 1 x_2 = 7,\\quad 3 x_1 + 1 \\times 1 =  7,\\quad x_1 = 2\n",
        "$$\n",
        "\n",
        "Generalizes to many rows. For $L$ it is “forward substitution”\n",
        "\n",
        "# Factorizations\n",
        "\n",
        "## Factorizing matrices\n",
        "\n",
        "-   Just like you can factor $6 = 2 \\cdot 3$, you can factor matrices\n",
        "-   Unlike integers, you have more choice over the properties of the\n",
        "    factors\n",
        "-   Many operations (e.g., solving systems of equations, finding\n",
        "    eigenvalues, inverting, finding determinants) have a factorization\n",
        "    done internally\n",
        "    -   Instead you can often just find the factorization and reuse it\n",
        "-   Key factorizations: LU, QR, Cholesky, SVD, Schur, Eigenvalue\n",
        "\n",
        "## LU(P) Decompositions\n",
        "\n",
        "-   We can “factor” any square $A$ into $P A = L U$ for triangular $L$\n",
        "    and $U$. Invertible can have $A = L U$, called the LU decomposition.\n",
        "    “P” is for partial-pivoting\n",
        "-   Singular matrices may not have full-rank $L$ or $U$ matrices"
      ],
      "id": "e52f14b5-f760-4e36-9155-6f3946d4b014"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "true"
            ]
          }
        }
      ],
      "source": [
        "N = 4\n",
        "A = rand(N, N)\n",
        "b = rand(N)\n",
        "# chooses the right factorization based on matrix structure\n",
        "# LU here\n",
        "Af = factorize(A)\n",
        "Af.P * A ≈ Af.L * Af.U"
      ],
      "id": "1a52618b"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Using a Factorization\n",
        "\n",
        "-   In Julia the factorization objects typically overload the `\\` and\n",
        "    functions such as `inv`"
      ],
      "id": "411af91f-0913-41a0-938e-b33b966ad058"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Af \\ b = [1.567631093835083, -1.8670423474177864, -0.7020922312927874, 1.0653095651070625]\n",
            "inv(Af) * b = [1.5676310938350828, -1.8670423474177873, -0.7020922312927873, 1.0653095651070625]"
          ]
        },
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "4-element Vector{Float64}:\n",
              "  1.5676310938350828\n",
              " -1.8670423474177873\n",
              " -0.7020922312927873\n",
              "  1.0653095651070625"
            ]
          }
        }
      ],
      "source": [
        "@show Af \\ b\n",
        "@show inv(Af) * b"
      ],
      "id": "4c0b7d8c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## LU Decompositions and Systems of Equations\n",
        "\n",
        "-   Pivoting is typically implied when talking about “LU”\n",
        "-   Used in the default `solve` algorithm (without more structure)\n",
        "-   Solving systems of equations with triangular matrices: for\n",
        "    $A x = L U x = b$\n",
        "    1.  Define $y = U x$\n",
        "    2.  Solve $L y = b$ for $y$ and $U x = y$ for $x$\n",
        "-   Since both are triangular, process is $O(N^2)$ (but LU itself\n",
        "    $O(N^3)$)\n",
        "-   Could be used to find `inv`\n",
        "    -   $A = L U$ then $A A^{-1} = I = L U A^{-1} = I$\n",
        "    -   Solve for $Y$ in $L Y = I$, then solve $U A^{-1} = Y$\n",
        "-   Tight connection to textbook Gaussian elimination (including\n",
        "    pivoting)\n",
        "\n",
        "## Cholesky\n",
        "\n",
        "-   LU is for general invertible matrices, but it doesn’t use\n",
        "    positive-definiteness or symmetry\n",
        "-   The Cholesky is the right factorization for general\n",
        "    positive-definite matrices. For general symmetric matrices you can\n",
        "    use Bunch-Kaufman or others\n",
        "-   $A = L L'$ for lower triangular $L$ or equivalent for upper\n",
        "    triangular"
      ],
      "id": "8b6db3a0-3e92-431f-ae9d-efbb874f309b"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A is symmetric? true"
          ]
        }
      ],
      "source": [
        "N = 500\n",
        "B = rand(N, N)\n",
        "A_dense = B' * B  # an easy way to generate a symmetric positive semi-definite matrix\n",
        "A = Symmetric(A_dense)  # flags the matrix as symmetric\n",
        "println(\"A is symmetric? $(issymmetric(A))\")"
      ],
      "id": "81da9c28"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Comparing Cholesky\n",
        "\n",
        "-   By default it doesn’t know the matrix is positive-definite, so\n",
        "    `factorize` is the best it can do given symmetric structure"
      ],
      "id": "18857b65-4f8f-4aa9-8adc-7abdf020aa0d"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A\\b for typeof(A) = Symmetric{Float64, Matrix{Float64}}\n",
            "  14.591 ms (6 allocations: 2.16 MiB)\n",
            "A\\b for typeof(A) = Matrix{Float64}\n",
            "  5.612 ms (4 allocations: 1.92 MiB)\n",
            "  3.185 ms (3 allocations: 1.91 MiB)"
          ]
        }
      ],
      "source": [
        "b = rand(N)\n",
        "factorize(A) |> typeof\n",
        "cholesky(A) \\ b  # use the factorization to solve\n",
        "\n",
        "benchmark_solve(A, b)\n",
        "benchmark_solve(A_dense, b)\n",
        "@btime cholesky($A, check = false) \\ $b;"
      ],
      "id": "80c1277a"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Eigen Decomposition\n",
        "\n",
        "-   For square, symmetric, non-singular matrix $A$ factor into\n",
        "\n",
        "$$\n",
        "A = Q \\Lambda Q^{-1}\n",
        "$$\n",
        "\n",
        "-   $Q$ is a matrix of eigenvectors, $\\Lambda$ is a diagonal matrix of\n",
        "    paired eigenvalues\n",
        "-   For symmetric matrices, the eigenvectors are orthogonal and\n",
        "    $Q^{-1} Q = Q^T Q = I$ which form an orthonormal basis\n",
        "-   Orthogonal matrices can be thought of as rotations without\n",
        "    stretching\n",
        "-   More general matrices all have a Singular Value Decomposition (SVD)\n",
        "-   With symmetric $A$, an interpretation of $A x$ is that we can first\n",
        "    rotate $x$ into the $Q$ basis, then stretch by $\\Lambda$, then\n",
        "    rotate back\n",
        "\n",
        "## Calculating the Eigen Decomposition"
      ],
      "id": "dd8bd0f4-be80-4a60-acd2-0ea8f7da632a"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "norm(Q * Λ * inv(Q) - A) = 4.345742334937357e-15\n",
            "norm(Q * Λ * Q' - A) = 3.0218863035051596e-15"
          ]
        }
      ],
      "source": [
        "A = Symmetric(rand(5, 5))  # symmetric matrices have real eigenvectors/eigenvalues\n",
        "A_eig = eigen(A)\n",
        "Λ = Diagonal(A_eig.values)\n",
        "Q = A_eig.vectors\n",
        "@show norm(Q * Λ * inv(Q) - A)\n",
        "@show norm(Q * Λ * Q' - A);"
      ],
      "id": "d5206990"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Eigendecompositions and Matrix Powers\n",
        "\n",
        "-   Can be used to find $A^t$ for large $t$ (e.g. for Markov chains)\n",
        "    -   $P^t$, i.e. $P \\cdot P \\cdot \\ldots \\cdot P$ for $t$ times\n",
        "    -   $P = Q \\Lambda Q^{-1}$ then $P^t = Q \\Lambda^t Q^{-1}$ where\n",
        "        $\\Lambda^t$ is just the pointwise power\n",
        "-   Related can find matrix exponential $e^A$ for square matrices\n",
        "    -   $e^A = Q e^\\Lambda Q^{-1}$ where $e^\\Lambda$ is just the\n",
        "        pointwise exponential\n",
        "    -   Useful for solving differential equations, e.g. $y' = A y$ for\n",
        "        $y(0) = y_0$ is $y(t) = e^{A t} y_0$\n",
        "\n",
        "## More on Factorizations\n",
        "\n",
        "-   Plenty more used in different circumstances. Start by looking at\n",
        "    structure\n",
        "-   Usually have some connection to textbook algorithms, for example LU\n",
        "    is Gaussian elimination with pivoting and QR is Gram-Schmidt Process\n",
        "-   Just as shortcuts can be done with sparse matrices in textbook\n",
        "    examples, direct sparse methods can be faster given enough sparsity\n",
        "    -   But don’t assume sparsity will be faster. Often slower unless\n",
        "        matrices are big and especially sparse\n",
        "    -   Dense algorithms on GPUs can be very fast because of parallelism\n",
        "-   Keep in mind that barring numerical roundoff issues, these are\n",
        "    “exact” methods. They don’t become more accurate with more\n",
        "    iterations\n",
        "\n",
        "## Large Scale Systems of Equations\n",
        "\n",
        "-   Packages that solve BIG problems with “direct methods” include\n",
        "    [MUMPS](https://mumps-solver.org/index.php),\n",
        "    [Paradiso](https://www.intel.com/content/www/us/en/docs/onemkl/developer-reference-c/2023-0/onemkl-pardiso-parallel-direct-sparse-solver-iface.html),\n",
        "    [UMFPACK](https://en.wikipedia.org/wiki/UMFPACK), and many others\n",
        "-   Sparse solvers are bread-and-butter scientific computing, so they\n",
        "    can crush huge problems, parallelize on a cluster, etc.\n",
        "-   But for smaller problems they may not be ideal. Profile and test,\n",
        "    and only if you need it.\n",
        "-   In Julia, the SciML package\n",
        "    [LinearSolver.jl](https://github.com/SciML/LinearSolve.jl) is your\n",
        "    best bet there, as it lets you swap out backends to profile\n",
        "-   On Python harder to flip between them, but scipy has many built in\n",
        "    and many wrappers exist. Same with Matlab\n",
        "\n",
        "## Preview of Conditioning\n",
        "\n",
        "-   It will turn out that for iterative methods, a different style of\n",
        "    algorithm, it is often necessary to multiple by a matrix to\n",
        "    transform the problem\n",
        "-   The ideal transform would be the matrix’s inverse, which requires a\n",
        "    full factorization.\n",
        "-   But instead, you can do only part of the way towards the\n",
        "    factorization. e.g., part of the way on gaussian elimination\n",
        "-   Called “Incomplete Cholesky”, “Incomplete LU”, etc.\n",
        "\n",
        "# Continuous Time Markov Chains\n",
        "\n",
        "## Markov Chains Transitions in in Continuous Time\n",
        "\n",
        "-   For a discrete number of states, we cannot have instantaneous\n",
        "    transitions between states or it ceases to be measurable\n",
        "-   Instead: intensity of switching from state $i$ to $j$ as a $q_{ij}$\n",
        "    where\n",
        "\n",
        "$$\n",
        "\\mathbb P \\{ X(t + \\Delta) = j  \\,|\\, X(t) \\} = \\begin{cases} q_{ij} \\Delta + o(\\Delta) & i \\neq j\\\\\n",
        "                                                              1 + q_{ii} \\Delta + o(\\Delta) & i = j \\end{cases}\n",
        "$$\n",
        "\n",
        "-   With $o(\\Delta)$ is [little-o\n",
        "    notation](https://en.wikipedia.org/wiki/Big_O_notation#Little-o_notation).\n",
        "    That is, $\\lim_{\\Delta\\to 0} o(\\Delta)/\\Delta = 0$.\n",
        "\n",
        "## Intensity Matrix\n",
        "\n",
        "-   $Q_{ij} = q_{ij}$ for $i \\neq j$ and\n",
        "    $Q_{ii} = -\\sum_{j \\neq i} q_{ij}$\n",
        "-   Rows sum to 0\n",
        "-   For example, consider a counting process\n",
        "\n",
        "$$\n",
        "Q = \\begin{bmatrix} -0.1 & 0.1  & 0 & 0 & 0 & 0\\\\\n",
        "                    0.1  &-0.2  & 0.1 &  0 & 0 & 0\\\\\n",
        "                    0 & 0.1 & -0.2 & 0.1 & 0 & 0\\\\\n",
        "                    0 & 0 & 0.1 & -0.2 & 0.1 & 0\\\\\n",
        "                    0 & 0 & 0 & 0.1 & -0.2 & 0.1\\\\\n",
        "                    0 & 0 & 0 & 0 & 0.1 & -0.1\\\\\n",
        "    \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "## Probability Dynamics\n",
        "\n",
        "-   The $Q$ is the [infinitesimal\n",
        "    generator](https://en.wikipedia.org/wiki/Infinitesimal_generator_(stochastic_processes))\n",
        "    of the stochastic process.\n",
        "\n",
        "-   Let $\\pi(t) \\in \\mathbb{R}^N$ with\n",
        "    $\\pi_i(t) \\equiv \\mathbb{P}[X_t = i\\,|\\,X_0]$\n",
        "\n",
        "-   Then the probability distribution evolution (Fokker-Planck or KFE),\n",
        "    is $$\n",
        "    \\frac{d}{dt} \\pi(t) = \\pi(t) Q,\\quad \\text{ given }\\pi(0)\n",
        "    $$\n",
        "\n",
        "-   Or, often written as $\\frac{d}{dt} \\pi(t) = Q^{\\top} \\cdot \\pi(t)$,\n",
        "    i.e. in terms of the “adjoint” of the linear operator $Q$\n",
        "\n",
        "-   A steady state is then a solution to $Q^{\\top} \\cdot \\bar{\\pi} = 0$\n",
        "\n",
        "    -   i.e., the $\\bar{\\pi}$ left-eigenvector associated with\n",
        "        eigenvalue 0 (i.e. $\\bar{\\pi} Q = 0\\times \\bar{\\pi}$)\n",
        "\n",
        "## Setting up a Counting Process"
      ],
      "id": "848fdc1f-1350-473d-8235-581640682cbf"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "6×6 Tridiagonal{Float64, Vector{Float64}}:\n",
              " -0.1   0.1    ⋅     ⋅     ⋅     ⋅ \n",
              "  0.1  -0.2   0.1    ⋅     ⋅     ⋅ \n",
              "   ⋅    0.1  -0.2   0.1    ⋅     ⋅ \n",
              "   ⋅     ⋅    0.1  -0.2   0.1    ⋅ \n",
              "   ⋅     ⋅     ⋅    0.1  -0.2   0.1\n",
              "   ⋅     ⋅     ⋅     ⋅    0.1  -0.1"
            ]
          }
        }
      ],
      "source": [
        "alpha = 0.1\n",
        "N = 6\n",
        "Q = Tridiagonal(fill(alpha, N - 1),\n",
        "                [-alpha; fill(-2*alpha, N - 2); -alpha],\n",
        "                fill(alpha, N - 1))\n",
        "Q"
      ],
      "id": "3da322ff"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Finding the Stationary Distribution\n",
        "\n",
        "-   There will always be at least one eigenvalue of 0, and the\n",
        "    corresponding eigenvector is the stationary distribution\n",
        "-   Teaser: Do we really need all of the eigenvectors/eigenvalues?"
      ],
      "id": "c757675e-d1b7-4ce6-a2fd-0a0a858b8a30"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "output-location": "column"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lambda = [-0.3732050807568874, -0.29999999999999993, -0.19999999999999998, -0.09999999999999995, -0.026794919243112274, 0.0]"
          ]
        },
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "6-element Vector{Float64}:\n",
              " 0.16666666666666657\n",
              " 0.16666666666666657\n",
              " 0.1666666666666667\n",
              " 0.16666666666666682\n",
              " 0.16666666666666685\n",
              " 0.16666666666666663"
            ]
          }
        }
      ],
      "source": [
        "Lambda, vecs = eigen(Array(Q'))\n",
        "@show Lambda\n",
        "vecs[:, N] ./ sum(vecs[:, N])"
      ],
      "id": "b41c2738"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Using the Generator in a Bellman Equation\n",
        "\n",
        "-   Let $r \\in \\mathbb{R}^N$ be a vector of payoffs in each state, and\n",
        "    $\\rho > 0$ a discount rate,\n",
        "-   Then we can use the $Q$ generator as a simple Bellman Equation\n",
        "    (using the Kolmogorov Backwards Equation) to find the value $v$ in\n",
        "    each state,\n",
        "\n",
        "$$\n",
        "\\rho v = r + Q v\n",
        "$$\n",
        "\n",
        "-   Rearranging, $(\\rho I - Q) v = r$\n",
        "-   Teaser: can we just implement $(\\rho I - Q)\\cdot v$ and avoid\n",
        "    factorizing the matrix?\n",
        "\n",
        "## Implementing the Bellman Equation"
      ],
      "id": "b25e2482-9c9a-444b-912e-04ae9a9276e3"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "output-location": "column"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "typeof(rho * I - Q) = Tridiagonal{Float64, Vector{Float64}}"
          ]
        },
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "6-element Vector{Float64}:\n",
              "  38.15384615384615\n",
              "  57.23076923076923\n",
              "  84.92307692307693\n",
              " 115.07692307692311\n",
              " 142.76923076923077\n",
              " 161.84615384615384"
            ]
          }
        }
      ],
      "source": [
        "rho = 0.05\n",
        "r = range(0.0, 10.0, length=N)\n",
        "@show typeof(rho*I - Q)\n",
        "\n",
        "# solve (rho * I - Q) v = r\n",
        "v = (rho * I - Q) \\ r"
      ],
      "id": "addfa39b"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "julia-1.9",
      "display_name": "Julia 1.9.3",
      "language": "julia"
    },
    "language_info": {
      "name": "julia",
      "file_extension": ".jl",
      "mimetype": "application/julia",
      "version": "1.9.3"
    }
  }
}