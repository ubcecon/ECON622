{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ECON622: Computational Economics with Data Science Applications\n",
        "\n",
        "Optimization for Machine Learning\n",
        "\n",
        "Jesse Perla (University of British Columbia)\n",
        "\n",
        "# Overview\n",
        "\n",
        "## Summary\n",
        "\n",
        "-   This lecture continues from the previous lecture on gradients to\n",
        "    further explore optimization methods in machine learning, and\n",
        "    discusses training pipelines and tooling\n",
        "-   Primary reference materials are:\n",
        "    -   [ProbML Book 1:\n",
        "        Introduction](https://probml.github.io/pml-book/book1.html)\n",
        "    -   [ProbML Book 2: Advanced\n",
        "        Topics](https://probml.github.io/pml-book/book2.html) including\n",
        "        Section 6.3\n",
        "    -   [Mark Schmidt’s ML Lecture\n",
        "        Notes](https://www.cs.ubc.ca/~schmidtm/Courses/LecturesOnML/)\n",
        "-   We will also give a sense of a standard machine learning pipeline of\n",
        "    training, validation, and test data and discuss generalization,\n",
        "    logging, etc.\n",
        "\n",
        "## Why the Emphasis on Optimization and Gradients?\n",
        "\n",
        "-   A huge number of algorithms for economists can be written as\n",
        "    optimization problems (e.g., MLE, interpolation) or as something\n",
        "    similar in spirit (e.g. Bayesian Sampling, Reinforcement Learning)\n",
        "-   Previous lectures on AD showed ways to find VJPs for extremely\n",
        "    complicated functions. **Differentiate everything, no excuses!**\n",
        "-   In practice, **all** problems with high-dimensions parameters or\n",
        "    latents require gradients for algorithms to be feasible\n",
        "-   We will soon take a further step: **are (unbiased) estimates of the\n",
        "    gradient good enough for many algorithms?**\n",
        "\n",
        "# Optimization Crash Course\n",
        "\n",
        "## Optimization Methods\n",
        "\n",
        "-   Learning continuous optimization methods is an enormous project\n",
        "-   See referenced materials and lecture notes\n",
        "-   Here we will give an overview of some key concepts\n",
        "-   Be warned! The details matter, so more study is required if you want\n",
        "    to use these methods in practice\n",
        "\n",
        "## Crash Course in Unconstrained Optimization\n",
        "\n",
        "$$\n",
        "\\min_{\\theta} \\mathcal{L}(\\theta)\n",
        "$$\n",
        "\n",
        "Will briefly introduce\n",
        "\n",
        "-   First-order methods\n",
        "-   Second-order methods\n",
        "-   Preconditioning\n",
        "-   Momentum\n",
        "-   Regularization\n",
        "\n",
        "## First-Order Methods\n",
        "\n",
        "-   See [ProbML Book 1](https://probml.github.io/pml-book/book1.html)\n",
        "    Section 8.2\n",
        "-   Armed with reverse-mode AD for\n",
        "    $\\mathcal{L} : \\mathbb{R}^N \\to \\mathbb{R}$ we can calculate\n",
        "    $\\nabla \\mathcal{L}(\\theta)$ with the same computational order as\n",
        "    $\\mathcal{L}(\\theta)$\n",
        "-   Furthermore, given JVPs we know we can calculate these objective\n",
        "    functions for extremely complicated functions (e.g., nested fixed\n",
        "    points, and implicit functions)\n",
        "-   Iterative: take $\\theta_0$ and provide $\\theta_t \\to \\theta_{t+1}$\n",
        "    -   May converge to a stationary point (hopefully close to a global\n",
        "        argmin)\n",
        "    -   If it doesn’t converge, the solution may still be an argmin\n",
        "    -   See references for details on convergence for convex and\n",
        "        non-convex problems\n",
        "\n",
        "## Gradient Descent\n",
        "\n",
        "-   See [Mark Schmidt’s\n",
        "    Notes](https://www.cs.ubc.ca/~schmidtm/Courses/340-F22/L13.pdf)\n",
        "-   Gradient descent takes $\\theta_0$, and stepsize $\\eta_t$ and\n",
        "    iterates until $\\nabla \\mathcal{L}(\\theta_t)$ is small, or\n",
        "    $\\theta_t$ stationary\n",
        "\n",
        "$$\n",
        "\\theta_{t+1} = \\theta_t - \\eta_t \\nabla \\mathcal{L}(\\theta_t)\n",
        "$$\n",
        "\n",
        "-   It is the simplest “first-order” method (i.e., ones using just the\n",
        "    gradient of $\\mathcal{L}$)\n",
        "-   Will call $\\eta_t$ a “learning rate schedule”\n",
        "-   Think of line-search methods as choosing the stepsize $\\eta_t$\n",
        "    optimally. Useful as well for economists, even if used infrequently\n",
        "    in M\n",
        "\n",
        "## When and Where Does This Converge?\n",
        "\n",
        "-   Skipping a million details, see [ProbML Book\n",
        "    1](https://probml.github.io/pml-book/book1.html) Section 8.2.2 and\n",
        "    [Mark Schmidt’s\n",
        "    basic](https://www.cs.ubc.ca/~schmidtm/Courses/340-F22/L14.pdf) and\n",
        "    [more advanced\n",
        "    notes](https://www.cs.ubc.ca/~schmidtm/Courses/5XX-S22/S1.pdf)\n",
        "\n",
        "-   For strictly convex problems this converges to the global minima,\n",
        "    though sufficient conditions include Robbins-Monro\n",
        "    $\\lim_{T\\to \\infty} \\eta_T \\to 0$ and\n",
        "\n",
        "    $$\n",
        "    \\lim_{T\\to\\infty}\\frac{\\sum_{t=1}^T \\eta_t}{\\sum_{t=1}^T \\eta_t^2} = 0\n",
        "    $$\n",
        "\n",
        "-   For problems that not globally convex this may go to local optima,\n",
        "    but if the function is locally strictly convex then it will converge\n",
        "    to a local optima\n",
        "\n",
        "-   For other types of functions (e.g.,\n",
        "    [invex](https://en.wikipedia.org/wiki/Invex_function)) it may still\n",
        "    converge to the “right” solution in some important sense\n",
        "\n",
        "## Preconditioned Gradient Descent\n",
        "\n",
        "-   As we saw analyzing LLS, badly conditioned problems converge slowly\n",
        "    with iterative methods\n",
        "\n",
        "-   We can precondition a problem as we did with linear systems, and it\n",
        "    has the same stationary point\n",
        "\n",
        "-   Choose some $C_t$ for preconditioned gradient descent $$\n",
        "    \\theta_{t+1} = \\theta_t - \\eta_t C_t \\nabla \\mathcal{L}(\\theta_t)\n",
        "    $$\n",
        "\n",
        "-   We saw before that the Hessian tells us the geometry, so the optimal\n",
        "    preconditioner must be related to $\\nabla^2 \\mathcal{L}(\\theta_t)$\n",
        "\n",
        "## Second-Order Methods\n",
        "\n",
        "-   See [ProbML Book 1](https://probml.github.io/pml-book/book1.html)\n",
        "    Section 8.3 and [Mark Schmidt’s\n",
        "    Notes](https://www.cs.ubc.ca/~schmidtm/Courses/5XX-S22/S2.pdf)\n",
        "-   Adapt $\\eta_t C_t$ to use the Hessian (e.g., Newton’s Method)\n",
        "\n",
        "$$\n",
        "\\theta_{t+1} = \\theta_t - \\eta_t \\left[\\nabla^2 \\mathcal{L}(\\theta_t) \\right]^{-1}\\nabla \\mathcal{L}(\\theta_t)\n",
        "$$\n",
        "\n",
        "-   Second order methods are rarer because the calculating the Hessian\n",
        "    is no longer the same computational order as $\\mathcal{L}(\\theta)$\n",
        "-   See [ProbML Book 1](https://probml.github.io/pml-book/book1.html)\n",
        "    Section 8.3.2 for info on Quasi-Newtonian methods which\n",
        "    approximation Hessian using gradients like BFGS\n",
        "\n",
        "## Momentum\n",
        "\n",
        "-   See [ProbML Book 1](https://probml.github.io/pml-book/book1.html)\n",
        "    Section 8.2.4 and [Mark Schmidt’s\n",
        "    Notes](https://www.cs.ubc.ca/~schmidtm/Courses/5XX-S22/S2.pdf)\n",
        "-   Can use “momentum”, which speeds up convergence, helps avoid local\n",
        "    optima, and moves fast in flat regions\n",
        "-   Momentum will be a common feature of many ML optimizers (e.g. Adam,\n",
        "    RMSProp, etc.) as it helps with heavily non-convex problems\n",
        "-   A classic method is called Nesterov Accelerated Gradient (NAG),\n",
        "    which is a modification of gradient descent for some\n",
        "    $\\beta_t\\in (0,1)$ (e.g., $0.9$)\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\hat{\\theta}_{t+1} &= \\theta_t + \\beta_t(\\theta_t - \\theta_{t-1})\\\\\n",
        "\\theta_{t+1} &= \\hat{\\theta}_{t+1} - \\eta_t \\nabla \\mathcal{L}(\\hat{\\theta}_{t+1})\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "## Does Uniqueness Matter?\n",
        "\n",
        "-   Remember from our previous lecture on Sobolev norms and\n",
        "    regularization that we care about functions, not parameters.\n",
        "-   Consider when $\\theta$ is used as parameters for a function\n",
        "    (e.g. $\\hat{f}_{\\theta}$)\n",
        "    -   Then what does a lack of convergence of the $\\theta_t$ or\n",
        "        multiplicity with multiple $\\theta$ solutions mean?\n",
        "    -   Maybe nothing! If\n",
        "        $||\\hat{f}_{\\theta_0} - \\hat{f}_{\\theta_1}||_S$ is small, then\n",
        "        the functions themselves may be in the same equivalence class.\n",
        "        Depends on the norm, of course.\n",
        "-   This topic will be discussed when we consider double-descent curves,\n",
        "    but the punchline for now is that the training/optimization is a\n",
        "    means to an end (i.e., generalization) and not an end in itself.\n",
        "\n",
        "## Regularization\n",
        "\n",
        "-   See [Mark Schmidt’s\n",
        "    Notes](https://www.cs.ubc.ca/~schmidtm/Courses/340-F22/L16.pdf). For\n",
        "    LLS this is the ridge regression\n",
        "-   We discussed regularization as a way to deal with multiplicity\n",
        "\n",
        "$$\n",
        "\\min_{\\theta}\\left[\\mathcal{L}(\\theta) + \\frac{\\alpha}{2} ||\\theta||^2\\right]\n",
        "$$\n",
        "\n",
        "-   Gradient descent becomes (called “weight decay” in ML, and “ridge\n",
        "    regression” if objective is LLS)\n",
        "\n",
        "$$\n",
        "\\theta_{t+1} = \\theta_t - \\eta_t \\left[\\nabla \\mathcal{L}(\\theta_t) + \\alpha \\theta_t\\right]\n",
        "$$\n",
        "\n",
        "-   Mapping of regularized $\\theta_t$ to a $f_{\\theta_t}$ is subtle if\n",
        "    nonlinear\n",
        "\n",
        "# Stochastic Optimization\n",
        "\n",
        "## Are Gradients Really that Cheap to Calculate?\n",
        "\n",
        "-   Consider that the objective often involves data (or grid points for\n",
        "    interpolation)\n",
        "    -   Denote $x_n$, and observables $y_n$ for $n=1, \\ldots N$\n",
        "-   With VJPs, the computational order of\n",
        "    $\\nabla_{\\theta} \\mathcal{L}(\\theta;\\{x_n, y_n\\}_{n=1}^N)$ may be\n",
        "    the same as that of $\\mathcal{L}$ itself\n",
        "-   However, keep in mind that reverse-mode requires storing the\n",
        "    intermediate values in the “primal” calculation (i.e.,\n",
        "    $\\mathcal{L}(\\theta;\\{x_n, y_n\\}_{n=1}^N)$)\n",
        "    -   Hence, the memory requirements grow with $N$\n",
        "    -   This may be a big problem for large datasets or complicated\n",
        "        calculations, especially with GPUs which have more limited\n",
        "        memory\n",
        "\n",
        "## Do We Need the Full Gradient?\n",
        "\n",
        "-   In practice, it is impossible to calculate the full gradient for\n",
        "    large datasets\n",
        "\n",
        "-   In GD, the gradient provided the direction of steepest descent\n",
        "\n",
        "-   Consider an algorithm with a $g_t$ as an unbiased estimate of the\n",
        "    gradient\n",
        "\n",
        "    $$\n",
        "    \\begin{aligned}\n",
        "    \\theta_{t+1} = \\theta_t - \\eta_t g_t\\\\\n",
        "    \\mathbb{E}[g_t] = \\nabla \\mathcal{L}(\\theta_t)\n",
        "    \\end{aligned}\n",
        "    $$\n",
        "\n",
        "    -   Make the $\\eta_t$ smaller to deal with noise if this is\n",
        "        high-variance\n",
        "    -   Choose $g_t$ to be far cheaper to calculate than\n",
        "        $\\nabla \\mathcal{L}(\\theta_t)$\n",
        "\n",
        "-   Will turn out that this also adds additional regularization, which\n",
        "    helps with generalization\n",
        "\n",
        "## Stochastic Optimization\n",
        "\n",
        "-   To formalize: Up until now our optimizers have been “deterministic”\n",
        "-   Now we introduce a source of randomness $z \\sim q_{\\theta}(z)$,\n",
        "    i.e. it might depend on the estimated parameters $\\theta$ later with\n",
        "    RL/etc.\n",
        "    -   $z$ could be a source of uncertainty in the environment\n",
        "    -   $z$ could involve latent variables\n",
        "    -   $z$ could come from randomness in the optimization process\n",
        "        (e.g., using subsets of data to form $g_t$)\n",
        "-   Denote expectations using this distribution as\n",
        "    $\\mathbb{E}_{q_{\\theta}(z)}$\n",
        "-   For now, drop the dependence on $\\theta$ for simplicity, though it\n",
        "    becomes crucial for understanding reinforcement learning/etc.\n",
        "\n",
        "## Stochastic Objective\n",
        "\n",
        "-   The full optimization problem is then to minimize this stochastic\n",
        "    objective\n",
        "\n",
        "$$\n",
        "\\min_{\\theta}\\overbrace{\\mathbb{E}_{q(z)} \\tilde{\\mathcal{L}}(\\theta, z)}^{\\equiv \\mathcal{L}(\\theta)}\n",
        "$$\n",
        "\n",
        "-   Under appropriate regularity conditions, could use GD on this\n",
        "    objective\n",
        "\n",
        "$$\n",
        "\\nabla \\mathcal{L}(\\theta) = \\mathbb{E}_{q(z)}\\left[\\nabla \\tilde{\\mathcal{L}}(\\theta, z)\\right]\n",
        "$$\n",
        "\n",
        "-   But in practice, it is rare that we can marginalize out the $z$\n",
        "\n",
        "## Unbiased Draws from the Gradient\n",
        "\n",
        "-   Assume we can sample $z_t \\sim q(z)$ IID\n",
        "-   Then with enough regularity the gradient using just $z_t$ is\n",
        "    unbiased\n",
        "\n",
        "$$\n",
        "\\mathbb{E}_{q(z)}\\left[  \\nabla \\tilde{\\mathcal{L}}(\\theta_t, z_t) \\right] = \\nabla \\mathcal{L}(\\theta_t)\n",
        "$$\n",
        "\n",
        "-   That is, on average $\\nabla \\tilde{\\mathcal{L}}(\\theta_t, z_t)$ is\n",
        "    in the right direction for minimizing $\\mathcal{L}(\\theta_t)$\n",
        "-   This basic approach of finding unbiased estimators of the gradient\n",
        "    (and finding ways to lower the variance) is at the heart of most ML\n",
        "    optimization algorithms\n",
        "\n",
        "## Stochastic Gradient Descent\n",
        "\n",
        "-   See [ProbML Book 1](https://probml.github.io/pml-book/book1.html)\n",
        "    Section 8.4, [ProbML Book\n",
        "    2](https://probml.github.io/pml-book/book2.html) Section 6.3, and\n",
        "    [Mark Schmidt’s\n",
        "    Notes](https://www.cs.ubc.ca/~schmidtm/Courses/340-F22/L23.pdf)\n",
        "-   Given the previous slide, given IID samples $z_t \\sim q$, the\n",
        "    gradient is unbiased and we have the simplest version of stochastic\n",
        "    gradient descent (SGD)\n",
        "\n",
        "$$\n",
        "\\theta_{t+1} = \\theta_t - \\eta_t \\nabla \\tilde{\\mathcal{L}}(\\theta_t, z_t)\n",
        "$$\n",
        "\n",
        "-   Which converges to the minima of $\\min_{\\theta} \\mathcal{L}(\\theta)$\n",
        "    under appropriate conditions\n",
        "-   We can layer on all of the other features we discussed (e.g.,\n",
        "    momentum, preconditioning, etc) with SGD, but some become especially\n",
        "    important (e.g. the $\\eta_t$ schedule)\n",
        "\n",
        "## Finite-Sum Objectives\n",
        "\n",
        "-   Consider a special case of the loss function which is the sum of $N$\n",
        "    terms. For example with empirical risk minimization used in LLS/etc.\n",
        "\n",
        "    -   $z_n \\equiv (x_n, y_n)$ are typically data, observables, or grid\n",
        "        points\n",
        "    -   $\\ell(\\theta, x_n, y_n)$ is a loss function for a single data\n",
        "        point (e.g., forecasting using some $f_{\\theta}$)\n",
        "\n",
        "    $$\n",
        "    \\mathcal{L}(\\theta) = \\frac{1}{N}\\sum_{n=1}^N \\tilde{\\mathcal{L}}(\\theta, z_n) \\equiv \\frac{1}{N}\\sum_{n=1}^N \\ell(\\theta, x_n, y_n)\n",
        "    $$\n",
        "\n",
        "    -   For example, LLS is\n",
        "        $\\ell(\\theta, x_n, y_n) = ||y_n - \\theta \\cdot x_n||^2_2$\n",
        "\n",
        "-   In this case, the randomness of $z_t$ is which data point is chosen\n",
        "\n",
        "## SGD for Finite-Sum Objectives\n",
        "\n",
        "-   Hence consider sampling $z_t \\equiv (x_t, y_t)$ from our data.\n",
        "    -   In principle, IID with replacement\n",
        "-   Then run SGD on one data point at a time\n",
        "\n",
        "$$\n",
        "\\theta_{t+1} = \\theta_t - \\eta_t \\nabla_{\\theta} \\ell(\\theta_t, x_t, y_t)\n",
        "$$\n",
        "\n",
        "-   This may converges to the minima of $\\mathcal{L}(\\theta)$, and\n",
        "    potentially the storage requirements for calculations the gradient\n",
        "    are radically reduced\n",
        "-   You can guess that the $\\eta_t$ parameter is especially sensitive to\n",
        "    the variance of the gradient estimate\n",
        "\n",
        "## Decrease Variance with Multiple Draws\n",
        "\n",
        "-   With a single draw, the variance of the gradient estimate may be\n",
        "    high\n",
        "\n",
        "$$\n",
        "\\mathbb{E}\\left[\\nabla_{\\theta} \\ell(\\theta_t, x_t, y_t)- \\nabla \\mathcal{L}(\\theta_t)\\right]^2\n",
        "$$\n",
        "\n",
        "-   One tool to decrease the variance is just more monte-carlo draws.\n",
        "    With finite-sum objectives draw $B \\subseteq \\{1,\\ldots N\\}$ indices\n",
        "\n",
        "$$\n",
        "\\frac{1}{|B|}\\sum_{n \\in B} \\nabla_{\\theta} \\ell(\\theta_t, x_n, y_n)\n",
        "$$\n",
        "\n",
        "-   Classic SGD: $|B|=1$; GD: $B = \\{1, \\ldots N\\}$ and in between is\n",
        "    called “minibatch SGD”. Usually minibatch is implied with “SGD”\n",
        "\n",
        "## Minibatch SGD\n",
        "\n",
        "-   Algorithm is to draw $B_t$ indices at each step and execute SGD $$\n",
        "    \\begin{aligned}\n",
        "    g_t \\equiv \\frac{1}{|B_t|}\\sum_{n \\in B_t} \\nabla_{\\theta} \\ell(\\theta_t, x_n, y_n)\\\\\n",
        "    \\theta_{t+1} = \\theta_t - \\eta_t g_t\n",
        "    \\end{aligned}\n",
        "    $$\n",
        "\n",
        "-   Note that we never need to calculate $\\mathcal{L}(\\theta_t)$\n",
        "    directly, so can write our code to all operate on batches $B_t$\n",
        "\n",
        "-   Then layer other tricks on top (e.g., momentum, preconditioning,\n",
        "    etc.)\n",
        "\n",
        "    -   In principle you could also use minibatch with second-order or\n",
        "        quasi-newtonian methods but much rarer\n",
        "\n",
        "## Choosing Batches\n",
        "\n",
        "-   Choosing the $B_t$ process may be tricky. You could sample from\n",
        "    $\\{1,\\ldots N\\}$\n",
        "    -   with replacement\n",
        "    -   without replacement\n",
        "    -   without replacement after shuffling the data, and then ensure\n",
        "        you have gone through all of the data before repeating\n",
        "    -   etc.\n",
        "-   Just remember the goal: variance reduction on gradient estimates\n",
        "-   You want it to be unbiased in principle (consider partitioning the\n",
        "    data into batches and operating sequentially?)\n",
        "-   More art than science in many cases, because it requires many priors\n",
        "\n",
        "## “Grad Student Descent”\n",
        "\n",
        "-   This is how virtually all deep learning works. Just swap SGD with\n",
        "    slightly fancier algorithms using momentum, tinker with parameters,\n",
        "    etc.\n",
        "-   In practice, all of these optimizer settings (e.g., how large for\n",
        "    $|B_t|$, $\\eta_t$, convergence criteria, etc.) are fragile and\n",
        "    require a lot of tuning\n",
        "    -   Part of a a process called **hyperparameter optimization (HPO)**\n",
        "        where you try to find the best non-model parameters for your\n",
        "        goals\n",
        "    -   Same issue with all numerical methods in economics\n",
        "        (e.g. convergence criteria of fixed point iteration, initial\n",
        "        conditions)\n",
        "-   The concern is not just that it is time-consuming for researchers\n",
        "    (and ML “Grad Students”), but that it is easy for priors to sneak in\n",
        "    and bias results\n",
        "\n",
        "## What was our Goal?\n",
        "\n",
        "-   We will address this more formally next lecture, but it is worth\n",
        "    stepping back to think about our goals. Loosely:\n",
        "    -   If we are solving an empirical risk minimization problem (like\n",
        "        regressions, etc.) or interpolation, then our goal is to use the\n",
        "        “data” to find a function $\\hat{f}_{\\theta}$ that is close to\n",
        "        the “true” function $f^*$\n",
        "-   Fitting $\\hat{f}_{\\theta}$ is easy, but we want it to **generalize**\n",
        "    within the true distribution\n",
        "    -   But we don’t know that distribution (hence the “empirical”)\n",
        "    -   So a typical approach is to emulate this by splitting the data\n",
        "        we have\n",
        "    -   But HPO is dangerous because if we are not careful we can\n",
        "        “contaminate” our process for finding $\\hat{f}_{\\theta}$ using\n",
        "        some of the data we intend to check it with. Which might lead to\n",
        "        overfitting/etc.\n",
        "\n",
        "# Training Loops\n",
        "\n",
        "## Splitting the Data\n",
        "\n",
        "A standard way to do this for Empirical Risk\n",
        "Minimization/Regressions/etc. is to split it into three parts:\n",
        "\n",
        "1.  **Training** data used in fitting our approximations\n",
        "    -   This is just a means to an end in ML and economics\n",
        "2.  **Validation** data used for HPO and checking convergence criteria\n",
        "    -   Be cautious to avoid using it for training\n",
        "3.  **Test** data used to evaluate the generalization performance\n",
        "    -   Ensure we don’t accidentally use it in training or validation\n",
        "\n",
        "Not all problems will have this structure, and not all with have\n",
        "validation data.\n",
        "\n",
        "## Why Separate Validation and Test?\n",
        "\n",
        "-   As we will see in deep learning, with massive over-parameterization\n",
        "    you typically can interpolate all of the training data.\n",
        "    -   Minimizing training loss is a means to an end, which usually\n",
        "        ends at zero\n",
        "-   The validation data might be used to check stopping criteria by\n",
        "    checking how well the approximation generalizes to data outside of\n",
        "    training\n",
        "-   But if we are using it for a stopping criteria or HPO, then is is\n",
        "    **contaminated**!\n",
        "    -   Distorts our picture of generalization if we combine it into\n",
        "        test data\n",
        "\n",
        "## What about Interpolation Problems?\n",
        "\n",
        "-   When simply trying to find interpolating functions which solve\n",
        "    functional equations, the risk of prior contamination is less clear\n",
        "-   However, you may still want to separate out validation and test grid\n",
        "    points because any data you use for HPO or convergence criteria\n",
        "    can’t be used to understand generalization.\n",
        "-   For example consider:\n",
        "    1.  Fit until “training” loss is zero\n",
        "    2.  Keep running stochastic optimizer until “validation” loss is\n",
        "        zero\n",
        "-   In that case, it crudely interpolating the validation data, which\n",
        "    makes it equivalent to training data? Not useful for generalization\n",
        "    -   May find that the model generalized better if you **stopped\n",
        "        earlier**\n",
        "\n",
        "## Level of Abstraction for Optimizers\n",
        "\n",
        "-   While you can setup a standard optimization objective and optimizer,\n",
        "    most ML frameworks work at a lower level\n",
        "-   The key reasons are that:\n",
        "    -   Minibatching (usually just called “batches”) requires more\n",
        "        flexibility in implementation to be efficient\n",
        "    -   Stopping criteria is more complicated with highly\n",
        "        overparameterized models\n",
        "    -   Logging and validation logic requires more flexibility\n",
        "    -   Often you will want to take a snapshot of the current best\n",
        "        solution and continue later for refinement (or to solve in\n",
        "        parallel)\n",
        "\n",
        "## Steps and Epochs\n",
        "\n",
        "-   There is a great deal of flexibility in how you setup the optimizer\n",
        "-   But a common approach is to randomly shuffle the data, create a set\n",
        "    of batches $B_t$ (without replacement), and then iterate through\n",
        "    them\n",
        "-   Terminology (when relevant)\n",
        "    -   Every iteration of SGD for a given batch is a **step**\n",
        "    -   If you have gone through the entire dataset once, we say that\n",
        "        you have completed an **epoch**\n",
        "-   At the end of an epoch is a good time to log, check the validation\n",
        "    loss, and potentially stop the training\n",
        "\n",
        "## Software Components used in ML\n",
        "\n",
        "Some common software components for optimization are\n",
        "\n",
        "1.  **Autodifferentiation** and libraries of functions provide the\n",
        "    approximation class\n",
        "2.  **Data loaders** which will take care of providing batches to the\n",
        "    optimizers\n",
        "3.  **Optimizers** are typically iterative, have an internal state, and\n",
        "    you can update with one sample of the gradient for that batch\n",
        "4.  **Logging** and visualization tools to track progress because the\n",
        "    optimization process may be slow and you want to do HPO\n",
        "5.  **HPO** software using training, validation, and possibly test loss\n",
        "\n",
        "## Logging and Visualization\n",
        "\n",
        "-   Several tools exist for logging to babysit optimizers, find good\n",
        "    hyperparameters, etc. including\n",
        "    [Tensorboard](https://www.tensorflow.org/tensorboard)\n",
        "    -   But we will use [Weights and Biases](https://wandb.ai/site)\n",
        "        (W&B) because it is a market leader, free for academics and\n",
        "        seems to be the frontrunner\n",
        "-   Many algorithms and frameworks exist for HPO:\n",
        "    -   [Weights and Biases](https://wandb.ai/site) (W&B) has a built-in\n",
        "        HPO framework using random search and bayesian optimization\n",
        "    -   [Optuna](https://optuna.org/) and [Ray\n",
        "        Tune](https://docs.ray.io/en/master/tune/index.html) is a\n",
        "        popular open-source HPO framework\n",
        "    -   [Ray Tune](https://docs.ray.io/en/master/tune/index.html) is a\n",
        "        popular open-source HPO framework\n",
        "-   HPO frameworks will often use the\n",
        "    [command-line](https://github.com/shadawck/awesome-cli-frameworks#python)\n",
        "    to run new jobs. [Python\n",
        "    Fire](https://github.com/google/python-fire)\n",
        "\n",
        "## Broad Frameworks for Machine Learning\n",
        "\n",
        "-   You can just hand-code loops/etc. which seems the best approach for\n",
        "    JAX\n",
        "    -   Even with Pytorch, it isn’t obvious that a framework is better\n",
        "        ex-post, though ex-ante it can help you try different\n",
        "        permutations easily\n",
        "-   [Pytorch Lightning](https://www.pytorchlightning.ai/) is a popular\n",
        "    framework which will formalize the training loops even across\n",
        "    distributed systems and make CLI, HPO, logging, etc. convenient\n",
        "    -   It remains fairly flexible because it is just wrapping Pytorch\n",
        "-   [Keras](https://keras.io/) is a similar framework with the ability\n",
        "    to target multiple backends (e.g., Pytorch, JAX)\n",
        "    -   The challenge is that it is much less flexible for non-typical\n",
        "        research\n",
        "-   [Hydra](https://github.com/facebookresearch/hydra) is a framework\n",
        "    for more serious engineering code\n",
        "\n",
        "# Detailed Pytorch Linear Regression Example\n",
        "\n",
        "## Linear Regression Examples\n",
        "\n",
        "-   Of course SGD is a terrible way to do a standard linear regression,\n",
        "    but it will help us understand the mechanics with a well-understood\n",
        "    problem\n",
        "-   These examples simulate data for some:\n",
        "    $y_n = x_n \\cdot \\theta + \\sigma \\epsilon$ for\n",
        "    $\\epsilon \\sim N(0,1)$\n",
        "-   They then show various features of the optimization pipeline and\n",
        "    software to implement the LLS ERM objective\n",
        "\n",
        "$$\n",
        "\\min_{\\theta} \\frac{1}{N} \\sum_{n=1}^N \\left[y_n - x_n \\cdot \\theta\\right]^2\n",
        "$$\n",
        "\n",
        "-   Which we note has a finite-sum objective, which lets us use\n",
        "    minibatch SGD with gradient estimates\n",
        "-   Install in your environment with `pip install -r requirements.txt`\n",
        "\n",
        "## Packages\n",
        "\n",
        "-   Before showing all of the variations, here we will implement an\n",
        "    inline SGD version with minibatches"
      ],
      "id": "cd6a89a3-cabe-4c36-94d6-64e170e17b57"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import equinox as eqx\n",
        "from torch.utils.data import TensorDataset, DataLoader, TensorDataset"
      ],
      "id": "f14a47b6"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Simulate Data\n",
        "\n",
        "$$\n",
        "y \\sim N(x \\cdot \\theta, \\sigma^2)\n",
        "$$"
      ],
      "id": "0fa7c842-ae2e-41ae-a1d8-e461bc49ba9b"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor([ 0.4478, -0.7485]), tensor(0.2630))"
          ]
        }
      ],
      "source": [
        "N = 500  # samples\n",
        "M = 2\n",
        "sigma = 0.001\n",
        "theta = torch.randn(M)\n",
        "X = torch.randn(N, M)\n",
        "Y = X @ theta + sigma * torch.randn(N)\n",
        "dataset = TensorDataset(X, Y)\n",
        "print(dataset[0]) # returns tuples of (x_n, y_n)"
      ],
      "id": "03e78c73"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataloaders Provide Batches\n",
        "\n",
        "-   Code which serves up random batches of data for gradient estimates\n",
        "    are called “dataloaders”\n",
        "-   The following code shuffles the data and provides batches of size\n",
        "    `batch_size`\n",
        "-   It returns a [Python\n",
        "    generator](https://docs.python.org/3/howto/functional.html#generator-expressions-and-list-comprehensions)\n",
        "    which can be iterated until it hits the end of the data"
      ],
      "id": "56ac5f00-29b5-438e-af63-952db32b2a46"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "output-location": "column"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([[ 2.0556, -0.9117],\n",
            "        [-0.4503, -0.7447],\n",
            "        [ 0.9591,  0.0824],\n",
            "        [ 1.0008, -0.4637],\n",
            "        [-2.1695, -1.3079],\n",
            "        [ 0.5311, -1.9840],\n",
            "        [-0.5480,  0.1194],\n",
            "        [-0.0364,  0.5186]]), tensor([-0.3133,  0.6384, -0.4519, -0.1410,  1.6973,  0.9745,  0.1600, -0.2956])]"
          ]
        }
      ],
      "source": [
        "batch_size = 8\n",
        "train_loader = DataLoader(dataset,\n",
        "                batch_size=batch_size,\n",
        "                shuffle=True)\n",
        "# e.g. iterate and get first element\n",
        "print(next(iter(train_loader)))"
      ],
      "id": "5f0204c7"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loss Function for Gradient Descent\n",
        "\n",
        "-   Reminder: need to provide AD-able functions which give a gradient\n",
        "    estimate, not necessarily the objective itself!\n",
        "-   In particular, for LLS we simply can find the MSE between the\n",
        "    prediction and the data for the batch itself"
      ],
      "id": "905fd68b-3ebb-466c-b0ee-a55a0e70fbcd"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def residuals(model, X, Y):  # batches or full data\n",
        "    Y_hat = model(X).squeeze()\n",
        "    return ((Y_hat - Y) ** 2).mean()"
      ],
      "id": "f2886ad9"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hypothesis Class\n",
        "\n",
        "-   The “Hypothesis Class” for our ERM approximation is linear in this\n",
        "    case.\n",
        "-   Anything with differentiable parameters is a sub-class of the\n",
        "    `nn.Module` in Pytorch. Special case of Neural Networks\n",
        "-   In this case, we can just use a prebuilt linear approximation from\n",
        "    $\\mathbb{R}^M \\to \\mathbb{R}^1$ without an affine constant term.\n",
        "-   The underlying parameters will have a random initialization, which\n",
        "    becomes **crucial** with overparameterized models (but wouldn’t be\n",
        "    important here)"
      ],
      "id": "e0d7e10f-5568-4709-a3f8-01ef80c50545"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear(in_features=2, out_features=1, bias=False)"
          ]
        }
      ],
      "source": [
        "model = nn.Linear(M, 1, bias=False)  # random initialization\n",
        "print(model)"
      ],
      "id": "0507e760"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optimizer\n",
        "\n",
        "-   First-order optimizers take steps using gradient estimates\n",
        "-   In many ML applications you will want control over the process, so\n",
        "    will manually call the function to collect the gradient estimate\n",
        "    then call the optimizer to take the next step\n",
        "-   Here we will just use SGD with a fixed learning rate\n",
        "-   Note that the optimizer is constructed to look directly at the\n",
        "    `parameters` for your underlying model(s)"
      ],
      "id": "a4f52236-ca20-4298-8bc5-3fe19b80156f"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "output-location": "column"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SGD (\n",
            "Parameter Group 0\n",
            "    dampening: 0\n",
            "    differentiable: False\n",
            "    foreach: None\n",
            "    lr: 0.001\n",
            "    maximize: False\n",
            "    momentum: 0\n",
            "    nesterov: False\n",
            "    weight_decay: 0\n",
            ")"
          ]
        }
      ],
      "source": [
        "optimizer = optim.SGD(\n",
        "    model.parameters(), lr=0.001\n",
        ")\n",
        "print(optimizer)"
      ],
      "id": "61428d22"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Loop\n",
        "\n",
        "-   Finally, we can loop for multiple “epochs” of passes through the\n",
        "    data with the dataloader, calling the optimizer to update each time\n",
        "-   The `for ... in train_loader:` will repeat until the end of the data\n",
        "    and continue to the next epoch (i.e., pass through data)\n",
        "-   Each batch updates a `step` using the optimizer, which is unaware of\n",
        "    epochs/batches/etc."
      ],
      "id": "67f8d864-9c80-4c78-8dbd-c3fa509576c8"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "||theta - theta_hat|| = 6.593696161871776e-05"
          ]
        }
      ],
      "source": [
        "for epoch in range(300):\n",
        "    for X_batch, Y_batch in train_loader:\n",
        "        optimizer.zero_grad() \n",
        "        loss = residuals(model, X_batch, Y_batch)  # primal\n",
        "        loss.backward()  # backprop/reverse-mode AD\n",
        "        # Now the model.parameters have gradients updated, so...\n",
        "        optimizer.step()  # Update the optimizers internal parameters\n",
        "print(f\"||theta - theta_hat|| = {torch.norm(theta - model.weight.squeeze())}\")        "
      ],
      "id": "a2a68c4a"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# More Pytorch Linear Regression Examples\n",
        "\n",
        "## Gradient Descent\n",
        "\n",
        "-   See\n",
        "    [examples/linear_regression_pytorch_gd.py](examples/linear_regression_pytorch_gd.py)\n",
        "-   Simulates data and shows the basic training loop\n",
        "-   Using the “full batch” to calculate the residuals\n",
        "\n",
        "## Stochastic Gradient Descent\n",
        "\n",
        "-   See\n",
        "    [examples/linear_regression_pytorch_sgd.py](examples/linear_regression_pytorch_sgd.py)\n",
        "-   This takes the existing code, but adds in code to calculate gradient\n",
        "    estimates using minibatches\n",
        "-   Note that this is creating batches by shuffling the data and the\n",
        "    going through it `batch_size` chunks at a time\n",
        "-   When it gets to the end of that data, it is the end of the `epoch`\n",
        "\n",
        "## Adam + Bells and Whistles\n",
        "\n",
        "-   See\n",
        "    [examples/linear_regression_pytorch_adam.py](examples/linear_regression_pytorch_adam.py)\n",
        "-   This extends the previous version and adds in the full\n",
        "    train/val/test datasplit\n",
        "    -   The `val_loss` is collected and displayed at the end of each\n",
        "        epoch\n",
        "-   It also shows a learning rate scheduler and a few utilities for\n",
        "    logging and early stopping\n",
        "\n",
        "## Logging with Weights and Biases\n",
        "\n",
        "-   See\n",
        "    [examples/linear_regression_pytorch_logging.py](examples/linear_regression_pytorch_logging.py)\n",
        "\n",
        "-   This adds in support for Weights and Biases, and also demonstrates\n",
        "    the use of a custom `nn.Module` for the hypothesis class\n",
        "\n",
        "    1.  Go to [wandb.ai](https://wandb.ai/) and create an account,\n",
        "        ideally linked to your github\n",
        "    2.  Ensure you have installed the packages with\n",
        "        `pip install -r requirements.txt`\n",
        "    3.  Run `wandb login` in terminal to connect to your account\n",
        "\n",
        "-   You will then be able to run these files and see results on\n",
        "    [wandb.ai](https://wandb.ai/)\n",
        "\n",
        "## Pytorch Lightning\n",
        "\n",
        "-   See\n",
        "    [examples/linear_regression_pytorch_lightning.py](examples/linear_regression_pytorch_lightning.py)\n",
        "-   See\n",
        "    [examples/linear_regression_pytorch_lightning_defaults.yaml](examples/linear_regression_pytorch_lightning_defaults.yaml)\n",
        "    for default HPO and parameters\n",
        "-   This is using many features in [Pytorch\n",
        "    Lightning](https://www.pytorchlightning.ai/) to simplify the code\n",
        "-   The optimizer, stopping rules, logging, learning rate scheduler,\n",
        "    etc. are all handled by the framework and can be configured in that\n",
        "    file or on the CLI.\n",
        "-   Can change values on commandline to override the `yaml` file,\n",
        "\n",
        "``` bash\n",
        "python lectures/examples/linear_regression_pytorch_lightning.py --optimizer.lr=0.0001 --model.N=500\n",
        "python lectures/examples/linear_regression_pytorch_lightning.py --trainer.max_epochs=500\n",
        "```\n",
        "\n",
        "## Sweeps with W&B\n",
        "\n",
        "-   See\n",
        "    [examples/linear_regression_pytorch_sweep.yaml](lectures/examples/linear_regression_pytorch_sweep.yaml)\n",
        "    for a [sweep file](https://docs.wandb.ai/guides/sweeps) which\n",
        "    provides a HPO experiment to run and log\n",
        "    -   Executes variations on `--model.batch_size=32` and\n",
        "        `--model.lr=0.001` etc\n",
        "    -   Tries to choose them to minimize the `val_loss` as a HPO\n",
        "        objective\n",
        "    -   First, create the sweep,\n",
        "        `wandb sweep lectures/examples/linear_regression_pytorch_sweep.yaml`\n",
        "    -   Then run `wandb agent <sweep_id>` with returned sweep id\n",
        "    -   Call `wandb agent <sweep_id>` on multiple computers to run in\n",
        "        parallel\n",
        "-   Any CLI implementation can be used with these sorts of frameworks\n",
        "\n",
        "# Detailed JAX Linear Regression Example\n",
        "\n",
        "## Packages\n",
        "\n",
        "-   `optax` is a common package for ML optimization methods"
      ],
      "id": "cf152186-68c5-4f29-9018-28c1bcb21d07"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import grad, jit, value_and_grad, vmap\n",
        "from jax import random\n",
        "import optax"
      ],
      "id": "ea59991e"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Simulate Data\n",
        "\n",
        "-   Few differences here, except for manual use of the `key`\n",
        "-   Remember that if you use the same `key` you get the same value.\n",
        "-   See [JAX\n",
        "    docs](https://jax.readthedocs.io/en/latest/jax-101/05-random-numbers.html)\n",
        "    for more details"
      ],
      "id": "6d70bfbb-d4ff-4076-a68b-8088001a6149"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "N = 500  # samples\n",
        "M = 2\n",
        "sigma = 0.001\n",
        "key = random.PRNGKey(42)\n",
        "# Pattern: split before using key, replace name \"key\"\n",
        "key, *subkey = random.split(key, num=4)\n",
        "theta = random.normal(subkey[0], (M,))\n",
        "X = random.normal(subkey[1], (N, M))\n",
        "Y = X @ theta + sigma * random.normal(subkey[2], (N,))  # Adding noise"
      ],
      "id": "e439c7de"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataloaders Provide Batches\n",
        "\n",
        "-   For more complicated data (e.g. images, text) JAX can use other\n",
        "    packages, but it doesn’t have a canonical dataloader at this point\n",
        "-   But in this case we can manually create this, using\n",
        "    [`yield`](https://docs.python.org/3/howto/functional.html#generators)"
      ],
      "id": "c9f9cb72-4482-4436-8e01-2315322238c5"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "output-location": "column"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Array([[ 0.05907545, -1.7277497 ],\n",
            "       [-1.3816313 ,  0.33074763],\n",
            "       [ 0.76224667, -0.07191363],\n",
            "       [-0.46871892,  0.24884424]], dtype=float32), Array([-1.3571022 ,  0.07165897,  0.0480412 ,  0.13437417], dtype=float32))"
          ]
        }
      ],
      "source": [
        "def data_loader(key, X, Y, batch_size):\n",
        "    N = X.shape[0]\n",
        "    assert N == Y.shape[0]\n",
        "    indices = jnp.arange(N)\n",
        "    indices = random.permutation(key, indices)\n",
        "    # Loop over batches and yield\n",
        "    for i in range(0, N, batch_size):\n",
        "        b_indices = indices[i:i + batch_size]\n",
        "        yield X[b_indices], Y[b_indices]\n",
        "# e.g. iterate and get first element\n",
        "dl_test = data_loader(key, X, Y, 4)\n",
        "print(next(iter(dl_test)))"
      ],
      "id": "4ec3771a"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hypothesis Class\n",
        "\n",
        "-   The “Hypothesis Class” for our ERM approximation is linear in this\n",
        "    case\n",
        "-   JAX is functional and non-mutating, so you must write stateless code\n",
        "-   We will move towards a more general class with the `equinox`\n",
        "    package, but for now we will implement the model with the parameters\n",
        "    directly\n",
        "-   The underlying parameters will have a random initialization, which\n",
        "    becomes **crucial** with overparameterized models (but wouldn’t be\n",
        "    important here)"
      ],
      "id": "b36209aa-4c92-45c0-b73b-15c1520907ec"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "theta_0 = [ 1.7535115  -0.07298409], theta = [0.1378821  0.79073715]"
          ]
        }
      ],
      "source": [
        "def predict(theta, X):\n",
        "    return jnp.matmul(X, theta) #or jnp.dot(X, theta)\n",
        "\n",
        "# Need to randomize our own theta_0 parameters\n",
        "key, subkey = random.split(key)\n",
        "theta_0 = random.normal(subkey, (M,))\n",
        "print(f\"theta_0 = {theta_0}, theta = {theta}\")"
      ],
      "id": "1d1ebe51"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loss Function for Gradient Descent\n",
        "\n",
        "-   Reminder: need to provide AD-able functions which give a gradient\n",
        "    estimate, not necessarily the objective itself!\n",
        "-   In particular, for LLS we simply can find the MSE between the\n",
        "    prediction and the data for the batch itself\n",
        "-   For now, we are passing the `params` rather than the `model` itself"
      ],
      "id": "b4b96783-015c-4c97-8120-013dde371239"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "def vectorized_residuals(params, X, Y):\n",
        "    Y_hat = predict(params, X)\n",
        "    return jnp.mean((Y_hat - Y) ** 2)"
      ],
      "id": "01d5f20f"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optimizer\n",
        "\n",
        "-   The `optimizer.init(theta_0)` provides the initial state for the\n",
        "    iterations\n",
        "-   With SGD it is empty, but with momentum/etc. it will have internal\n",
        "    state"
      ],
      "id": "c09f95b2-91c5-44c7-a51d-ce25c15828e6"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "output-location": "column"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimizer state:(EmptyState(), EmptyState())"
          ]
        }
      ],
      "source": [
        "lr = 0.001\n",
        "batch_size = 16\n",
        "num_epochs = 201\n",
        "\n",
        "# optax.adam(lr) is worse here\n",
        "optimizer = optax.sgd(lr)\n",
        "opt_state = optimizer.init(theta_0)\n",
        "print(f\"Optimizer state:{opt_state}\")\n",
        "params = theta_0 # initial condition"
      ],
      "id": "6b3dfe9e"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Using Optimizer for a Step\n",
        "\n",
        "-   Here we write a (compiled) utility function which:\n",
        "    1.  Calculates the loss and gradient estimates for the batch\n",
        "    2.  Updates the optimizer state\n",
        "    3.  Applies the updates to the parameters\n",
        "    4.  Returns the updated parameters, optimizer state, and loss\n",
        "-   The reason to set this up as a function is to maintain JAXs “pure”\n",
        "    style"
      ],
      "id": "e790c0ad-b4aa-4b63-98e1-048215eccde4"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "@jax.jit\n",
        "def make_step(params, opt_state, X, Y):\n",
        "  loss_value, grads = jax.value_and_grad(vectorized_residuals)(params, X, Y)\n",
        "  updates, opt_state = optimizer.update(grads, opt_state, params)\n",
        "  params = optax.apply_updates(params, updates)\n",
        "  return params, opt_state, loss_value"
      ],
      "id": "3ccecde7"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Loop Version 1\n",
        "\n",
        "-   Note that unlike Pytorch the gradients are passed as parameters"
      ],
      "id": "f4281659-234d-4935-be36-0efb357410ef"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0,||theta - theta_hat|| = 1.714521050453186\n",
            "Epoch 100,||theta - theta_hat|| = 0.0020757941529154778\n",
            "Epoch 200,||theta - theta_hat|| = 6.367397145368159e-05\n",
            "||theta - theta_hat|| = 6.367397145368159e-05"
          ]
        }
      ],
      "source": [
        "for epoch in range(num_epochs):\n",
        "    key, subkey = random.split(key) # changing key for shuffling each epoch\n",
        "    train_loader = data_loader(subkey, X, Y, batch_size)\n",
        "    for X_batch, Y_batch in train_loader:\n",
        "        params, opt_state, train_loss = make_step(params, opt_state, X_batch, Y_batch)  \n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"Epoch {epoch},||theta - theta_hat|| = {jnp.linalg.norm(theta - params)}\")\n",
        "\n",
        "print(f\"||theta - theta_hat|| = {jnp.linalg.norm(theta - params)}\")"
      ],
      "id": "7494afab"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Auto-Vectorizing\n",
        "\n",
        "-   In the above case the `vectorized_residuals` was able to use a\n",
        "    directly vectorized function.\n",
        "-   However in many cases it will be more convenient to write code for a\n",
        "    single element of the finite-sum objectives\n",
        "-   Now we will rewrite our objective to demonstrate how to use `vmap`"
      ],
      "id": "7fd1a389-8e85-4856-a5bd-c55d79af537f"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.021030858\n",
            "3.546122"
          ]
        }
      ],
      "source": [
        "def residual(theta, x, y):\n",
        "    y_hat = predict(theta, x)\n",
        "    return (y_hat - y) ** 2\n",
        "\n",
        "@jit\n",
        "def residuals(theta, X, Y):\n",
        "    # Use vmap, fixing the 1st argument\n",
        "    batched_residuals = jax.vmap(residual, in_axes=(None, 0, 0))\n",
        "    return jnp.mean(batched_residuals(theta, X, Y))\n",
        "print(residual(theta_0, X[0], Y[0]))\n",
        "print(residuals(theta_0, X, Y))"
      ],
      "id": "479a810e"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## New Step and Initialization\n",
        "\n",
        "-   This simply changes the function used for the `value_and_grad` call\n",
        "    to use the new `residuals` function and resets our optimizer"
      ],
      "id": "32bef6e2-3e38-4ef9-af1d-441a1c0aadc1"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "@jax.jit\n",
        "def make_step(params, opt_state, X, Y):     \n",
        "  loss_value, grads = jax.value_and_grad(residuals)(params, X, Y)\n",
        "  updates, opt_state = optimizer.update(grads, opt_state, params)\n",
        "  params = optax.apply_updates(params, updates)\n",
        "  return params, opt_state, loss_value\n",
        "optimizer = optax.sgd(lr) # better than optax.adam here\n",
        "opt_state = optimizer.init(theta_0)\n",
        "params = theta_0"
      ],
      "id": "bbdeda03"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Loop Version 2\n",
        "\n",
        "-   Otherwise the training loop is the same"
      ],
      "id": "a5178393-0360-4804-90a6-2ad040c27521"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0,||theta - theta_hat|| = 1.7061582803726196\n",
            "Epoch 100,||theta - theta_hat|| = 0.002027335576713085\n",
            "Epoch 200,||theta - theta_hat|| = 6.216356268851086e-05\n",
            "||theta - theta_hat|| = 6.216356268851086e-05"
          ]
        }
      ],
      "source": [
        "for epoch in range(num_epochs):\n",
        "    key, subkey = random.split(key) # changing key for shuffling each epoch\n",
        "    train_loader = data_loader(subkey, X, Y, batch_size)\n",
        "    for X_batch, Y_batch in train_loader:\n",
        "        params, opt_state, train_loss = make_step(params, opt_state, X_batch, Y_batch)  \n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"Epoch {epoch},||theta - theta_hat|| = {jnp.linalg.norm(theta - params)}\")\n",
        "\n",
        "print(f\"||theta - theta_hat|| = {jnp.linalg.norm(theta - params)}\")"
      ],
      "id": "77bb6d97"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## JAX Examples\n",
        "\n",
        "-   See\n",
        "    [examples/linear_regression_jax_sgd.py](examples/linear_regression_jax_sgd.py)\n",
        "    -   This implements the inline code above without the vmap\n",
        "-   See\n",
        "    [examples/linear_regression_jax_vmap.py](examples/linear_regression_jax_vmap.py)\n",
        "    -   This implements the `vmap` as above\n",
        "    -   This also adds in an [learning rate\n",
        "        schedule](https://optax.readthedocs.io/en/latest/api.html#optimizer-schedules)\n",
        "-   See\n",
        "    [examples/linear_regression_jax_equinox.py](examples/linear_regression_jax_equinox.py)\n",
        "    -   See the following example for more\n",
        "\n",
        "# JAX Linear Regression with Equinox\n",
        "\n",
        "## Using an Equinox Model\n",
        "\n",
        "-   While it seems convenient to work in a functional style, when we\n",
        "    move towards nested, deep approximations it can become cumbersome\n",
        "-   An excellent package to both define hypothesis classes and to work\n",
        "    with their gradients is\n",
        "    [equinox](https://github.com/patrick-kidger/equinox)\n",
        "-   Key to the design of equinox is that if we use that the nested\n",
        "    parameters of an approximation class are PyTrees, then we can find\n",
        "    the derivative with respect to that entire type. See our previous\n",
        "    example on differentiating PyTrees\n",
        "\n",
        "## Equinox Macros\n",
        "\n",
        "-   There are several macros in equinox which make it easier to work\n",
        "    with differentiable PyTrees, all of which have the name `filter_`\n",
        "    -   Loosely: these macros go through the underlying python\n",
        "        datastructures and filter out values into static values vs. ones\n",
        "        which could be perturbed\n",
        "    -   For example, if I have a type with a `jnp.array` for the\n",
        "        differentiable weights and a `int` which stores the number of\n",
        "        hidden dimensions, then the `filter_grad` will flag the\n",
        "        `jnp.array` as differentiable and the `int` will have the\n",
        "        gradient type as `None`\n",
        "-   When in doubt, always replace them. e.g. `@jax.jit` can be replied\n",
        "    with `@equinox.filter_jit`, `jax.vmap` should be replaced with\n",
        "    `eqx.filter_vmap`, etc.\n",
        "\n",
        "## Hypothesis Class\n",
        "\n",
        "-   We are moving towards Neural Networks, which are a very broad class\n",
        "    of approximations.\n",
        "-   Here lets just use a linear approximation with no constant term\n",
        "-   As always, the initial randomization will become increasingly\n",
        "    important"
      ],
      "id": "a892b66b-b08e-4eac-b123-1e81260583e2"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-0.01143495  0.6449629 ]]"
          ]
        }
      ],
      "source": [
        "key, subkey = random.split(key)\n",
        "model = eqx.nn.Linear(M, 1, use_bias = False, key = subkey)\n",
        "print(model.weight)"
      ],
      "id": "ba22e3c9"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Residuals using the Model\n",
        "\n",
        "-   The model now contains all of the, potentially nested, parameters\n",
        "    for the approximation class\n",
        "-   It provides call notation to evaluate the function with those\n",
        "    parameters"
      ],
      "id": "c7049bbc-1817-4b25-9490-41faeb827654"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "def residual(model, x, y):\n",
        "    y_hat = model(x)\n",
        "    return (y_hat - y) ** 2\n",
        "\n",
        "def residuals(model, X, Y):\n",
        "    batched_residuals = vmap(residual, in_axes=(None, 0, 0))\n",
        "    return jnp.mean(batched_residuals(model, X, Y))\n",
        "# Alternatively, could have combined and still jit/grad\n",
        "def residuals_2(model, X, Y):\n",
        "    Y_hat = vmap(model)(X).squeeze()\n",
        "    return jnp.mean((Y - Y_hat) ** 2)"
      ],
      "id": "755263c1"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Gradients of Models\n",
        "\n",
        "-   As discussed, we can find the gradients of richer objects than just\n",
        "    arrays\n",
        "-   Optimizer updates use perturbations of the underlying PyTree\n",
        "-   Updates can be applied because the type of the gradients matches the\n",
        "    underlying PyTree"
      ],
      "id": "ed9b728b-4cdc-407f-8563-34e957307f4d"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear(\n",
            "  weight=f32[1,2],\n",
            "  bias=None,\n",
            "  in_features=2,\n",
            "  out_features=1,\n",
            "  use_bias=False\n",
            ")\n",
            "[[-0.30091333 -0.31419277]]"
          ]
        }
      ],
      "source": [
        "grads = jax.grad(residuals)(model, X, Y)\n",
        "print(grads)\n",
        "print(grads.weight)"
      ],
      "id": "65d662d6"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup of the Optimizer and `make_step`\n",
        "\n",
        "-   The `make_step` isn’t very different, except for using a few\n",
        "    `equinox` utility functions"
      ],
      "id": "294c7fe0-9548-4cd6-8d4a-9226e6c48f3a"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "lr = 0.001\n",
        "optimizer = optax.sgd(lr)\n",
        "# Needs to remove the non-differentiable parts of the \"model\" object\n",
        "opt_state = optimizer.init(eqx.filter(model,eqx.is_inexact_array))\n",
        "\n",
        "@eqx.filter_jit\n",
        "def make_step(model, opt_state, X, Y):     \n",
        "  loss_value, grads = eqx.filter_value_and_grad(residuals)(model, X, Y)\n",
        "  updates, opt_state = optimizer.update(grads, opt_state, model)\n",
        "  model = eqx.apply_updates(model, updates)\n",
        "  return model, opt_state, loss_value"
      ],
      "id": "f06a9ac9"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Loop"
      ],
      "id": "9099a459-4c84-4e9d-8eae-59f84f5922c4"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0,||theta - theta_hat|| = 0.20521600544452667\n",
            "Epoch 100,||theta - theta_hat|| = 0.038740210235118866\n",
            "Epoch 200,||theta - theta_hat|| = 0.0073235719464719296\n",
            "||theta - theta_hat|| = 0.0013871045084670186"
          ]
        }
      ],
      "source": [
        "num_epochs = 300\n",
        "batch_size = 64\n",
        "key, subkey = random.split(key) # will keep same key for shuffling each epoch\n",
        "for epoch in range(num_epochs):\n",
        "    key, subkey = random.split(key) # changing key for shuffling each epoch\n",
        "    train_loader = data_loader(subkey, X, Y, batch_size)\n",
        "    for X_batch, Y_batch in train_loader:\n",
        "        model, opt_state, train_loss = make_step(model, opt_state, X_batch, Y_batch)    \n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"Epoch {epoch},||theta - theta_hat|| = {jnp.linalg.norm(theta - model.weight)}\")\n",
        "\n",
        "print(f\"||theta - theta_hat|| = {jnp.linalg.norm(theta - model.weight)}\")"
      ],
      "id": "ff7f9a08"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Custom Types\n",
        "\n",
        "-   To prepare for layers of NN, we see that this class could have been\n",
        "    written manually, layering as we see fit\n",
        "-   See\n",
        "    [examples/linear_regression_jax_equinox.py](examples/linear_regression_jax_equinox.py)\n",
        "    for more, but we could have manually created the following"
      ],
      "id": "ed2003cf-bad5-4efc-b834-f9eacfd88707"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MyLinear(eqx.Module):\n",
        "    weight: jax.Array\n",
        "    def __init__(self, in_size, out_size, key):\n",
        "        self.weight = jax.random.normal(key, (out_size, in_size))\n",
        "    # Similar to Pytorch's forward\n",
        "    def __call__(self, x):\n",
        "        return self.weight @ x\n",
        "\n",
        "model = MyLinear(M, 1, key = subkey)"
      ],
      "id": "bfbc8ec8"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": "3"
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  }
}